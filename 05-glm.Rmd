# Generalized Linear Models {#glm}



```{example}
Consider the relation between cigarettes smoked, and the occurance of lung cancer. 
Do we expect it to be liner? Probably not.
Do we expect the variability to be constant about the trend, be it linear or not? Probably not.
```



## Problem Setup

In the Linear Models Chapter \@ref(linear), we assumed the generative process to be 
\begin{align}
  y|x=x'\beta+\varepsilon
  (\#eq:linear-mean-again)
\end{align}

This does not allow for assumingly non-linear relations, nor does it allow for the variablity of $\varepsilon$ to change with $x$.
Generalize linear models (GLM), as the name suggests, are a generalization that allow that.

```{remark}
Do not confuse _generalized linear models_ with _non-linear regression_, or _generalized least squares_.
These are different things, that we will not discuss.
```

To undestand GLM, we recall that with the normality of $\varepsilon$, Eq.\@ref(eq:linear-mean-again) implies that 
$$
 y|x \sim \mathcal{N}(x'\beta, \sigma^2)
$$
For the above example, we would like something in the lines of 
$$
 y|x \sim Binom(1,p(x))
$$
More generally, for some distribution $F(\theta)$, with a parameter $\theta$, we would like
\begin{align}
  y|x \sim F(\theta(x))
\end{align}

Possible examples include 
\begin{align}
 y|x &\sim Poisson(\lambda(x)) \\
 y|x &\sim Exp(\lambda(x)) \\
 y|x &\sim \mathcal{N}(\mu(x),\sigma^2(x)) 
\end{align}

GLMs constrain $\theta$ to be some function, $g$, of a linear combination of the $x$'s. 
Formally, $$\theta(x)=g(x'\beta)$$, where $$x'\beta=\beta_0 + \sum_j x_j \beta_j$$. 
The function $g$ is called the _link_ function.


## Logistic Regression

The best known of the GLM class of models is the _logistic regression_ that deals with Binomial, or more precisely, Brenoulli distributed data. 
The link function implied by the logistic regression is the logistic function
\begin{align}
  g(t)=\frac{e^t}{(1+e^t)}
  (\#eq:logistic-link)  
\end{align}
 implying that 
\begin{align}
  y|x \sim Binom \left( 1, p=\frac{e^{x'\beta}}{1+e^{x'\beta}} \right)
  (\#eq:logistic)
\end{align}

Before we fit such a model, we try to justify this contruction, in particular, this enigmatic link function in Eq.\@ref(eq:logistic-link).
Let's look at the simplest possible case: the comparison of two groups indexed by $x$: $x=0$ for the first, and $x=1$ for the second.
\begin{align}
   p(x=0)=P(y=1|x=0) &= \frac{e^{\beta_0}}{(1+e^{\beta_0})} (\#eq:odds-one) \\ 
   \Rightarrow 
   \frac{P(y=1|x=0)}{P(y=0|x=0)} &= e^{\beta_0} \\
   p(x=1)= P(y=1|x=1) &= \frac{e^{\beta_0+\beta_1}}{(1+e^{\beta_0+\beta_1})} \\
   \Rightarrow 
   \frac{P(y=1|x=1)}{P(y=0|x=1)} &= e^{\beta_0+\beta_1} (\#eq:odds-two)\\
   \Rightarrow 
   \frac{P(y=1|x=1)/P(y=0|x=1)}{P(y=1|x=0)/P(y=0|x=0)} 
   &= e^{\beta_1}  (\#eq:odds-ratio) \\
   \Rightarrow 
   \log \frac{P(y=1|x=1)/P(y=0|x=1)}{P(y=1|x=0)/P(y=0|x=0)} &= \beta_1. (\#eq:log-odds-ratio)
\end{align}

The magnitudes in Eqs.\@ref(eq:odds-one) and \@ref(eq:odds-two), are known as the _odds_. 
Odds are the same as probabilities, but instead of of telling me there is a $66\%$ of succeess, they tell me the odds of success are "2 to 1".

The magnitude in Eq.\@ref(eq:odds-ratio) is known as the _odds ratio_. 
The odds ratio compares between the probabilities of two groups, only that it does not compare  them in probability scale, but rather in odds scale. 

The magnitude in Eq.\@ref(eq:log-odds-ratio) is known as the _log odds ratio_.
Besides some nice theoretical properties of log odds ratios, which we will not discuss, they are important since it demistifies the choice of the link function in \@ref(eq:logistic-link): __it allows us to interpret $\beta$ of the logistic regression as the odds-ratios (in log scale)__.



### Logistic Regression with R

Let's get us some data. 
The `PlantGrowth` data records the weight of plants under three conditions: control, treatment1, and treatment2.
```{r}
head(PlantGrowth)
```

We will now `attach` the data so that its contents is available in the workspace (don't forget to `detach` afterwards, or you can expect some conflicting object names).
We will also use the `cut` function to create a two-class response variable for Light, and Heavy plants (we are doing logistic regression, so we need a two-class respose).
As a general rule of thumb, when we discretize continous variables, we lose information. 
for pedagogical reasons, however, we will proceed with this bad practice. 
```{r}
attach(PlantGrowth)
weight.factor<- cut(weight, 2, labels=c('Light', 'Heavy'))
plot(table(group, weight.factor))
```


Let's fit a logistic regression, and inspect the output.
```{r}
glm.1<- glm(weight.factor~group, family=binomial)
summary(glm.1)
```

Things to note:

- The `glm` function is our workhorse for all GLM models.
- The `family` argument of `glm` tells R the output is binomial, thus, performing a logistic regression.
- The `summary` function is content aware. It gives a different output for `glm` class objects than for other objects, such as the `lm` we saw in Chapter \@ref(linear).
- As usual, we get the coefficients table, but recall that they are to be interpreted as (log) odd-ratios.
- As usual, we get the significance for the test of no-effect, versus a two-sided alternative. 
- The residuals of `glm` are slightly different than the `lm` residuals, and called _Deviance Residuals_. 
- For help see `?summary.glm`.

Like for linear models, we can use an ANOVA table to check if treatments have any effect, and not one treatment at a time. 
In the case of GLMS, this is called an _analysis of deviance_ table. 
```{r}
anova(glm.1, test='LRT')
```

Things to note:

- The `anova` function, like the `summary` function, are content-aware and produce a different output for the `glm` class than for the `lm` class.
- In GLMs there is no canonical test (like the F test for `lm`). 
We thus specify the type of test desired with the `test` argument. 
- The distribution of the weights of the plants does vary with the treatment given, as we may see from the significance of the `group` factor.
- Readers familiar with ANOVA tables, should know that we computed the GLM equivalent of a type I sum- of-squares. 
Run `drop1(glm.1, test='Chisq')` for a GLM equivalent of a type III sum-of-squares. 
- For help see `?anova.glm`.


Let's predict the probability of a heavy plant for each treatment.
```{r}
predict(glm.1, type='response')
```

Things to note:

- Like the `summary` and `anova` functions, the `predict` function is aware that its input is of `glm` class. 
- In GLMs there are many types of predictions. The `type` argument controls which type is returned. 
- How do I know we are predicting the probability of a heavy plant, and not a light plant? Just run `contrasts(weight.factor)` to see which of the categories of the factor `weight.factor` is encoded as 1, and which as 0.
- For help see `?predict.glm`.


Let's detach the data so it is no longer in our workspace, and object names do not collide. 
```{r}
detach(PlantGrowth)
```


We gave an example with a factorial (i.e. discrete) predictor.
We can do the same with mltiple continous predictors.
```{r}
data('Pima.te', package='MASS') # Loads data
head(Pima.te)
```


```{r}
glm.2<- step(glm(type~., data=Pima.te, family=binomial))
summary(glm.2)
```

Things to note:

- We used the `~.` syntax to tell R to fit a model with all the available predictors.
- Since we want to focus on significant predictors, we used the `step` function to perform a _step-wise_ regression, i.e. sequantially remove non-significant predictors. 
The function reports each model it has checked, and the variable it has decided to remove at each step.
- The output of `step` is a single model, with the subset of significant predictors. 



## Poisson Regression

Poisson regression means we fit a model assuming $y|x &\sim Poisson(\lambda(x)) \\$.
Put differently, we assume that for each treatment, encoded as a combinations of predictors $x$, the response is Poisson distributed with a rate that depends on the predictors. 

The typical link function for Poisson regression is $g(t)=e^t$.
This means that we assume $y|x \sim Poisson(\lambda = e^{x'\beta})$

We take the example from `?glm`.
```{r}
counts <- c(18,17,15,20,10,20,25,13,12)
outcome <- gl(3,1,9)
treatment <- gl(3,3)
d.AD <- data.frame(treatment, outcome, counts)
head(d.AD)
```

Things to note:

- 

## main-effects fit as Poisson GLM with offset
<!-- glm(Claims ~ District + Group + Age + offset(log(Holders)), -->
<!--     data = Insurance, family = poisson) -->





## Bibliographic Notes
The ultimate reference on GLMs is @mccullagh1984generalized. 
For a less technical exposition, we refer to the usual @venables2013modern.
