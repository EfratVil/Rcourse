# Generalized Linear Models {#glm}



```{example}
Consider the relation between cigarettes smoked, and the occurance of lung cancer. 
Do we expect it to be liner? Probably not.
Do we expect the variability to be constant about the trend, be it linear or not? Probably not.
```



## Problem Setup

In the Linear Models Chapter \@ref(linear), we assumed the generative process to be 
\begin{align}
  y|x=x'\beta+\varepsilon
  (\#eq:linear-mean-again)
\end{align}

This does not allow for assumingly non-linear relations, nor does it allow for the variablity of $\varepsilon$ to change with $x$.
Generalize linear models (GLM), as the name suggests, are a generalization that allow that.

```{remark}
Do not confuse _generalized linear models_ with _non-linear regression_, or _generalized least squares_.
These are different things, that we will not discuss.
```

To undestand GLM, we recall that with the normality of $\varepsilon$, Eq.\@ref(eq:linear-mean-again) implies that 
$$
 y|x \sim \mathcal{N}(x'\beta, \sigma^2)
$$
For the above example, we would like something in the lines of 
$$
 y|x \sim Binom(1,p(x))
$$
More generally, for some distribution $F(\theta)$, with a parameter $\theta$, we would like
\begin{align}
  y|x \sim F(\theta(x))
\end{align}

Possible examples include 
\begin{align}
 y|x &\sim Poisson(\lambda(x)) \\
 y|x &\sim Exp(\lambda(x)) \\
 y|x &\sim \mathcal{N}(\mu(x),\sigma^2(x)) 
\end{align}

GLMs constrain $\theta$ to be some function, $g$, of a linear combination of the $x$'s. 
Formally, $$\theta(x)=g(x'\beta)$$, where $$x'\beta=\beta_0 + \sum_j x_j \beta_j$$. 
The function $g$ is called the _link_ function.


## Logistic Regression

The best known of the GLM class of models is the _logistic regression_ that deals with Binomial, or more precisely, Brenoulli distributed data. 
The link function implied by the logistic regression is the logistic function
\begin{align}
  g(t)=\frac{e^t}{(1+e^t)}
  (\#eq:logistic-link)  
\end{align}
 implying that 
\begin{align}
  y|x \sim Binom \left( 1, p=\frac{e^{x'\beta}}{1+e^{x'\beta}} \right)
  (\#eq:logistic)
\end{align}

Before we fit such a model, we try to justify this contruction, in particular, this enigmatic link function in Eq.\@ref(eq:logistic-link).
Let's look at the simplest possible case: the comparison of two groups indexed by $x$: $x=0$ for the first, and $x=1$ for the second.
\begin{align}
   p(x=0)=P(y=1|x=0) &= \frac{e^{\beta_0}}{(1+e^{\beta_0})} (\#eq:odds-one) \\ 
   \Rightarrow 
   \frac{P(y=1|x=0)}{P(y=0|x=0)} &= e^{\beta_0} \\
   p(x=1)= P(y=1|x=1) &= \frac{e^{\beta_0+\beta_1}}{(1+e^{\beta_0+\beta_1})} \\
   \Rightarrow 
   \frac{P(y=1|x=1)}{P(y=0|x=1)} &= e^{\beta_0+\beta_1} (\#eq:odds-two)\\
   \Rightarrow 
   \frac{P(y=1|x=1)/P(y=0|x=1)}{P(y=1|x=0)/P(y=0|x=0)} 
   &= e^{\beta_1}  (\#eq:odds-ratio) \\
   \Rightarrow 
   \log \frac{P(y=1|x=1)/P(y=0|x=1)}{P(y=1|x=0)/P(y=0|x=0)} &= \beta_1. (\#eq:log-odds-ratio)
\end{align}

The magnitudes in Eqs.\@ref(eq:odds-one) and \@ref(eq:odds-two), are known as the _odds_. 
Odds are the same as probabilities, but instead of of telling me there is a $66\%$ of succeess, they tell me the odds of success are "2 to 1".

The magnitude in Eq.\@ref(eq:odds-ratio) is known as the _odds ratio_. 
The odds ratio compares between the probabilities of two groups, only that it does not compare  them in probability scale, but rather in odds scale. 

The magnitude in Eq.\@ref(eq:log-odds-ratio) is known as the _log odds ratio_.
Besides some nice theoretical properties of log odds ratios, which we will not discuss, they are important since it demistifies the choice of the link function in \@ref(eq:logistic-link): __it allows us to interpret $\beta$ of the logistic regression as the odds-ratios (in log scale)__.



### Logistic Regression with R

Let's get us some data. 
The `PlantGrowth` data records the weight of plants under three conditions: control, treatment1, and treatment2.
```{r}
head(PlantGrowth)
```

We will now `attach` the data so that its contents is available in the workspace (don't forget to `detach` afterwards, or you can expect some conflicting object names).
We will also use the `cut` function to create a two-class response variable for Light, and Heavy plants (we are doing logistic regression, so we need a two-class respose).
We then inspect visually inspect the distribution of weights for each treatmet. 
```{r}
attach(PlantGrowth)
weight.factor<- cut(weight, 2, labels=c('Light', 'Heavy'))
plot(table(group, weight.factor))
```


Let's fit a logistic regression, and inspect the output.
```{r}
glm.1<- glm(weight.factor~group, family=binomial)
summary(glm.1)
prop.table(table(group, weight.factor), margin=1)
log(0.6/0.4)- log(0.2/0.8)
```

Things to note:

- The `glm` function is our workhorse for all GLM models.
- The `family` argument of `glm` tells R the output is binomial, thus, performing a logistic regression.
- The `summary` function is content aware. It gives a different output for `glm` class objects than for other objects, such as the `lm` we saw in Chapter \@ref(linear).
- As usual, we get the coefficients table, but recall that they are to be interpreted as (log) odd-ratios.
- As usual, we get the significance for the test of no-effect, versus a two-sided alternative. 
- The residuals of `glm` are slightly different than the `lm` residuals, and called _Deviance Residuals_. 

Like for linear models, we can use an ANOVA table to check if treatments have any effect, and not one treatment at a time. 
In the case of GLMS, this is called an _analysis of deviance_ table. 
```{r}
anova(glm.1, test='LRT')
```

Things to note:

- The `anova` function, like the `summary` function, are content-aware and produce a different output for the `glm` class than for the `lm` class.
- In GLMs, 
- 

```{r}
(predict.1<- predict(glm.1))
exp(predict.1)/(exp(predict.1)+1)
predict(glm.1, type='response')

```

# For help see ?predict.glm

# Analysis of Deviance ("ANOVA table")
anova(glm.1)
anova(glm.1, test='Chisq') # Equivalent to Type I sum of squares
drop1(glm.1, test='Chisq') # Equivalent to Type III sum of squares

# Analysis of residuals:
residuals(glm.1, type='response')
residuals(glm.1, type='deviance')
residuals.1<- residuals(glm.1, type='pearson')
# For help see residuals.glm

detach(PlantGrowth)



# Same with continous predictors:
rm(list=ls())
data('Pima.te', package='MASS') # Loads data
head(Pima.te)
plot(Pima.te)
glm.1<- step(glm(type~., data=Pima.te, family=binomial))
summary(glm.1)
coef(glm.1)

anova(glm.1, test='Chisq')
plot(glm.1)

residuals.1<- residuals(glm.1, type='pearson')
plot(residuals.1~Pima.te$npreg); abline(0,0, lty=2); lines(lowess(residuals.1~Pima.te$npreg), col='red')
plot(residuals.1~Pima.te$glu); abline(0,0, lty=2); lines(lowess(residuals.1~Pima.te$glu), col='red')
plot(residuals.1~Pima.te$bmi); abline(0,0, lty=2); lines(lowess(residuals.1~Pima.te$bmi), col='red')
plot(residuals.1~Pima.te$ped); abline(0,0, lty=2); lines(lowess(residuals.1~Pima.te$ped), col='red')

glm.2<- step(glm(type~.^2, data=Pima.te, family=binomial))
summary(glm.2)




## Other binomial regressions:
glm(type~., data=Pima.te, family=binomial(link='logit')) # Logistic CDF
glm(type~., data=Pima.te, family=binomial(link='probit')) # Gaissuain CDF
glm(type~., data=Pima.te, family=binomial(link='cauchit')) # Cauchy CDF
glm(type~., data=Pima.te, family=binomial(link='cloglog')) # Complementary log-log















## Bibliographic Notes
The ultimate reference on GLMs is @mccullagh1984generalized. 
For a less technical exposition, we refer to the usual @venables2013modern.
