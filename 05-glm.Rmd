# Generalized Linear Models {#glm}



```{example}
Consider the relation between cigarettes smoked, and the occurance of lung cancer. 
Do we expect it to be liner? Probably not.
Do we expect the variability to be constant about the trend, be it linear or not? Probably not.
```



## Problem Setup

In the Linear Models Chapter \@ref(linear), we assumed the generative process to be 
\begin{align}
  y|x=x'\beta+\varepsilon
  (\#eq:linear-mean-again)
\end{align}

This does not allow for assumingly non-linear relations, nor does it allow for the variablity of $\varepsilon$ to change with $x$.
Generalize linear models (GLM), as the name suggests, are a generalization that allow that.

```{remark}
Do not confuse _generalized linear models_ with _non-linear regression_, or _generalized least squares_.
These are different things, that we will not discuss.
```

To undestand GLM, we recall that with the normality of $\varepsilon$, Eq.\@ref(eq:linear-mean-again) implies that 
$$
 y|x \sim \mathcal{N}(x'\beta, \sigma^2)
$$
For the above example, we would like something in the lines of 
$$
 y|x \sim Binom(1,p(x))
$$
More generally, for some distribution $F(\theta)$, with a parameter $\theta$, we would like
\begin{align}
  y|x \sim F(\theta(x))
\end{align}

Possible examples include 
\begin{align}
 y|x &\sim Poisson(\lambda(x)) \\
 y|x &\sim Exp(\lambda(x)) \\
 y|x &\sim \mathcal{N}(\mu(x),\sigma^2(x)) 
\end{align}

GLMs constrain $\theta$ to be some function, $g$, of a linear combination of the $x$'s. 
Formally, $$\theta(x)=g(x'\beta)$$, where $$x'\beta=\beta_0 + \sum_j x_j \beta_j$$. 
The function $g$ is called the _link_ function.


## Logistic Regression

The best known of the GLM class of models is the _logistic regression_ that deals with Binomial, or more precisely, Brenoulli distributed data. 
The link function implied by the logistic regression is the logistic function
\begin{align}
  g(t)=\frac{e^t}{(1+e^t)}
  (\#eq:logistic-link)  
\end{align}
 implying that 
\begin{align}
  y|x \sim Binom \left( 1, p=\frac{e^{x'\beta}}{1+e^{x'\beta}} \right)
  (\#eq:logistic)
\end{align}

Before we fit such a model, we try to justify this contruction, in particular, this enigmatic link function in Eq.\@ref(eq:logistic-link).
Let's look at the simplest possible case: the comparison of two groups indexed by $x$: $x=0$ for the first, and $x=1$ for the second.
\begin{align}
   p(x=0)=P(y=1|x=0) &= \frac{e^{\beta_0}}{(1+e^{\beta_0})} (\#eq:odds-one) \\ 
   \Rightarrow 
   \frac{P(y=1|x=0)}{P(y=0|x=0)} &= e^{\beta_0} \\
   p(x=1)= P(y=1|x=1) &= \frac{e^{\beta_0+\beta_1}}{(1+e^{\beta_0+\beta_1})} \\
   \Rightarrow 
   \frac{P(y=1|x=1)}{P(y=0|x=1)} &= e^{\beta_0+\beta_1} (\#eq:odds-two)\\
   \Rightarrow 
   \frac{P(y=1|x=1)/P(y=0|x=1)}{P(y=1|x=0)/P(y=0|x=0)} 
   &= e^{\beta_1}  (\#eq:odds-ratio) \\
   \Rightarrow 
   \log \frac{P(y=1|x=1)/P(y=0|x=1)}{P(y=1|x=0)/P(y=0|x=0)} &= \beta_1. (\#eq:log-odds-ratio)
\end{align}

The magnitudes in Eqs.\@ref(eq:odds-one) and \@ref(eq:odds-two)  , is known as the _odds_. 
It is the same as a probability, but instead of of telling me there is a $66\%$ of succeess, it tells me the odds of success are "2 to 1".

The magnitude in Eq.\@ref(eq:odds-ratio) is knon as the _odds ratio_. 
It can be thought of as the compariosn between the probabilities of two groups, only that it does not compare probabilities but rather odds. 

The magnitude in Eq.\@ref(eq:log-odds-ratio) is known as the _log odds ratio_.
Besides some theoretical properties of the log odds ratio we will not discuss, it is important since it demistifies the choice of the link function in \@ref(eq:logistic-link): it allows us to interpret $\beta$ as the odds-ratios (in log scale).


### Logistic Regression with R

attach(PlantGrowth)
weight.factor<- cut(weight, 2, labels=c('Light', 'Heavy'))
plot(weight.factor)
plot(table(group, weight.factor))

contrasts(group)
options(contrasts=c(unordered='contr.treatment', ordered='contr.poly'))
contrasts(group)

# Fitting and interpreting a logistic regression:
glm.1<- glm(weight.factor~group, family=binomial)
summary(glm.1)
prop.table(table(group, weight.factor), margin=1)
log(0.6/0.4)- log(0.2/0.8)

# Predictions
(predict.1<- predict(glm.1))
exp(predict.1)/(exp(predict.1)+1) 
predict(glm.1, type='response')
# For help see ?predict.glm

# Analysis of Deviance ("ANOVA table")
anova(glm.1)
anova(glm.1, test='Chisq') # Equivalent to Type I sum of squares
drop1(glm.1, test='Chisq') # Equivalent to Type III sum of squares

# Analysis of residuals:
residuals(glm.1, type='response')
residuals(glm.1, type='deviance')
residuals.1<- residuals(glm.1, type='pearson')
# For help see residuals.glm

detach(PlantGrowth)



# Same with continous predictors:
rm(list=ls())
data('Pima.te', package='MASS') # Loads data
head(Pima.te)
plot(Pima.te)
glm.1<- step(glm(type~., data=Pima.te, family=binomial))
summary(glm.1)
coef(glm.1)

anova(glm.1, test='Chisq')
plot(glm.1)

residuals.1<- residuals(glm.1, type='pearson')
plot(residuals.1~Pima.te$npreg); abline(0,0, lty=2); lines(lowess(residuals.1~Pima.te$npreg), col='red')
plot(residuals.1~Pima.te$glu); abline(0,0, lty=2); lines(lowess(residuals.1~Pima.te$glu), col='red')
plot(residuals.1~Pima.te$bmi); abline(0,0, lty=2); lines(lowess(residuals.1~Pima.te$bmi), col='red')
plot(residuals.1~Pima.te$ped); abline(0,0, lty=2); lines(lowess(residuals.1~Pima.te$ped), col='red')

glm.2<- step(glm(type~.^2, data=Pima.te, family=binomial))
summary(glm.2)




## Other binomial regressions:
glm(type~., data=Pima.te, family=binomial(link='logit')) # Logistic CDF
glm(type~., data=Pima.te, family=binomial(link='probit')) # Gaissuain CDF
glm(type~., data=Pima.te, family=binomial(link='cauchit')) # Cauchy CDF
glm(type~., data=Pima.te, family=binomial(link='cloglog')) # Complementary log-log 















## Bibliographic Notes
glm
