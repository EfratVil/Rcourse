<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>R (BGU course)</title>
  <meta name="description" content="Class notes for the R course at the BGU’s IE&amp;M dept.">
  <meta name="generator" content="bookdown 0.3.9 and GitBook 2.6.7">

  <meta property="og:title" content="R (BGU course)" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Class notes for the R course at the BGU’s IE&amp;M dept." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="R (BGU course)" />
  
  <meta name="twitter:description" content="Class notes for the R course at the BGU’s IE&amp;M dept." />
  

<meta name="author" content="Jonathan D. Rosenblatt">


<meta name="date" content="2017-02-19">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="multivariate.html">
<link rel="next" href="unsupervised.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.8/htmlwidgets.js"></script>
<link href="libs/plotlyjs-1.16.3/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.16.3/plotly-latest.min.js"></script>
<script src="libs/plotly-binding-4.5.6/plotly.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">R Course</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#what-r"><i class="fa fa-check"></i><b>2.1</b> What is R?</a></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#ecosystem"><i class="fa fa-check"></i><b>2.2</b> The R Ecosystem</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="basics.html"><a href="basics.html"><i class="fa fa-check"></i><b>3</b> R Basics</a><ul>
<li class="chapter" data-level="3.1" data-path="basics.html"><a href="basics.html#simple-calculator"><i class="fa fa-check"></i><b>3.1</b> Simple calculator</a></li>
<li class="chapter" data-level="3.2" data-path="basics.html"><a href="basics.html#probability-calculator"><i class="fa fa-check"></i><b>3.2</b> Probability calculator</a></li>
<li class="chapter" data-level="3.3" data-path="basics.html"><a href="basics.html#getting-help"><i class="fa fa-check"></i><b>3.3</b> Getting Help</a></li>
<li class="chapter" data-level="3.4" data-path="basics.html"><a href="basics.html#variable-asignment"><i class="fa fa-check"></i><b>3.4</b> Variable Asignment</a></li>
<li class="chapter" data-level="3.5" data-path="basics.html"><a href="basics.html#piping"><i class="fa fa-check"></i><b>3.5</b> Piping</a></li>
<li class="chapter" data-level="3.6" data-path="basics.html"><a href="basics.html#vector-creation-and-manipulation"><i class="fa fa-check"></i><b>3.6</b> Vector creation and manipulation</a></li>
<li class="chapter" data-level="3.7" data-path="basics.html"><a href="basics.html#search-paths-and-packages"><i class="fa fa-check"></i><b>3.7</b> Search paths and packages</a></li>
<li class="chapter" data-level="3.8" data-path="basics.html"><a href="basics.html#simple-plotting"><i class="fa fa-check"></i><b>3.8</b> Simple plotting</a></li>
<li class="chapter" data-level="3.9" data-path="basics.html"><a href="basics.html#object-types"><i class="fa fa-check"></i><b>3.9</b> Object types</a></li>
<li class="chapter" data-level="3.10" data-path="basics.html"><a href="basics.html#data-frames"><i class="fa fa-check"></i><b>3.10</b> Data Frames</a></li>
<li class="chapter" data-level="3.11" data-path="basics.html"><a href="basics.html#exctraction"><i class="fa fa-check"></i><b>3.11</b> Exctraction</a></li>
<li class="chapter" data-level="3.12" data-path="basics.html"><a href="basics.html#data-import-and-export"><i class="fa fa-check"></i><b>3.12</b> Data Import and Export</a><ul>
<li class="chapter" data-level="3.12.1" data-path="basics.html"><a href="basics.html#import-from-web"><i class="fa fa-check"></i><b>3.12.1</b> Import from WEB</a></li>
<li class="chapter" data-level="3.12.2" data-path="basics.html"><a href="basics.html#export-as-csv"><i class="fa fa-check"></i><b>3.12.2</b> Export as CSV</a></li>
<li class="chapter" data-level="3.12.3" data-path="basics.html"><a href="basics.html#reading-from-text-files"><i class="fa fa-check"></i><b>3.12.3</b> Reading From Text Files</a></li>
<li class="chapter" data-level="3.12.4" data-path="basics.html"><a href="basics.html#writing-data-to-text-files"><i class="fa fa-check"></i><b>3.12.4</b> Writing Data to Text Files</a></li>
<li class="chapter" data-level="3.12.5" data-path="basics.html"><a href="basics.html#xlsx-files"><i class="fa fa-check"></i><b>3.12.5</b> .XLS(X) files</a></li>
<li class="chapter" data-level="3.12.6" data-path="basics.html"><a href="basics.html#massive-files"><i class="fa fa-check"></i><b>3.12.6</b> Massive files</a></li>
<li class="chapter" data-level="3.12.7" data-path="basics.html"><a href="basics.html#databases"><i class="fa fa-check"></i><b>3.12.7</b> Databases</a></li>
</ul></li>
<li class="chapter" data-level="3.13" data-path="basics.html"><a href="basics.html#functions"><i class="fa fa-check"></i><b>3.13</b> Functions</a></li>
<li class="chapter" data-level="3.14" data-path="basics.html"><a href="basics.html#looping"><i class="fa fa-check"></i><b>3.14</b> Looping</a></li>
<li class="chapter" data-level="3.15" data-path="basics.html"><a href="basics.html#recursion"><i class="fa fa-check"></i><b>3.15</b> Recursion</a></li>
<li class="chapter" data-level="3.16" data-path="basics.html"><a href="basics.html#bibliographic-notes"><i class="fa fa-check"></i><b>3.16</b> Bibliographic Notes</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="eda.html"><a href="eda.html"><i class="fa fa-check"></i><b>4</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="eda.html"><a href="eda.html#summary-statistics"><i class="fa fa-check"></i><b>4.1</b> Summary Statistics</a><ul>
<li class="chapter" data-level="4.1.1" data-path="eda.html"><a href="eda.html#categorical-data"><i class="fa fa-check"></i><b>4.1.1</b> Categorical Data</a></li>
<li class="chapter" data-level="4.1.2" data-path="eda.html"><a href="eda.html#continous-data"><i class="fa fa-check"></i><b>4.1.2</b> Continous Data</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="eda.html"><a href="eda.html#visualization"><i class="fa fa-check"></i><b>4.2</b> Visualization</a><ul>
<li class="chapter" data-level="4.2.1" data-path="eda.html"><a href="eda.html#categorical-data-1"><i class="fa fa-check"></i><b>4.2.1</b> Categorical Data</a></li>
<li class="chapter" data-level="4.2.2" data-path="eda.html"><a href="eda.html#continous-data-1"><i class="fa fa-check"></i><b>4.2.2</b> Continous Data</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="eda.html"><a href="eda.html#bibliographic-notes-1"><i class="fa fa-check"></i><b>4.3</b> Bibliographic Notes</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lm.html"><a href="lm.html"><i class="fa fa-check"></i><b>5</b> Linear Models</a><ul>
<li class="chapter" data-level="5.1" data-path="lm.html"><a href="lm.html#problem-setup"><i class="fa fa-check"></i><b>5.1</b> Problem Setup</a></li>
<li class="chapter" data-level="5.2" data-path="lm.html"><a href="lm.html#ols-estimation"><i class="fa fa-check"></i><b>5.2</b> OLS Estimation</a></li>
<li class="chapter" data-level="5.3" data-path="lm.html"><a href="lm.html#inference"><i class="fa fa-check"></i><b>5.3</b> Inference</a><ul>
<li class="chapter" data-level="5.3.1" data-path="lm.html"><a href="lm.html#testing-a-hypothesis-on-a-single-coefficient"><i class="fa fa-check"></i><b>5.3.1</b> Testing a Hypothesis on a Single Coefficient</a></li>
<li class="chapter" data-level="5.3.2" data-path="lm.html"><a href="lm.html#constructing-a-confidence-interval-on-a-single-coefficient"><i class="fa fa-check"></i><b>5.3.2</b> Constructing a Confidence Interval on a Single Coefficient</a></li>
<li class="chapter" data-level="5.3.3" data-path="lm.html"><a href="lm.html#multiple-regression"><i class="fa fa-check"></i><b>5.3.3</b> Multiple Regression</a></li>
<li class="chapter" data-level="5.3.4" data-path="lm.html"><a href="lm.html#testing-a-hypothesis-on-a-single-contrast"><i class="fa fa-check"></i><b>5.3.4</b> Testing a Hypothesis on a Single Contrast</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="lm.html"><a href="lm.html#bibliographic-notes-2"><i class="fa fa-check"></i><b>5.4</b> Bibliographic Notes</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>6</b> Generalized Linear Models</a><ul>
<li class="chapter" data-level="6.1" data-path="glm.html"><a href="glm.html#problem-setup-1"><i class="fa fa-check"></i><b>6.1</b> Problem Setup</a></li>
<li class="chapter" data-level="6.2" data-path="glm.html"><a href="glm.html#logistic-regression"><i class="fa fa-check"></i><b>6.2</b> Logistic Regression</a><ul>
<li class="chapter" data-level="6.2.1" data-path="glm.html"><a href="glm.html#logistic-regression-with-r"><i class="fa fa-check"></i><b>6.2.1</b> Logistic Regression with R</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="glm.html"><a href="glm.html#poisson-regression"><i class="fa fa-check"></i><b>6.3</b> Poisson Regression</a></li>
<li class="chapter" data-level="6.4" data-path="glm.html"><a href="glm.html#extensions"><i class="fa fa-check"></i><b>6.4</b> Extensions</a></li>
<li class="chapter" data-level="6.5" data-path="glm.html"><a href="glm.html#bibliographic-notes-3"><i class="fa fa-check"></i><b>6.5</b> Bibliographic Notes</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="lme.html"><a href="lme.html"><i class="fa fa-check"></i><b>7</b> Linear Mixed Models</a><ul>
<li class="chapter" data-level="7.1" data-path="lme.html"><a href="lme.html#problem-setup-2"><i class="fa fa-check"></i><b>7.1</b> Problem Setup</a></li>
<li class="chapter" data-level="7.2" data-path="lme.html"><a href="lme.html#mixed-models-with-r"><i class="fa fa-check"></i><b>7.2</b> Mixed Models with R</a><ul>
<li class="chapter" data-level="7.2.1" data-path="lme.html"><a href="lme.html#a-single-random-effect"><i class="fa fa-check"></i><b>7.2.1</b> A Single Random Effect</a></li>
<li class="chapter" data-level="7.2.2" data-path="lme.html"><a href="lme.html#several-random-effects"><i class="fa fa-check"></i><b>7.2.2</b> Several Random Effects</a></li>
<li class="chapter" data-level="7.2.3" data-path="lme.html"><a href="lme.html#a-full-mixed-model"><i class="fa fa-check"></i><b>7.2.3</b> A Full Mixed-Model</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="lme.html"><a href="lme.html#bibliographic-notes-4"><i class="fa fa-check"></i><b>7.3</b> Bibliographic Notes</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="multivariate.html"><a href="multivariate.html"><i class="fa fa-check"></i><b>8</b> Multivariate Data Analysis</a><ul>
<li class="chapter" data-level="8.1" data-path="multivariate.html"><a href="multivariate.html#signal-detection"><i class="fa fa-check"></i><b>8.1</b> Signal Detection</a></li>
<li class="chapter" data-level="8.2" data-path="multivariate.html"><a href="multivariate.html#signal-counting"><i class="fa fa-check"></i><b>8.2</b> Signal Counting</a></li>
<li class="chapter" data-level="8.3" data-path="multivariate.html"><a href="multivariate.html#signal-identification"><i class="fa fa-check"></i><b>8.3</b> Signal Identification</a></li>
<li class="chapter" data-level="8.4" data-path="multivariate.html"><a href="multivariate.html#signal-estimation"><i class="fa fa-check"></i><b>8.4</b> Signal Estimation</a></li>
<li class="chapter" data-level="8.5" data-path="multivariate.html"><a href="multivariate.html#multivariate-regression"><i class="fa fa-check"></i><b>8.5</b> Multivariate Regression</a></li>
<li class="chapter" data-level="8.6" data-path="multivariate.html"><a href="multivariate.html#distribution-fitting"><i class="fa fa-check"></i><b>8.6</b> Distribution Fitting</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="supervised.html"><a href="supervised.html"><i class="fa fa-check"></i><b>9</b> Supervised Learning</a><ul>
<li class="chapter" data-level="9.1" data-path="supervised.html"><a href="supervised.html#problem-setup-3"><i class="fa fa-check"></i><b>9.1</b> Problem setup</a><ul>
<li class="chapter" data-level="9.1.1" data-path="supervised.html"><a href="supervised.html#common-hypothesis-classes"><i class="fa fa-check"></i><b>9.1.1</b> Common Hypothesis Classes</a></li>
<li class="chapter" data-level="9.1.2" data-path="supervised.html"><a href="supervised.html#common-complexity-penalties"><i class="fa fa-check"></i><b>9.1.2</b> Common Complexity Penalties</a></li>
<li class="chapter" data-level="9.1.3" data-path="supervised.html"><a href="supervised.html#unbiased-risk-estimation"><i class="fa fa-check"></i><b>9.1.3</b> Unbiased Risk Estimation</a></li>
<li class="chapter" data-level="9.1.4" data-path="supervised.html"><a href="supervised.html#collecting-the-pieces"><i class="fa fa-check"></i><b>9.1.4</b> Collecting the Pieces</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="supervised.html"><a href="supervised.html#supervised-learning-in-r"><i class="fa fa-check"></i><b>9.2</b> Supervised Learning in R</a><ul>
<li class="chapter" data-level="9.2.1" data-path="supervised.html"><a href="supervised.html#least-squares"><i class="fa fa-check"></i><b>9.2.1</b> Linear Models with Least Squares Loss</a></li>
<li class="chapter" data-level="9.2.2" data-path="supervised.html"><a href="supervised.html#svm"><i class="fa fa-check"></i><b>9.2.2</b> SVM</a></li>
<li class="chapter" data-level="9.2.3" data-path="supervised.html"><a href="supervised.html#neural-nets"><i class="fa fa-check"></i><b>9.2.3</b> Neural Nets</a></li>
<li class="chapter" data-level="9.2.4" data-path="supervised.html"><a href="supervised.html#classification-and-regression-trees-cart"><i class="fa fa-check"></i><b>9.2.4</b> Classification and Regression Trees (CART)</a></li>
<li class="chapter" data-level="9.2.5" data-path="supervised.html"><a href="supervised.html#k-nearest-neighbour-knn"><i class="fa fa-check"></i><b>9.2.5</b> K-nearest neighbour (KNN)</a></li>
<li class="chapter" data-level="9.2.6" data-path="supervised.html"><a href="supervised.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.2.6</b> Linear Discriminant Analysis (LDA)</a></li>
<li class="chapter" data-level="9.2.7" data-path="supervised.html"><a href="supervised.html#naive-bayes"><i class="fa fa-check"></i><b>9.2.7</b> Naive Bayes</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="supervised.html"><a href="supervised.html#bibliographic-notes-5"><i class="fa fa-check"></i><b>9.3</b> Bibliographic Notes</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="unsupervised.html"><a href="unsupervised.html"><i class="fa fa-check"></i><b>10</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="10.1" data-path="unsupervised.html"><a href="unsupervised.html#dim-reduce"><i class="fa fa-check"></i><b>10.1</b> Dimensionality Reduction</a><ul>
<li class="chapter" data-level="10.1.1" data-path="unsupervised.html"><a href="unsupervised.html#pca"><i class="fa fa-check"></i><b>10.1.1</b> Principal Component Analysis</a></li>
<li class="chapter" data-level="10.1.2" data-path="unsupervised.html"><a href="unsupervised.html#preliminaries"><i class="fa fa-check"></i><b>10.1.2</b> Preliminaries</a></li>
<li class="chapter" data-level="10.1.3" data-path="unsupervised.html"><a href="unsupervised.html#latent-variable-approaches"><i class="fa fa-check"></i><b>10.1.3</b> Latent Variable Approaches</a></li>
<li class="chapter" data-level="10.1.4" data-path="unsupervised.html"><a href="unsupervised.html#purely-algorithmic-approaches"><i class="fa fa-check"></i><b>10.1.4</b> Purely Algorithmic Approaches</a></li>
<li class="chapter" data-level="10.1.5" data-path="unsupervised.html"><a href="unsupervised.html#dimensionality-reduction-in-r"><i class="fa fa-check"></i><b>10.1.5</b> Dimensionality Reduction in R</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="unsupervised.html"><a href="unsupervised.html#cluster"><i class="fa fa-check"></i><b>10.2</b> Clustering</a><ul>
<li class="chapter" data-level="10.2.1" data-path="unsupervised.html"><a href="unsupervised.html#latent-variable-approaches-1"><i class="fa fa-check"></i><b>10.2.1</b> Latent Variable Approaches</a></li>
<li class="chapter" data-level="10.2.2" data-path="unsupervised.html"><a href="unsupervised.html#purely-algorithmic-approaches-1"><i class="fa fa-check"></i><b>10.2.2</b> Purely Algorithmic Approaches</a></li>
<li class="chapter" data-level="10.2.3" data-path="unsupervised.html"><a href="unsupervised.html#clustering-in-r"><i class="fa fa-check"></i><b>10.2.3</b> Clustering in R</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="unsupervised.html"><a href="unsupervised.html#bibliographic-notes-6"><i class="fa fa-check"></i><b>10.3</b> Bibliographic Notes</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="plotting.html"><a href="plotting.html"><i class="fa fa-check"></i><b>11</b> Plotting</a><ul>
<li class="chapter" data-level="11.1" data-path="plotting.html"><a href="plotting.html#the-graphics-system"><i class="fa fa-check"></i><b>11.1</b> The graphics System</a><ul>
<li class="chapter" data-level="11.1.1" data-path="plotting.html"><a href="plotting.html#using-existing-plotting-functions"><i class="fa fa-check"></i><b>11.1.1</b> Using Existing Plotting Functions</a></li>
<li class="chapter" data-level="11.1.2" data-path="plotting.html"><a href="plotting.html#the-power-of-the-graphics-device"><i class="fa fa-check"></i><b>11.1.2</b> The Power of the graphics device</a></li>
<li class="chapter" data-level="11.1.3" data-path="plotting.html"><a href="plotting.html#exporting-a-plot"><i class="fa fa-check"></i><b>11.1.3</b> Exporting a Plot</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="plotting.html"><a href="plotting.html#the-ggplot2-system"><i class="fa fa-check"></i><b>11.2</b> The ggplot2 System</a></li>
<li class="chapter" data-level="11.3" data-path="plotting.html"><a href="plotting.html#interactive-graphics"><i class="fa fa-check"></i><b>11.3</b> Interactive Graphics</a><ul>
<li class="chapter" data-level="11.3.1" data-path="plotting.html"><a href="plotting.html#plotly"><i class="fa fa-check"></i><b>11.3.1</b> Plotly</a></li>
<li class="chapter" data-level="11.3.2" data-path="plotting.html"><a href="plotting.html#html-widgets"><i class="fa fa-check"></i><b>11.3.2</b> HTML Widgets</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="plotting.html"><a href="plotting.html#bibliographic-notes-7"><i class="fa fa-check"></i><b>11.4</b> Bibliographic Notes</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="report.html"><a href="report.html"><i class="fa fa-check"></i><b>12</b> Reports</a><ul>
<li class="chapter" data-level="12.1" data-path="report.html"><a href="report.html#knitr"><i class="fa fa-check"></i><b>12.1</b> knitr</a><ul>
<li class="chapter" data-level="12.1.1" data-path="report.html"><a href="report.html#installation"><i class="fa fa-check"></i><b>12.1.1</b> Installation</a></li>
<li class="chapter" data-level="12.1.2" data-path="report.html"><a href="report.html#markdown"><i class="fa fa-check"></i><b>12.1.2</b> Markdown</a></li>
<li class="chapter" data-level="12.1.3" data-path="report.html"><a href="report.html#rmarkdown"><i class="fa fa-check"></i><b>12.1.3</b> Rmarkdown</a></li>
<li class="chapter" data-level="12.1.4" data-path="report.html"><a href="report.html#compiling"><i class="fa fa-check"></i><b>12.1.4</b> Compiling</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="report.html"><a href="report.html#bookdown"><i class="fa fa-check"></i><b>12.2</b> bookdown</a></li>
<li class="chapter" data-level="12.3" data-path="report.html"><a href="report.html#shiny"><i class="fa fa-check"></i><b>12.3</b> Shiny</a><ul>
<li class="chapter" data-level="12.3.1" data-path="report.html"><a href="report.html#installation-1"><i class="fa fa-check"></i><b>12.3.1</b> Installation</a></li>
<li class="chapter" data-level="12.3.2" data-path="report.html"><a href="report.html#the-basics-of-shiny"><i class="fa fa-check"></i><b>12.3.2</b> The Basics of Shiny</a></li>
<li class="chapter" data-level="12.3.3" data-path="report.html"><a href="report.html#beyond-the-basics"><i class="fa fa-check"></i><b>12.3.3</b> Beyond the Basics</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="report.html"><a href="report.html#bibliographic-notes-8"><i class="fa fa-check"></i><b>12.4</b> Bibliographic Notes</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="hadley.html"><a href="hadley.html"><i class="fa fa-check"></i><b>13</b> The Hadleyverse</a><ul>
<li class="chapter" data-level="13.1" data-path="hadley.html"><a href="hadley.html#readr"><i class="fa fa-check"></i><b>13.1</b> readr</a></li>
<li class="chapter" data-level="13.2" data-path="hadley.html"><a href="hadley.html#dplyr"><i class="fa fa-check"></i><b>13.2</b> dplyr</a></li>
<li class="chapter" data-level="13.3" data-path="hadley.html"><a href="hadley.html#tidyr"><i class="fa fa-check"></i><b>13.3</b> tidyr</a></li>
<li class="chapter" data-level="13.4" data-path="hadley.html"><a href="hadley.html#reshape2"><i class="fa fa-check"></i><b>13.4</b> reshape2</a></li>
<li class="chapter" data-level="13.5" data-path="hadley.html"><a href="hadley.html#stringr"><i class="fa fa-check"></i><b>13.5</b> stringr</a></li>
<li class="chapter" data-level="13.6" data-path="hadley.html"><a href="hadley.html#anytime"><i class="fa fa-check"></i><b>13.6</b> anytime</a></li>
<li class="chapter" data-level="13.7" data-path="hadley.html"><a href="hadley.html#biblipgraphic-notes"><i class="fa fa-check"></i><b>13.7</b> Biblipgraphic Notes</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="sparse.html"><a href="sparse.html"><i class="fa fa-check"></i><b>14</b> Sparse Representations</a></li>
<li class="chapter" data-level="15" data-path="memory.html"><a href="memory.html"><i class="fa fa-check"></i><b>15</b> Memory Efficiency</a></li>
<li class="chapter" data-level="16" data-path="parallel.html"><a href="parallel.html"><i class="fa fa-check"></i><b>16</b> Parallel Computing</a></li>
<li class="chapter" data-level="17" data-path="algebra.html"><a href="algebra.html"><i class="fa fa-check"></i><b>17</b> Numerical Linear Algebra</a></li>
<li class="chapter" data-level="18" data-path="convex.html"><a href="convex.html"><i class="fa fa-check"></i><b>18</b> Convex Optimization</a></li>
<li class="chapter" data-level="19" data-path="rcpp.html"><a href="rcpp.html"><i class="fa fa-check"></i><b>19</b> RCpp</a></li>
<li class="chapter" data-level="20" data-path="package.html"><a href="package.html"><i class="fa fa-check"></i><b>20</b> Writing Packages</a></li>
<li class="chapter" data-level="21" data-path="bib.html"><a href="bib.html"><i class="fa fa-check"></i><b>21</b> Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">R (BGU course)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="supervised" class="section level1">
<h1><span class="header-section-number">Chapter 9</span> Supervised Learning</h1>
<p>Machine learning is very similar to statistics, but it is certainly not the same. As the name suggests, in machine learning we want machines to learn. This means that we want to replace hard-coded expert algorithm, with data-driven self-learned algorithm.</p>
<p>There are many learning setups, that depend on what is available to the machine. The most common setup, discussed in this chapter, is <em>supervised learning</em>. The name takes from the fact that by giving the machine data samples with known inputs (a.k.a. features) and desired outputs (a.k.a. labels), the human is effectively supervising the learning. If we think of the inputs as predictors, and outcomes as predicted, it is no wonder that supervised learning is very similar to statistical prediction. When asked “are these the same?” I like to give the example of internet fraud. If you take a sample of fraud “attacks”, a statistical formulation of the problem is highly unlikely. This is because fraud events are not randomly drawn from some distribution, but rather, arrive from an adversary learning the defenses and adapting to it. This instance of supervised learning belongs in game theory, more than it does in statistics.</p>
<p>Other types of machine learning problems include:</p>
<ul>
<li><strong>Unsupervised learning</strong>: See Chapter <a href="unsupervised.html#unsupervised">10</a>.</li>
<li><strong>Semi supervised learning</strong>: Where only part of the samples are labeled. A.k.a. <em>co-training</em>, <em>learning from labeled and unlabeled data</em>, <em>transductive learning</em>.</li>
<li><strong>Active learning</strong>: Where the machine is allowed to query the user for labels. Very similar to <em>adaptive design of experiments</em>.</li>
<li><strong>Reinforcement learning</strong>: Similar to active learning, in that the machine may query for labels. Different from active learning, in that the machine does not receive labels, but <em>rewards</em>.</li>
<li><strong>Learning on a budget</strong>: A version of active learning where querying for labels induces variable costs.</li>
<li><strong>Structure learning</strong>: The learning of the dependence structure between variables.</li>
<li><strong>Learning to learn</strong>: Deals with the carriage of “experience” from one learning problem to another. A.k.a. <em>cummulative learning</em> and <em>meta learning</em>.</li>
<li><strong>Manifold learning</strong>: An instance of unsupervised learning, where the goal is to reduce the dimension of the data by embedding it into a lower dimensional manifold. A.k.a. <em>support estimation</em>.</li>
</ul>
<div id="problem-setup-3" class="section level2">
<h2><span class="header-section-number">9.1</span> Problem setup</h2>
<p>We now present the <em>empirical risk minimization</em> to supervised learning.</p>

<div class="remark">
<span class="remark"><em>Remark. </em></span> We do not discuss purely algorithmic approaches such as K-nearest neighbour and <em>kernel smoothing</em> due to space constraints. For a broader review of supervised learning, see the Bibliographic Notes section.
</div>
<p></p>
<p>Given <span class="math inline">\(n\)</span> samples with inputs <span class="math inline">\(x\)</span> from some space <span class="math inline">\(\mathcal{X}\)</span> and desired outcome, <span class="math inline">\(y\)</span>, from some space <span class="math inline">\(\mathcal{Y}\)</span>. Samples, <span class="math inline">\((x,y)\)</span> have some distribution we denote <span class="math inline">\(P\)</span>. We want to learn a function that maps inputs to outputs. This function is called a <em>hypothesis</em>, or <em>predictor</em>, or <em>classifier</em> denoted <span class="math inline">\(f\)</span>, that belongs to a hypothesis class <span class="math inline">\(\mathcal{F}\)</span> such that <span class="math inline">\(f:\mathcal{X} \to \mathcal{Y}\)</span>. We also choose some other function that fines us for erroneous prediction. This function is called the <em>loss</em>, and we denote it by <span class="math inline">\(l:\mathcal{Y}\times \mathcal{Y} \to \mathbb{R}^+\)</span>.</p>

<div class="remark">
<span class="remark"><em>Remark. </em></span> The <em>hypothesis</em> in machine learning is only vaguely related the <em>hypothesis</em> in statistical testing, which is quite confusing.
</div>
<p></p>

<div class="remark">
<span class="remark"><em>Remark. </em></span> The <em>hypothesis</em> in machine learning is not a bona-fide <em>statistical model</em> since we don’t assume it is the data generating process, but rather some function which we choose for its good predictive performance.
</div>
<p></p>
The fundamental task in supervised (statistical) learning is to recover a hypothesis that minimizes the average loss in the sample, and not in the population. This is know as the <em>risk minimization problem</em>.
<span class="math display" id="eq:risk">\[\begin{align}
  f^* := argmin_f \{ E_P[l(f(x),y)] \}
  \tag{9.1}  
\end{align}\]</span>
To make things more explicit, <span class="math inline">\(f\)</span> may be a linear function, and <span class="math inline">\(l\)</span> a squared error loss, in which case problem <a href="supervised.html#eq:risk">(9.1)</a> collapses to
<span class="math display">\[\begin{align}
  f^* := argmin_\beta \{ E_P[(x&#39;\beta-y)^2] \}
\end{align}\]</span>
Another fundamental problem is that we do not know the distribution of all possible inputs and outputs, <span class="math inline">\(P\)</span>. We typically only have a sample of <span class="math inline">\((x_i,y_i), i=1,\dots,n\)</span>. We thus state the <em>empirical</em> counterpart of <a href="supervised.html#eq:risk">(9.1)</a>, which consists of minimizing the average loss. This is known as the <em>empirical risk miminization</em> problem (ERM).
<span class="math display" id="eq:erm">\[\begin{align}
  \hat f := argmin_f \{ \sum_i l(f(x_i),y_i) \}
  \tag{9.2}  
\end{align}\]</span>
Making things more explicit again by using a linear hypothesis with squared loss, we see that the empirical risk minimization problem collapses to an ordinary least-squares problem:
<span class="math display">\[\begin{align}
  \hat f := argmin_\beta \{ \sum_i (x_\beta-y_i)^2 \}
\end{align}\]</span>
<p>When data is samples are independent, then maximum likelihood estimation is also an instance of ERM, when using the (negative) log likelihood as the loss function.</p>
<p>If we don’t assume any structure on the hypothesis, <span class="math inline">\(f\)</span>, then <span class="math inline">\(\hat f\)</span> from <a href="supervised.html#eq:erm">(9.2)</a> will interpolate the data, and will be a very bad predictor. We say, it will <em>overfit</em> the observed data, and will have bad performance on new data.</p>
<p>We have several ways to avoid overfitting:</p>
<ol style="list-style-type: decimal">
<li>Restrict the hypothesis class <span class="math inline">\(\mathcal{F}\)</span> (such as linear functions).</li>
<li>Penalize for the complexity of <span class="math inline">\(f\)</span>. The penalty denoted by <span class="math inline">\(\Vert f \Vert\)</span>.</li>
<li>Unbiased risk estimation, where we deal with the overfitted optimism of the empirical risk by debiasing it.</li>
</ol>
<div id="common-hypothesis-classes" class="section level3">
<h3><span class="header-section-number">9.1.1</span> Common Hypothesis Classes</h3>
<p>Some common hypothesis classes, <span class="math inline">\(\mathcal{F}\)</span>, with restricted complexity, are:</p>
<ol style="list-style-type: decimal">
<li><strong>Linear hypotheses</strong>: such as linear models, GLMs, and (linear) support vector machines (SVM).</li>
<li><strong>Neural networks</strong>: a.k.a. <em>feed-forward</em> neural nets, <em>artificial</em> neural nets, and the celebrated class of <em>deep</em> neural nets.</li>
<li><strong>Tree</strong>: a.k.a. <em>decision rules</em>, is a class of hypotheses which can be stated as “if-then” rules.</li>
<li><strong>Reproducing Kernel Hilbert Space</strong>: a.k.a. RKHS, is a subset of “the space of all functions<a href="#fn13" class="footnoteRef" id="fnref13"><sup>13</sup></a>” that is both large enough to capture very complicated relations, but small enough so that it is less prone to overfitting, and also surprisingly simple to compute with.</li>
<li><strong>Ensembles</strong>: a “meta” hypothesis class, which consists of taking multiple hypotheses, possibly from different classes, and combining them.</li>
</ol>
</div>
<div id="common-complexity-penalties" class="section level3">
<h3><span class="header-section-number">9.1.2</span> Common Complexity Penalties</h3>
<p>The most common complexity penalty applied to classes that have a finite dimensional parametric representation, such as a the linear class parametrized via its coefficients <span class="math inline">\(\beta\)</span>. In such classes we may penalize for the norm of the parameters. Common penalties include:</p>
<ol style="list-style-type: decimal">
<li><strong>Ridge penalty</strong>: penalizing the <span class="math inline">\(l_2\)</span> norm of the parameter. I.e. <span class="math inline">\(\Vert f \Vert=\Vert \beta \Vert_2^2=\sum_j \beta_j^2\)</span>.</li>
<li><strong>Lasso penalty</strong>: penalizing the <span class="math inline">\(l_1\)</span> norm of the parameter. I.e., <span class="math inline">\(\Vert f \Vert=\Vert \beta \Vert_1=\sum_j |\beta_j|\)</span></li>
<li><strong>Elastic net</strong>: a combination of the lasso and ridge penalty. I.e. ,<span class="math inline">\(\Vert f \Vert= \alpha \Vert \beta \Vert_2^2 + (a-\alpha) \Vert \beta \Vert_1\)</span>.</li>
</ol>
<p>If the hypothesis class <span class="math inline">\(\mathcal{F}\)</span> does not admit a finite dimensional parametric representation, we may penalize it with some functional norm such as <span class="math inline">\(\Vert f \Vert_2^2=\int f(t)^2 dt\)</span>.</p>
</div>
<div id="unbiased-risk-estimation" class="section level3">
<h3><span class="header-section-number">9.1.3</span> Unbiased Risk Estimation</h3>
<p>The fundamental problem of overfitting, is that the empirical risk, <a href="supervised.html#eq:erm">(9.2)</a>, is downward biased to the true risk <a href="supervised.html#eq:risk">(9.1)</a>, a.k.a. <em>generalization error</em>, and <em>test error</em>. Why is that? Think of estimating a population’s mean with the sample minimum. It can be done, but the minimum has to be debiased for it to estimate the population mean. Debiasing methods broadly fall under purely algorithmic <em>resampling</em> based approaches, and theory driven debiasing corrections. These corrections feel like the penalties above, but we state them here because unlike the ridge, and lasso, they are designed for a different purpose.</p>
<ol style="list-style-type: decimal">
<li><strong>Train,Validate,Test</strong>: The simplest form of validation is to split the data. A <em>train</em> set to train a set of hypotheses. A <em>validation</em> set to compute the out-of-sample expected loss, and pick the best performing hypothesis. A <em>test</em> sample to compute the out-of-sample performance of the selected hypothesis. This is a very simple approach, but it is very “data inefficient”, thus motivating the next method.</li>
<li><strong>V-fold cross validation</strong>: By far the most popular performance assessment algorithm, in <em>V-fold CV</em> we “fold” the data into <span class="math inline">\(V\)</span> non-overlapping sets. For each of the <span class="math inline">\(V\)</span> sets, we fit a hypothesis to the non-selected fold, and assess the expected loss on the selected loss. We then aggregate results over the <span class="math inline">\(V\)</span> folds, typically by averaging.</li>
<li><strong>AIC</strong>: Akaike’s information criterion (AIC) is a theory driven correction of the empirical risk, so that it is unbiased to the true risk. It is appropriate when using the likelihood loss.</li>
<li><strong>Cp</strong>: Mallow’s Cp is an instance of AIC for likelihood loss under normal noise.</li>
</ol>
<p>Other theory driven unbiased risk estimators include the <em>Bayesian Information Criterion</em> (BIC, aka SBC, aka SBIC), the <em>Minimum Description Length</em> (MDL), <em>Vapnic’s Structural Risk Minimization</em> (SRM), the <em>Deviance Information Criterion</em> (DIC), and the <em>Hannan-Quinn Information Criterion</em> (HQC).</p>
<p>Other resampling based unbiased risk estimators include resampling <strong>without replacement</strong> algorithms like <em>delete-d cross validation</em> with its many variations, and <strong>resampling with replacement</strong>, like the <em>bootstrap</em>, with its many variations.</p>
</div>
<div id="collecting-the-pieces" class="section level3">
<h3><span class="header-section-number">9.1.4</span> Collecting the Pieces</h3>
An ERM problem with regularization will look like
<span class="math display" id="eq:erm-regularized">\[\begin{align}
  \hat f := argmin_f \{ \sum_i l(f(x_i),y_i)  + \lambda \Vert f \Vert \}
  \tag{9.3}  
\end{align}\]</span>
<p>Collecting ideas from the above sections, a typical supervised learning pipeline will include: choosing the hypothesis class, choosing the penalty function and level, choosing the assessment algorithm. We emphasize that choosing the penalty function is not enough, and we need to choose how “hard” to apply it. This if known as the <em>regularization level</em>, typically denoted by <span class="math inline">\(\lambda\)</span>, which enters</p>
<p>Examples of such combos include:</p>
<ol style="list-style-type: decimal">
<li>Linear regression, no penalty, train-validate test.</li>
<li>Linear regression, no penalty, AIC.</li>
<li>Linear regression, <span class="math inline">\(l_2\)</span> penalty, V-fold CV. This combo is typically known as <em>ridge regression</em>.</li>
<li>Linear regression, <span class="math inline">\(l_1\)</span> penalty, V-fold CV. This combo is typically known as <em>lasso regression</em>.</li>
<li>Linear regression, <span class="math inline">\(l_1\)</span> and <span class="math inline">\(l_2\)</span> penalty, V-fold CV. This combo is typically known as <em>elastic net regression</em>.</li>
<li>Logistic regression, <span class="math inline">\(l_2\)</span> penalty, V-fold CV.</li>
<li>SVM classification, <span class="math inline">\(l_2\)</span> penalty, V-fold CV.</li>
<li>Deep network, no penalty, V-fold CV.</li>
</ol>
<p>For fans of statistical hypothesis testing we will also emphasize: Testing and prediction are related, but are not the same. <strong>It is indeed possible that we will want to ignore a significant predictor, and add a non-significant one!</strong> <span class="citation">(Foster and Stine <a href="#ref-foster2004variable">2004</a>)</span> Some authors will use hypothesis testing as an initial screening of candidate predictors. This is a useful heuristic, but that is all it is– a heuristic.</p>
</div>
</div>
<div id="supervised-learning-in-r" class="section level2">
<h2><span class="header-section-number">9.2</span> Supervised Learning in R</h2>
<p>At this point, we have a rich enough language to do supervised learning with R.</p>
<p>In these examples, I will use two data sets from the <strong>ElemStatLearn</strong> package: <code>spam</code> for categorical predictions (spam mail or not spam?), and <code>prostate</code> for continuous predictions (size of cancerous tumor). In <code>spam</code> we will try to decide if a mail is spam or not. In <code>prostate</code> we will try to predict the size of a cancerous tumor. You can now call <code>?prostate</code> and <code>?spam</code> to learn more about these data sets.</p>
<p>Some boring pre-processing.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ElemStatLearn) <span class="co"># for data</span>
<span class="kw">data</span>(<span class="st">&quot;prostate&quot;</span>)
<span class="kw">data</span>(<span class="st">&quot;spam&quot;</span>)

<span class="kw">library</span>(magrittr) <span class="co"># for piping</span>

<span class="co"># Preparing prostate data</span>
prostate.train &lt;-<span class="st"> </span>prostate[prostate$train, <span class="kw">names</span>(prostate)!=<span class="st">&#39;train&#39;</span>]
prostate.test &lt;-<span class="st"> </span>prostate[!prostate$train, <span class="kw">names</span>(prostate)!=<span class="st">&#39;train&#39;</span>] 
y.train &lt;-<span class="st"> </span>prostate.train$lcavol
X.train &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(prostate.train[, <span class="kw">names</span>(prostate.train)!=<span class="st">&#39;lcavol&#39;</span>] )
y.test &lt;-<span class="st"> </span>prostate.test$lcavol 
X.test &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(prostate.test[, <span class="kw">names</span>(prostate.test)!=<span class="st">&#39;lcavol&#39;</span>] )

<span class="co"># Preparing spam data:</span>
n &lt;-<span class="st"> </span><span class="kw">nrow</span>(spam)

train.prop &lt;-<span class="st"> </span><span class="fl">0.66</span>
train.ind &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="ot">TRUE</span>,<span class="ot">FALSE</span>) %&gt;%<span class="st">  </span>
<span class="st">  </span><span class="kw">sample</span>(<span class="dt">size =</span> n, <span class="dt">prob =</span> <span class="kw">c</span>(train.prop,<span class="dv">1</span>-train.prop), <span class="dt">replace=</span><span class="ot">TRUE</span>)
spam.train &lt;-<span class="st"> </span>spam[train.ind,]
spam.test &lt;-<span class="st"> </span>spam[!train.ind,]

y.train.spam &lt;-<span class="st"> </span>spam.train$spam
X.train.spam &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(spam.train[,<span class="kw">names</span>(spam.train)!=<span class="st">&#39;spam&#39;</span>] )
y.test.spam &lt;-<span class="st"> </span>spam.test$spam
X.test.spam &lt;-<span class="st">  </span><span class="kw">as.matrix</span>(spam.test[,<span class="kw">names</span>(spam.test)!=<span class="st">&#39;spam&#39;</span>]) 

spam.dummy &lt;-<span class="st"> </span>spam
spam.dummy$spam &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(spam$spam==<span class="st">&#39;spam&#39;</span>) 
spam.train.dummy &lt;-<span class="st"> </span>spam.dummy[train.ind,]
spam.test.dummy &lt;-<span class="st"> </span>spam.dummy[!train.ind,]</code></pre></div>
<p>We also load some utility functions that we will require down the road.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">l2 &lt;-<span class="st"> </span>function(x) x^<span class="dv">2</span> %&gt;%<span class="st"> </span>sum %&gt;%<span class="st"> </span>sqrt 
l1 &lt;-<span class="st"> </span>function(x) <span class="kw">abs</span>(x) %&gt;%<span class="st"> </span>sum  
MSE &lt;-<span class="st"> </span>function(x) x^<span class="dv">2</span> %&gt;%<span class="st"> </span>mean 
missclassification &lt;-<span class="st"> </span>function(tab) <span class="kw">sum</span>(tab[<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)])/<span class="kw">sum</span>(tab)</code></pre></div>
<div id="least-squares" class="section level3">
<h3><span class="header-section-number">9.2.1</span> Linear Models with Least Squares Loss</h3>
<p>Starting with OLS regression, and a train-test data approach. Notice the better in-sample MSE than the out-of-sample. That is overfitting in action.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ols<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">lm</span>(lcavol~. ,<span class="dt">data =</span> prostate.train)
<span class="co"># Train error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(ols<span class="fl">.1</span>)-<span class="st"> </span>prostate.train$lcavol) </code></pre></div>
<pre><code>## [1] 0.4383709</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(ols<span class="fl">.1</span>, <span class="dt">newdata =</span> prostate.test)-<span class="st"> </span>prostate.test$lcavol)</code></pre></div>
<pre><code>## [1] 0.5084068</code></pre>
<p>We now implement a V-fold CV, instead of our train-test approach. The assignment of each observation to each fold is encoded in <code>fold.assignment</code>. The following implementation is extremely inefficient, but easy to read.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">folds &lt;-<span class="st"> </span><span class="dv">10</span>
fold.assignment &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span>:<span class="dv">5</span>, <span class="kw">nrow</span>(prostate), <span class="dt">replace =</span> <span class="ot">TRUE</span>)
errors &lt;-<span class="st"> </span><span class="ot">NULL</span>

for (k in <span class="dv">1</span>:folds){
  prostate.cross.train &lt;-<span class="st"> </span>prostate[fold.assignment!=k,] <span class="co"># train subset</span>
  prostate.cross.test &lt;-<span class="st">  </span>prostate[fold.assignment==k,] <span class="co"># test subset</span>
  .ols &lt;-<span class="st"> </span><span class="kw">lm</span>(lcavol~. ,<span class="dt">data =</span> prostate.cross.train) <span class="co"># train</span>
  .predictions &lt;-<span class="st"> </span><span class="kw">predict</span>(.ols, <span class="dt">newdata=</span>prostate.cross.test)
  .errors &lt;-<span class="st">  </span>.predictions -<span class="st"> </span>prostate.cross.test$lcavol <span class="co"># save prediction errors in the fold</span>
  errors &lt;-<span class="st"> </span><span class="kw">c</span>(errors, .errors) <span class="co"># aggregate error over folds.</span>
}

<span class="co"># Cross validated prediction error:</span>
<span class="kw">MSE</span>(errors)</code></pre></div>
<pre><code>## [1] 0.528486</code></pre>
<p>Let’s try all possible models, and choose the best performer with respect to the Cp criterion. We see that the best performer has 3 predictors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(leaps)
regfit.full &lt;-<span class="st"> </span>prostate.train %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">regsubsets</span>(lcavol~.,<span class="dt">data =</span> ., <span class="dt">method =</span> <span class="st">&#39;exhaustive&#39;</span>) <span class="co"># best subset selection</span>
<span class="kw">plot</span>(regfit.full, <span class="dt">scale =</span> <span class="st">&quot;Cp&quot;</span>)</code></pre></div>
<p><img src="Rcourse_files/figure-html/unnamed-chunk-150-1.png" width="50%" /></p>
<p>Instead of the Cp criterion, we now compute the train and test errors for all the possible predictors<a href="#fn14" class="footnoteRef" id="fnref14"><sup>14</sup></a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model.n &lt;-<span class="st"> </span>regfit.full %&gt;%<span class="st"> </span>summary %&gt;%<span class="st"> </span>length
X.train.named &lt;-<span class="st"> </span>prostate.train %&gt;%<span class="st"> </span><span class="kw">model.matrix</span>(lcavol ~<span class="st"> </span>., <span class="dt">data =</span> .)
X.train.named &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(lcavol ~<span class="st"> </span>., <span class="dt">data =</span> prostate.train ) 
X.test.named &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(lcavol ~<span class="st"> </span>., <span class="dt">data =</span> prostate.test ) 


val.errors &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, model.n)
train.errors &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, model.n)
for (i in <span class="dv">1</span>:model.n) {
    coefi &lt;-<span class="st"> </span><span class="kw">coef</span>(regfit.full, <span class="dt">id =</span> i)
    
    pred &lt;-<span class="st">  </span>X.train.named[, <span class="kw">names</span>(coefi)] %*%<span class="st"> </span>coefi
    train.errors[i] &lt;-<span class="st"> </span><span class="kw">MSE</span>(y.train -<span class="st"> </span>pred)

    pred &lt;-<span class="st">  </span>X.test.named[, <span class="kw">names</span>(coefi)] %*%<span class="st"> </span>coefi
    val.errors[i] &lt;-<span class="st"> </span><span class="kw">MSE</span>(y.test -<span class="st"> </span>pred)
}
<span class="kw">plot</span>(train.errors, <span class="dt">ylab =</span> <span class="st">&quot;MSE&quot;</span>, <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&quot;o&quot;</span>)
<span class="kw">points</span>(val.errors, <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&quot;b&quot;</span>, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)
<span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, 
       <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;Training&quot;</span>, <span class="st">&quot;Validation&quot;</span>), 
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;blue&quot;</span>), 
       <span class="dt">pch =</span> <span class="dv">19</span>)</code></pre></div>
<p><img src="Rcourse_files/figure-html/unnamed-chunk-151-1.png" width="50%" /></p>
<p>Checking all possible models is computationally very hard. <em>Forward selection</em> is a greedy approach that adds one variable at a time, using the AIC criterion. If AIC falls, the variable is added.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ols<span class="fl">.0</span> &lt;-<span class="st"> </span><span class="kw">lm</span>(lcavol~<span class="dv">1</span> ,<span class="dt">data =</span> prostate.train)
model.scope &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">upper=</span>ols<span class="fl">.1</span>, <span class="dt">lower=</span>ols<span class="fl">.0</span>)
<span class="kw">step</span>(ols<span class="fl">.0</span>, <span class="dt">scope=</span>model.scope, <span class="dt">direction=</span><span class="st">&#39;forward&#39;</span>, <span class="dt">trace =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## Start:  AIC=30.1
## lcavol ~ 1
## 
##           Df Sum of Sq     RSS     AIC
## + lpsa     1    54.776  47.130 -19.570
## + lcp      1    48.805  53.101 -11.578
## + svi      1    35.829  66.077   3.071
## + pgg45    1    23.789  78.117  14.285
## + gleason  1    18.529  83.377  18.651
## + lweight  1     9.186  92.720  25.768
## + age      1     8.354  93.552  26.366
## &lt;none&gt;                 101.906  30.097
## + lbph     1     0.407 101.499  31.829
## 
## Step:  AIC=-19.57
## lcavol ~ lpsa
## 
##           Df Sum of Sq    RSS     AIC
## + lcp      1   14.8895 32.240 -43.009
## + svi      1    5.0373 42.093 -25.143
## + gleason  1    3.5500 43.580 -22.817
## + pgg45    1    3.0503 44.080 -22.053
## + lbph     1    1.8389 45.291 -20.236
## + age      1    1.5329 45.597 -19.785
## &lt;none&gt;                 47.130 -19.570
## + lweight  1    0.4106 46.719 -18.156
## 
## Step:  AIC=-43.01
## lcavol ~ lpsa + lcp
## 
##           Df Sum of Sq    RSS     AIC
## &lt;none&gt;                 32.240 -43.009
## + age      1   0.92315 31.317 -42.955
## + pgg45    1   0.29594 31.944 -41.627
## + gleason  1   0.21500 32.025 -41.457
## + lbph     1   0.13904 32.101 -41.298
## + lweight  1   0.05504 32.185 -41.123
## + svi      1   0.02069 32.220 -41.052</code></pre>
<pre><code>## 
## Call:
## lm(formula = lcavol ~ lpsa + lcp, data = prostate.train)
## 
## Coefficients:
## (Intercept)         lpsa          lcp  
##     0.08798      0.53369      0.38879</code></pre>
<p>We now learn a linear predictor on the <code>spam</code> data using, with least squares loss, and train-test validation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># train the predictor</span>
ols<span class="fl">.2</span> &lt;-<span class="st"> </span><span class="kw">lm</span>(spam~., <span class="dt">data =</span> spam.train.dummy) 

<span class="co"># make in-sample predictions</span>
.predictions.train &lt;-<span class="st"> </span><span class="kw">predict</span>(ols<span class="fl">.2</span>) &gt;<span class="st"> </span><span class="fl">0.5</span> 
<span class="co"># inspect the confusion matrix</span>
(confusion.train &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.train, <span class="dt">truth=</span>spam.train.dummy$spam)) </code></pre></div>
<pre><code>##           truth
## prediction    0    1
##      FALSE 1766  266
##      TRUE    81  912</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute the train (in sample) misclassification</span>
<span class="kw">missclassification</span>(confusion.train) </code></pre></div>
<pre><code>## [1] 0.1147107</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># make out-of-sample prediction</span>
.predictions.test &lt;-<span class="st"> </span><span class="kw">predict</span>(ols<span class="fl">.2</span>, <span class="dt">newdata =</span> spam.test.dummy) &gt;<span class="st"> </span><span class="fl">0.5</span> 
<span class="co"># inspect the confusion matrix</span>
(confusion.test &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.test, <span class="dt">truth=</span>spam.test.dummy$spam))</code></pre></div>
<pre><code>##           truth
## prediction   0   1
##      FALSE 902 162
##      TRUE   39 473</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute the train (in sample) misclassification</span>
<span class="kw">missclassification</span>(confusion.test)</code></pre></div>
<pre><code>## [1] 0.1275381</code></pre>
<p>The <code>glmnet</code> package is an excellent package that provides ridge, lasso, and elastic net regularization, for all GLMs, so for linear models in particular.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">suppressMessages</span>(<span class="kw">library</span>(glmnet))
ridge<span class="fl">.2</span> &lt;-<span class="st"> </span><span class="kw">glmnet</span>(<span class="dt">x=</span>X.train, <span class="dt">y=</span>y.train, <span class="dt">family =</span> <span class="st">&#39;gaussian&#39;</span>, <span class="dt">alpha =</span> <span class="dv">0</span>)

<span class="co"># Train error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(ridge<span class="fl">.2</span>, <span class="dt">newx =</span>X.train)-<span class="st"> </span>y.train)</code></pre></div>
<pre><code>## [1] 1.006028</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(ridge<span class="fl">.2</span>, <span class="dt">newx =</span> X.test)-<span class="st"> </span>y.test)</code></pre></div>
<pre><code>## [1] 0.7678264</code></pre>
<p>Things to note:</p>
<ul>
<li>The <code>alpha=0</code> parameters tells R to do ridge regression. Setting <span class="math inline">\(alpha=1\)</span> will do lasso, and any other value, with return an elastic net with appropriate weights.</li>
<li>The `family=‘gaussian’ argument tells R to fit a linear model, with least squares loss.</li>
</ul>
<p>We now use the lasso penalty.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lasso<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">glmnet</span>(<span class="dt">x=</span>X.train, <span class="dt">y=</span>y.train, , <span class="dt">family=</span><span class="st">&#39;gaussian&#39;</span>, <span class="dt">alpha =</span> <span class="dv">1</span>)

<span class="co"># Train error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(lasso<span class="fl">.1</span>, <span class="dt">newx =</span>X.train)-<span class="st"> </span>y.train)</code></pre></div>
<pre><code>## [1] 0.5525279</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(lasso<span class="fl">.1</span>, <span class="dt">newx =</span> X.test)-<span class="st"> </span>y.test)</code></pre></div>
<pre><code>## [1] 0.5211263</code></pre>
<p>We now use <code>glmnet</code> for classification.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">logistic<span class="fl">.2</span> &lt;-<span class="st"> </span><span class="kw">cv.glmnet</span>(<span class="dt">x=</span>X.train.spam, <span class="dt">y=</span>y.train.spam, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>, <span class="dt">alpha =</span> <span class="dv">0</span>)</code></pre></div>
<p>Things to note:</p>
<ul>
<li>We used <code>cv.glmnet</code> to do an automatic search for the optimal level of regularization (the <code>lambda</code> argument in <code>glmnet</code>).</li>
<li>We set <code>alpha=0</code> for ridge regression.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Train confusion matrix:</span>
.predictions.train &lt;-<span class="st"> </span><span class="kw">predict</span>(logistic<span class="fl">.2</span>, <span class="dt">newx =</span> X.train.spam, <span class="dt">type =</span> <span class="st">&#39;class&#39;</span>) 
(confusion.train &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.train, <span class="dt">truth=</span>spam.train$spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email  1767  203
##      spam     80  975</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Train misclassification error</span>
<span class="kw">missclassification</span>(confusion.train)</code></pre></div>
<pre><code>## [1] 0.09355372</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test confusion matrix:</span>
.predictions.test &lt;-<span class="st"> </span><span class="kw">predict</span>(logistic<span class="fl">.2</span>, <span class="dt">newx =</span> X.test.spam, <span class="dt">type=</span><span class="st">&#39;class&#39;</span>) 
(confusion.test &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.test, <span class="dt">truth=</span>y.test.spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email   896  117
##      spam     45  518</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test misclassification error:</span>
<span class="kw">missclassification</span>(confusion.test)</code></pre></div>
<pre><code>## [1] 0.1027919</code></pre>
</div>
<div id="svm" class="section level3">
<h3><span class="header-section-number">9.2.2</span> SVM</h3>
<p>A support vector machine (SVM) is a linear model with a particular loss function known as a <em>hinge loss</em>. We learn an SVM with the <code>svm</code> function from the <strong>e1071</strong> package, which is merely a wrapper for the <strong>libsvm</strong> C library, which is the most popular implementation of SVM today.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(e1071)
svm<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">svm</span>(spam~., <span class="dt">data =</span> spam.train)

<span class="co"># Train confusion matrix:</span>
.predictions.train &lt;-<span class="st"> </span><span class="kw">predict</span>(svm<span class="fl">.1</span>) 
(confusion.train &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.train, <span class="dt">truth=</span>spam.train$spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email  1793   99
##      spam     54 1079</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.train)</code></pre></div>
<pre><code>## [1] 0.05057851</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test confusion matrix:</span>
.predictions.test &lt;-<span class="st"> </span><span class="kw">predict</span>(svm<span class="fl">.1</span>, <span class="dt">newdata =</span> spam.test) 
(confusion.test &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.test, <span class="dt">truth=</span>spam.test$spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email   904   76
##      spam     37  559</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.test)</code></pre></div>
<pre><code>## [1] 0.07170051</code></pre>
<p>We can also use SVM for regression.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">svm<span class="fl">.2</span> &lt;-<span class="st"> </span><span class="kw">svm</span>(lcavol~., <span class="dt">data =</span> prostate.train)

<span class="co"># Train error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(svm<span class="fl">.2</span>)-<span class="st"> </span>prostate.train$lcavol)</code></pre></div>
<pre><code>## [1] 0.3336868</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(svm<span class="fl">.2</span>, <span class="dt">newdata =</span> prostate.test)-<span class="st"> </span>prostate.test$lcavol)</code></pre></div>
<pre><code>## [1] 0.5633183</code></pre>
</div>
<div id="neural-nets" class="section level3">
<h3><span class="header-section-number">9.2.3</span> Neural Nets</h3>
<p>Neural nets (non deep) can be fitted, for example, with the <code>nnet</code> function in the <strong>nnet</strong> package. We start with a nnet regression.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(nnet)
nnet<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">nnet</span>(lcavol~., <span class="dt">size=</span><span class="dv">20</span>, <span class="dt">data=</span>prostate.train, <span class="dt">rang =</span> <span class="fl">0.1</span>, <span class="dt">decay =</span> <span class="fl">5e-4</span>, <span class="dt">maxit =</span> <span class="dv">1000</span>, <span class="dt">trace=</span><span class="ot">FALSE</span>)

<span class="co"># Train error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(nnet<span class="fl">.1</span>)-<span class="st"> </span>prostate.train$lcavol)</code></pre></div>
<pre><code>## [1] 1.189929</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(nnet<span class="fl">.1</span>, <span class="dt">newdata =</span> prostate.test)-<span class="st"> </span>prostate.test$lcavol)</code></pre></div>
<pre><code>## [1] 1.221043</code></pre>
<p>And nnet classification.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nnet<span class="fl">.2</span> &lt;-<span class="st"> </span><span class="kw">nnet</span>(spam~., <span class="dt">size=</span><span class="dv">5</span>, <span class="dt">data=</span>spam.train, <span class="dt">rang =</span> <span class="fl">0.1</span>, <span class="dt">decay =</span> <span class="fl">5e-4</span>, <span class="dt">maxit =</span> <span class="dv">1000</span>, <span class="dt">trace=</span><span class="ot">FALSE</span>)

<span class="co"># Train confusion matrix:</span>
.predictions.train &lt;-<span class="st"> </span><span class="kw">predict</span>(nnet<span class="fl">.2</span>, <span class="dt">type=</span><span class="st">&#39;class&#39;</span>) 
(confusion.train &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.train, <span class="dt">truth=</span>spam.train$spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email  1818   33
##      spam     29 1145</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.train)</code></pre></div>
<pre><code>## [1] 0.02049587</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test confusion matrix:</span>
.predictions.test &lt;-<span class="st"> </span><span class="kw">predict</span>(nnet<span class="fl">.2</span>, <span class="dt">newdata =</span> spam.test, <span class="dt">type=</span><span class="st">&#39;class&#39;</span>) 
(confusion.test &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.test, <span class="dt">truth=</span>spam.test$spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email   895   61
##      spam     46  574</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.test)</code></pre></div>
<pre><code>## [1] 0.0678934</code></pre>
</div>
<div id="classification-and-regression-trees-cart" class="section level3">
<h3><span class="header-section-number">9.2.4</span> Classification and Regression Trees (CART)</h3>
<p>A CART, is not a linear model. It partitions the feature space <span class="math inline">\(\mathcal{X}\)</span>, thus creating a set of if-then rules for prediction or classification. This view clarifies the name of the function <code>rpart</code>, which <em>recursively partitions</em> the feature space.</p>
<p>We start with a regression tree.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rpart)
tree<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">rpart</span>(lcavol~., <span class="dt">data=</span>prostate.train)

<span class="co"># Train error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(tree<span class="fl">.1</span>)-<span class="st"> </span>prostate.train$lcavol)</code></pre></div>
<pre><code>## [1] 0.4909568</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(tree<span class="fl">.1</span>, <span class="dt">newdata =</span> prostate.test)-<span class="st"> </span>prostate.test$lcavol)</code></pre></div>
<pre><code>## [1] 0.5623316</code></pre>
<p>Tree are very prone to overfitting. To avoid this, we reduce a tree’s complexity by <em>pruning</em> it. This is done with the <code>prune</code> function.</p>
<p>We now fit a classification tree.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tree<span class="fl">.2</span> &lt;-<span class="st"> </span><span class="kw">rpart</span>(spam~., <span class="dt">data=</span>spam.train)

<span class="co"># Train confusion matrix:</span>
.predictions.train &lt;-<span class="st"> </span><span class="kw">predict</span>(tree<span class="fl">.2</span>, <span class="dt">type=</span><span class="st">&#39;class&#39;</span>) 
(confusion.train &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.train, <span class="dt">truth=</span>spam.train$spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email  1734  172
##      spam    113 1006</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.train)</code></pre></div>
<pre><code>## [1] 0.09421488</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test confusion matrix:</span>
.predictions.test &lt;-<span class="st"> </span><span class="kw">predict</span>(tree<span class="fl">.2</span>, <span class="dt">newdata =</span> spam.test, <span class="dt">type=</span><span class="st">&#39;class&#39;</span>) 
(confusion.test &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.test, <span class="dt">truth=</span>spam.test$spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email   874  104
##      spam     67  531</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.test)</code></pre></div>
<pre><code>## [1] 0.1085025</code></pre>
</div>
<div id="k-nearest-neighbour-knn" class="section level3">
<h3><span class="header-section-number">9.2.5</span> K-nearest neighbour (KNN)</h3>
<p>KNN is not an ERM problem. For completeness, we still show how to fit such a hypothesis.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(class)
knn<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">knn</span>(<span class="dt">train =</span> X.train.spam, <span class="dt">test =</span> X.test.spam, <span class="dt">cl =</span>y.train.spam, <span class="dt">k =</span> <span class="dv">1</span>)

<span class="co"># Test confusion matrix:</span>
.predictions.test &lt;-<span class="st"> </span>knn<span class="fl">.1</span> 
(confusion.test &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.test, <span class="dt">truth=</span>spam.test$spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email   797  143
##      spam    144  492</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.test)</code></pre></div>
<pre><code>## [1] 0.1821066</code></pre>
</div>
<div id="linear-discriminant-analysis-lda" class="section level3">
<h3><span class="header-section-number">9.2.6</span> Linear Discriminant Analysis (LDA)</h3>
<p>LDA is equivalent to least squares classification <a href="supervised.html#least-squares">9.2.1</a>. There are, however, some dedicated functions to fit it.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(MASS) 
lda<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">lda</span>(spam~., spam.train)

<span class="co"># Train confusion matrix:</span>
.predictions.train &lt;-<span class="st"> </span><span class="kw">predict</span>(lda<span class="fl">.1</span>)$class
(confusion.train &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.train, <span class="dt">truth=</span>spam.train$spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email  1765  261
##      spam     82  917</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.train)</code></pre></div>
<pre><code>## [1] 0.1133884</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test confusion matrix:</span>
.predictions.test &lt;-<span class="st"> </span><span class="kw">predict</span>(lda<span class="fl">.1</span>, <span class="dt">newdata =</span> spam.test)$class
(confusion.test &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.test, <span class="dt">truth=</span>spam.test$spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email   899  161
##      spam     42  474</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.test)</code></pre></div>
<pre><code>## [1] 0.1288071</code></pre>
</div>
<div id="naive-bayes" class="section level3">
<h3><span class="header-section-number">9.2.7</span> Naive Bayes</h3>
<p>A Naive-Bayes classifier is also not part of the ERM framework. It is, however, very popular, so we present it.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(e1071)
nb<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">naiveBayes</span>(spam~., <span class="dt">data =</span> spam.train)

<span class="co"># Train confusion matrix:</span>
.predictions.train &lt;-<span class="st"> </span><span class="kw">predict</span>(nb<span class="fl">.1</span>, <span class="dt">newdata =</span> spam.train)
(confusion.train &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.train, <span class="dt">truth=</span>spam.train$spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email  1005   62
##      spam    842 1116</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.train)</code></pre></div>
<pre><code>## [1] 0.298843</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test confusion matrix:</span>
.predictions.test &lt;-<span class="st"> </span><span class="kw">predict</span>(nb<span class="fl">.1</span>, <span class="dt">newdata =</span> spam.test)
(confusion.test &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.test, <span class="dt">truth=</span>spam.test$spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email   521   35
##      spam    420  600</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.test)</code></pre></div>
<pre><code>## [1] 0.2887056</code></pre>
</div>
</div>
<div id="bibliographic-notes-5" class="section level2">
<h2><span class="header-section-number">9.3</span> Bibliographic Notes</h2>
<p>The ultimate reference on (statistical) machine learning is <span class="citation">Friedman, Hastie, and Tibshirani (<a href="#ref-friedman2001elements">2001</a>)</span>. For a softer introduction, see <span class="citation">James et al. (<a href="#ref-james2013introduction">2013</a>)</span>. A statistician will also like <span class="citation">Ripley (<a href="#ref-ripley2007pattern">2007</a>)</span>. For an R oriented view see <span class="citation">Lantz (<a href="#ref-lantz2013machine">2013</a>)</span>. For a very algorithmic view, see the seminal <span class="citation">Leskovec, Rajaraman, and Ullman (<a href="#ref-leskovec2014mining">2014</a>)</span> or <span class="citation">Conway and White (<a href="#ref-conway2012machine">2012</a>)</span>. For a much more theoretical reference, see <span class="citation">Mohri, Rostamizadeh, and Talwalkar (<a href="#ref-mohri2012foundations">2012</a>)</span>, <span class="citation">Vapnik (<a href="#ref-vapnik2013nature">2013</a>)</span>, <span class="citation">Shalev-Shwartz and Ben-David (<a href="#ref-shalev2014understanding">2014</a>)</span>. Terminology taken from <span class="citation">Sammut and Webb (<a href="#ref-sammut2011encyclopedia">2011</a>)</span>. For a review of resampling based unbiased risk estimation (i.e. cross validation) see the exceptional review of <span class="citation">Arlot, Celisse, and others (<a href="#ref-arlot2010survey">2010</a>)</span>.</p>

</div>
</div>
<h3> Bibliography</h3>
<div id="refs" class="references">
<div id="ref-foster2004variable">
<p>Foster, Dean P, and Robert A Stine. 2004. “Variable Selection in Data Mining: Building a Predictive Model for Bankruptcy.” <em>Journal of the American Statistical Association</em> 99 (466). Taylor &amp; Francis: 303–13.</p>
</div>
<div id="ref-friedman2001elements">
<p>Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. <em>The Elements of Statistical Learning</em>. Vol. 1. Springer series in statistics Springer, Berlin.</p>
</div>
<div id="ref-james2013introduction">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 6. Springer.</p>
</div>
<div id="ref-ripley2007pattern">
<p>Ripley, Brian D. 2007. <em>Pattern Recognition and Neural Networks</em>. Cambridge university press.</p>
</div>
<div id="ref-lantz2013machine">
<p>Lantz, Brett. 2013. <em>Machine Learning with R</em>. Packt Publishing Ltd.</p>
</div>
<div id="ref-leskovec2014mining">
<p>Leskovec, Jure, Anand Rajaraman, and Jeffrey David Ullman. 2014. <em>Mining of Massive Datasets</em>. Cambridge University Press.</p>
</div>
<div id="ref-conway2012machine">
<p>Conway, Drew, and John White. 2012. <em>Machine Learning for Hackers</em>. “ O’Reilly Media, Inc.”</p>
</div>
<div id="ref-mohri2012foundations">
<p>Mohri, Mehryar, Afshin Rostamizadeh, and Ameet Talwalkar. 2012. <em>Foundations of Machine Learning</em>. MIT press.</p>
</div>
<div id="ref-vapnik2013nature">
<p>Vapnik, Vladimir. 2013. <em>The Nature of Statistical Learning Theory</em>. Springer science &amp; business media.</p>
</div>
<div id="ref-shalev2014understanding">
<p>Shalev-Shwartz, Shai, and Shai Ben-David. 2014. <em>Understanding Machine Learning: From Theory to Algorithms</em>. Cambridge university press.</p>
</div>
<div id="ref-sammut2011encyclopedia">
<p>Sammut, Claude, and Geoffrey I Webb. 2011. <em>Encyclopedia of Machine Learning</em>. Springer Science &amp; Business Media.</p>
</div>
<div id="ref-arlot2010survey">
<p>Arlot, Sylvain, Alain Celisse, and others. 2010. “A Survey of Cross-Validation Procedures for Model Selection.” <em>Statistics Surveys</em> 4. The author, under a Creative Commons Attribution License: 40–79.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="13">
<li id="fn13"><p>It is even a subset of the Hilbert space, itself a subset of the space of all functions.<a href="supervised.html#fnref13">↩</a></p></li>
<li id="fn14"><p>Example taken from <a href="https://lagunita.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/ch6.html" class="uri">https://lagunita.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/ch6.html</a><a href="supervised.html#fnref14">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="multivariate.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="unsupervised.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/08-supervised.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
