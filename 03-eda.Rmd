# Exploratory Data Analysis {#eda}


Exploratory Data Analysis (EDA) is a term cast by [John W. Tukey](https://en.wikipedia.org/wiki/John_Tukey) in his seminal book @tukey1977exploratory.
It is the practice of inspecting, exploring your data before stating hypotheses, fitting predictors, and other more ambitious inferential goals.
It typically includes the computation of simple _summary statistics_ which capture some property of interest in the data, and _visualization_.
EDA can be thought of as an assumption free, purely algorithmic practice.

In this text we present EDA techniques along the following lines:

- How we explore: with a summary statistic or visually.
- How many variable analyzed simultaneously: univariate, bivariate, or multivariate.
- What type of variable: categorical or continuous.


## Summary Statistics

### Categorical Data
Categorical variable do not admit any mathematical operations on them. 
We cannot sum them, or even sort them. 
We can only __count__ them. 
As such, summaries of categorical variables will always start with the counting of the frequency of each category.

#### Summary of Univariate Categorical Data

```{r}
gender <- c(rep('Boy', 10), rep('Girl', 12))
drink <- c(rep('Coke', 5), rep('Sprite', 3), rep('Coffee', 6), rep('Tea', 7), rep('Water', 1))  
age <-  sample(c('Young', 'Old'), size = length(gender), replace = TRUE)

table(gender)
table(drink)
table(age)
```

If instead of the level counts you want the proportions, you can use `prop.table`

```{r}
prop.table(table(gender))
```


#### Summary of Bivariate Categorical Data

```{r}
library(magrittr)
cbind(gender, drink) %>% head # inspect the raw data
table1 <- table(gender, drink) 
table1										
```

#### Summary of Multivariate Categorical Data
You may be wondering how does R handle tables with more than two dimensions.
It is indeed not trivial, and R offers several solutions.

```{r}
table2.1 <- table(gender, drink, age) # A multilevel table. 
table2.1
table.2.2 <- ftable(gender, drink, age) # A human readable table.
table.2.2
```

If you want proportions instead of counts, you need to specify the denominator, i.e., the margins. 

```{r}
prop.table(table1, margin = 1)
prop.table(table1, margin = 2)
```


### Continous Data
Continuous variables admit many more operations than categorical.
We can thus compute sums, means, quantiles, and more.

#### Summary of Univariate Continous Data
We distinguish between several types of summaries, each capturing a different property of the data.

#### Summary of Location
Capture the "location" of the data. These include:

```{definition, name="Average"}
The mean, or average, of a sample $x$ of lenth $n$, denoted $\bar x$ is defined as 
$$ \bar x := n^{-1} \sum x_i $$
```

The sample mean is __non robust__. 
A single large observation may inflate the mean indefinitely.
For this reason, we define several other summaries of location, which are more robust, i.e., less affected by "contaminations" of the data.

We start by defining the sample quantiles, themselves __not__ a summary of location.

```{definition, name="Quantiles"}
The $\alpha$ quantile of a sample $x$, denoted $x_\alpha$, is (non uniquely) defined as a value above $100 \alpha \%$ of the sample, and below $100 (1-\alpha) \%$.
```

We emphasize that sample quantiles are non-uniquely defined. See `?quantile` for the 9(!) different definitions that R provides. 

We can now define another summary of location, the median.

```{definition, name="Median"}
The median of a sample $x$, denoted $x_{0.5}$ is the $\alpha=0.5$ quantile of the sample.
```

A whole family of summaries of locations is the __alpha trimmed mean__.

```{definition, name="Alpha Trimmed Mean"}
The $\alpha$ trimmed mean of a sample $x$, denoted $\bar x_\alpha$ is the average of the sample after removing the $\alpha$ largest and $\alpha$ smallest observations.
```

The simple mean and median are instances of the alpha trimmed mean: $\bar x_0$ and $\bar x_{0.5}$ respectively.

Here are the R implementations:

```{r}
x <- rexp(100)
mean(x) # simple mean
median(x) # median
mean(x, trim = 0.2) # alpha trimmed mean with alpha=0.2
```


#### Summary of Scale
The _scale_ of the data, sometimes known as _spread_, can be thought of its variability. 

```{definition, name="Standard Deviation"}
The standard deviation of a sample $x$, denoted $S(x)$, is defined as 
$$ S(x):=\sqrt{(n-1)^{-1} \sum (x_i-\bar x)^2} $$
```

For reasons of robustness, we define other, more robust, measures of scale.

```{definition, name="MAD"}
The Median Absolute Deviation from the median, denoted as $MAD(x)$, is defined as
$$MAD(x):= c \: |x-x_{0.5}|_{0.5} $$
```

where $c$ is some constant, typically set to $c=1.4826$ so that the MAD is a robust estimate of $S(x)$.

```{definition, name="IQR"}
The Inter Quantile Range of a sample $x$, denoted as $IQR(x)$, is defined as 
$$ IQR(x):= x_{0.75}-x_{0.25} $$
```

Here are the R implementations

```{r}
sd(x) # standard deviation
mad(x) # MAD
IQR(x) # IQR
```



#### Summary of Asymmetry
The symmetry of a univariate sample is easily understood.
Summaries of asymmetry, also known as _skewness_ quantify the departure of the $x$ from a symmetric distribution.

```{definition, name="Yule"}
The Yule measure of assymetry, denoted $Yule(x)$ is defined as 
$$Yule(x) := \frac{1/2 \: (x_{0.75}+x_{0.25}) - x_{0.5} }{1/2 \: IQR(x)} $$
```

Here is an R implementation

```{r yule}
yule <- function(x){
  numerator <- 0.5 * (quantile(x,0.75) + quantile(x,0.25))-median(x) 
  denominator <- 0.5* IQR(x)
  numerator/denominator
}
yule(x)
```


#### Summary of Bivariate Continous Data
When dealing with bivariate, or multivariate data, we can obviously compute univariate summaries for each variable. 
This is __not__ the topic of this section, in which we want to summarize the association __between__ the variables, and not withing them.

```{definition, name="Covariance"}
The covariance between two samples, $x$ and $y$, of same length $n$, is defined as 
$$Cov(x,y):= (n-1)^{-1} \sum (x_i-\bar x)(y_i-\bar y)  $$
```

We emphasize this is not the covariance you learned about in probability classes, since it is not the covariance between two _random variables_ but rather, between two _samples_. 
For this reasons, some authors call it the _empirical_ covariance. 

```{definition, name="Pearson's Correlation Coefficient"}
Peasrson's correlation coefficient, a.k.a. Pearson's moment product correlation, or simply, the correlation, denoted by is defined as 
$$r(x,y):=\frac{Cov(x,y)}{S(x)S(y)} $$
```

If you find this definition enigmatic, just think of the correlation as the covariance between $x$ and $y$ after transforming each to the unitless scale of z-scores.

```{definition, name="Z-Score"}
The z-scores of a sample $x$ are defined as the mean-centered, scale normalized observations:
$$z_i(x):= \frac{x_i-\bar x}{S(x)}$$ 
```

We thus have that $r(x,y)=Cov(z(x),z(y))$. 


#### Summary of Multivariate Continous Data

The covariance is a simple summary of association between two variables, but it certainly may not capture the whole "story".
Things get more complicated when summarizing the relation between multiple variables. 
The most common summary of relation, is the __covariance matrix__, but we warn that only the simplest multivariate relations are fully summarized by this matrix. 

```{definition, name="Sample Covariance Matrix"}
Given $n$ observations on $p$ variables, the _sample covariance matrix_, denoted $\hat \Sigma$ is defined as 
$$\hat \Sigma_{i,j}=Cov(x_i,x_j)$$
where $x_i,x_j$ are the $n$ observations on variables $x_i$ and $x_j$ respectively.
```

```{remark}
$\hat \Sigma$ is clearly non robust.
How would you define a robust covariance matrix?
```





## Visualization
Summarizing the story in a variable to a single number clearly conceals much of the story in the data. 
This is akin to inspecting a person by its caricature, instead of a picture. 
Visualizing the data, when possible, is more informative. 

### Categorical Data
Recalling that with categorical variables we can only count the frequency of each level, the plotting of such variables are typically variations on the _bar plot_.

#### Visualizing Univariate Categorical Data

```{r, barplot}
plot(table(age))
```



#### Visualizing Bivariate Categorical Data
There are several generalizations of the barplot, aimed to deal with the visualization of bivariate categorical data. 
There are sometimes known as the _clustered bar plot_ and the _stacked bar plot_.
In this text, we advocate the use of the _mosaic plot_ which is also the default in R.
```{r}
plot(table1, main='Bivariate mosaic plot')
```


#### Visualizing Multivariate Categorical Data 
The _mosaic plot_ is not easy to generalize to more than two variables, but it is still possible (at the cost of interpretability).

```{r}
plot(table2.1, main='Trivaraite mosaic plot')
```


### Continous Data

#### Visualizing Univariate Continous Data
There are endlessly many way to visualize continuous univariate data.
The simplest way is to look at the raw data via the `stripcart`.

```{r}
sample1 <- rexp(10) 							
stripchart(sample1)
```

Clearly, if there are many observations, the `stripchart` will be a useless line of black dots. 
We thus bin them together, and look at the frequency of each bin; this is the _histogram_.
R's `histogram` function has very good defaults to choose the number of bins.
```{r}
sample1 <- rexp(100) 							
hist(sample1, freq=T, main='Counts')      	
hist(sample1, freq=F, main='Frequencies') 	
```

The bins of a histogram are non overlapping.
We can adopt a sliding window approach, instead of binning. 
This is the _density plot_ which is produce with the `density` functions, and added to an existing plot with the `lines` function.
The `rug` function adds the original data points as ticks on the axes, and is strongly recommended to detect artifacts due to the binning of the histogram, or the smoothing of the density plot. 

```{r, results='hold'}
hist(sample1, freq=F, main='Frequencies') 	
lines(density(sample1))                   	
rug(sample1)
```

```{remark}
Why would it make no sense of making a table, or a barplot, of continous data?
```

One particularly useful visualization, due to John w. Tukey, is the _boxplot_.
The boxplot is designed to capture the main phenomena in the data, and simultaneously point to outlines. 

```{r}
boxplot(sample1)	
```


#### Visualizing Bivariate Continous Data
The bivariate counterpart of the `stipchart` is the celebrated scatter plot. 
```{r}
n <- 20
x1 <- rexp(n)
x2 <- 2* x1 + 4 + rexp(n)
plot(x2~x1)
```

Like the univariate `stripchart`, the scatter plot will be an uninformative mess in the presence of a lot of data. 
A nice bivariate counterpart of the univariate histogram is the _hexbin plot_, which tessellates the bivariate plane with hexagons.

```{r}
library(hexbin)
n <- 2e5
x1 <- rexp(n)
x2 <- 2* x1 + 4 + rnorm(n)
plot(hexbin(x = x1, y = x2))
```



#### Visualizing Multivariate Continous Data
Visualizing multivariate data is a tremendous challenge given that we cannot grasp $4$ dimensional spaces, nor can the computer screen present more than $2$ dimensional spaces.
We thus have several options: 
(i) To project the data to 2D. This is discussed in the Dimensionality Reduction section (\@ref(dim-reduce)).
(ii) To visualize not the data, but the summaries. Like the covariance matrix.

Since the covariance matrix, $\hat \Sigma$ is a matrix, it can be visualized as a matrix.

```{r}
covariance <- cov(longley) # The covariance of the longley dataset
lattice::levelplot(covariance)
```



## Bibliographic Notes

Like any other topic in this book, you can consult @venables2013modern.
The seminal book on EDA, written long before R was around, is @tukey1977exploratory.



## Practice Yourself
