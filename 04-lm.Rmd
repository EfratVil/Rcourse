# Linear Models {#lm}

## Problem Setup

```{example}
Consider a randomized experiment designed to study the effects of temperature and pressure on the  diameter of a bottle cap.
```

```{example}
Consider the prediction of rental prices given an appartment's attributes. 
```

Both examples require some statistical model, but they are very different.
The first is a _causal inference_ problem: we want to design an intervention so that we need to recover the causal effect of temperature and pressure.
The second is a _prediction_ problem. We don't care about the causal effects, we just want good predictions.

In this chapter we discuss the causal problem in 
This means that when we assume a model, we assume it is the actual _data generating process_.
The second type of problems is discussed in the Supervised Learning Chapter (\@ref(supervised)).


Lets present the linear model.
We assume that a response^[The "response" is also know as the "dependent" variable, of the "labels" in the machine learning literature.] variable is the sum of effects of some factors^[The "factors" are also known as the "independent variable", the "design", the "features" and the "attributes".].
Denoting the dependent by $y$, the factors by $x$, and the effects by $\beta$ the linear model assumption implies that 


\begin{align}
  E[y]=\sum_j x_j \beta_j=x'\beta .
  (\#eq:linear-mean)
\end{align}
Clearly, there may be other factors that affet the the caps' diameters. 
We thus introduce an error term^[The "error term" is also known as the "noise", or the "common causes of variability".], denoted by $\varepsilon$, to capture the effects of all unmodeled factors.
The implied generative process of a sample of $i=1,\dots,n$ observations it thus
\begin{align}
  y_i = \sum x_{i,j} \beta_j + \varepsilon_i , i=1,\dots,n .
  (\#eq:linear-observed)
\end{align}
or in matrix notation
\begin{align}
  y = X \beta + \varepsilon .
  (\#eq:linear-matrix)
\end{align}


Lets demonstrate Eq.\@ref(eq:linear-observed):

In our cap example, assuming that pressure and temperature have two levels each (say, high and low), we would write $x_{i,1}=1$ if the pressure of the $i$'th measurement was set to high, and $x_{i,1}=-1$ if the pressure was set to low. 
Similarly, we would write $x_{i,2}=1$, and $x_{i,2}=-1$, if the temperature was set to high, or low, respectively.
The coding with $\{-1,1\}$ is known as _effect coding_.
If you prefer coding with $\{0,1\}$, this is known as _dummy coding_.

In Gosset's classical regression problem, where we try to seek the relation between the heights of sons and fathers then $p=1$, $y_i$ is the height of the $i$'th father, and $x_i$ the height of the $i$'th son.


There are many reasons these models are so popular:

1. Before the computer age, these were pretty much the only models that could actually be computed^[By "computed" we mean what statisticians call "fitted", or "estimated", and computer scientists call "learned".]. 
The whole Analysis of Variance (ANOVA) literature is an instance of linear models.

1. For purposes of prediction, where the actual data generating process is not of primary importance, they are popular because they simply work. 
Why is that? 
They are simple so that they do not require a lot of data to be computed. 
Put differnetly, they may be biased, but their variance is small enough to make them more accurate than other models.

1. For categorical or factorial predictors, __any__ functional relation can be cast as a linear model.

1. For the purpose of _screening_, where we only want to show the existance of an effect, and are less interested in the magnitude of the effect, a linear model is enough.

1. If the true generative relation is not linear, but smooth enough, then the linear function is a good approximation via Taylor's theorem.


There are still two matters we have to attend: 
How the estimate $\beta$, and how to perform inference.

In linear models the estimation of $\beta$ is done using the method of least squares. 
For this reason, a linear model with least squares estimation is known as Ordinary Least Sqaures (OLS).
The OLS problem:

\begin{align}
  \hat \beta_{OLS}:= \argmin_\beta \{ \sum_i (y_i-x_i'\beta)^2 \},
  (\#eq:ols)
\end{align}
and in matrix notation
\begin{align}
  \hat \beta_{OLS}:= \argmin_\beta \{ \Vert y-X\beta \Vert^2_2 \}.
  (\#eq:ols-matrix)
\end{align}

```{remark}
Personally, I prefer the matrix notation because it suggests of the geometry of the problem. 
The reader is referred to @friedman2001elements, Sec 3.2, for more on the geometry of OLS.
```

Different software suits solve Eq\@ref(ols) in different ways so that we skip the details of how exactly it is solved. 

The last matter we need to attend is how to do inference on $\hat \beta_{OLS}$.
For that, we will need some assumptions on $\varepsilon$.
A typical set of assumptions is the following:

1. __Independence__: we assume $\varepsilon_i$ are independent of everything else. 
Think of them as the measurement error of an instrument: it is indepndent of the measured value and of previous measurements. 
1. __Centered__: we assume that $E[\varepsilon]=0$, meaning there is no systematics error. 
1. __Normality__: we will typically assume that $\varepsilon \sim \mathcal{N}(0,\sigma^2)$, but we will later see that this is not really required. 

We emphasize that these assumptions are only needed for inference on $\hat \beta$ and not for the estimation itself, which is done by the purely algorithmic framwork of OLS.



## OLS Estimation 

We are now ready to estimate some linear models with R.
We will use the `whiteside` data from the __MASS__ package,recording the outside temperature and gas consumption, before and after insulation.
```{r, cache=TRUE}
library(MASS)
head(whiteside) # inspect the data
```
We do the OLS estimation with `lm` function, possibly the most important function in R.
```{r}
lm.1 <- lm(Gas~Temp, data=whiteside[whiteside$Insul=='Before',]) # OLS estimation 
```
Things to note:

- We used the tilde syntax `Gas~Temp`, reading "gas as linear function of temperature".
- The `data` argument tells R where to look for the variables Gas and Temp.
We used only observations before the insulation.
- The result is assigned to the object `lm.1`.

Alternative formulations with the same results would be
```{r, eval=FALSE}
lm.1 <- lm(y=Gas, x=Temp, data=whiteside[whiteside$Insul=='Before',]) 
lm.1 <- lm(y=whiteside[whiteside$Insul=='Before',]$Gas, x=whiteside[whiteside$Insul=='Before',]$Temp)  
```

The output is an object of class `lm`.
```{r}
class(lm.1)
```
Objects of class `lm` are very complicated. 
It stored a lot of information which will be later used for inference, plotting, etc.
The `str` function, short for "structure" shows us the various elements of the object.
```{r}
str(lm.1)
```
At this point, we only want $\hat \beta_{OLS}$ which can be extracted with the `coef` function.
```{r}
coef(lm.1)
```

Things to note:

- R automatically adds an `(Intercept)` term. 
This means we estimate $y_i=\beta_0 + \beta_1 Gas + \varepsilon$ and not $y_i=\beta_1 Gas + \varepsilon$. 
This makes sense because we are interested in the variability of the gas consumption about its mean, and not about zero.

- The effect of temperature, i.e., $\hat \beta_1$, is `r coef(lm.1)[[2]]`. 
The negative sign means that the higher the temperature, the less gas is consumed. 

