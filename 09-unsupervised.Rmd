# Unsupervised Learning {#unsupervised}

This chapter deals with machine learning problems which are unsupervised.
This means the machine has access to a set of inputs, $x$, but the desired outcome, $y$ is not available. 
Clearly, learning a relation between inputs and outcomes is impossible, but there are still a lot of problems of interest. 
In particular, we may want to find a compact representation of the inputs, be it for visualization of further processing. This is the problem of _dimensionality reduction_.
For the same reasons we may want to group similar inputs. This is the problem of _clustering_.

In the statistical terminology, and with some exceptions, this chapter can be thought of as multivariate __exploratory__ statistics.
For multivariate __inference__, see Chapter \@ref(multivariate).



## Dimensionality Reduction {#dim-reduce}

```{example, label='bmi'}
Consider the heights and weights of a sample of individuals. 
The data may seemingly reside in $2$ dimensions but given the height, we have a pretty good guess of a persons weight, and vice versa. 
We can thus state that heights and weights are not really two dimensional, but roughly lay on a $1$ dimensional subspace of $\mathbb{R}^2$. 
```

```{example, label='iq'}
Consider the correctness of the answers to a questionnaire with $p$ questions. 
The data may seemingly reside in a $p$ dimensional space, but assuming there is a thing as ``skill'', then given the correctness of a person's reply to a subset of questions, we have a good idea how he scores on the rest. 
Put differently, we don't really need a $200$ question questionnaire-- $100$ is more than enough.
If skill is indeed a one dimensional quality, then the questionnaire data should organize around a single line in the $p$ dimensional cube. 
```


```{example, label='blind-signal'}
Consider $n$ microphones recording an individual. 
The digitized recording consists of $p$ samples. 
Are the recordings really a shapeless cloud of $n$ points in $\mathbb{R}^p$?
Since they all record the same sound, one would expect the $n$ $p$-dimensional points to arrange around the source sound bit: a point in $\mathbb{R}^p$.
If microphones have different distances to the source, volumes may differ. 
We would thus expect the $n$ points to arrange about a line that ends at the source. 	
```

### Principal Component Analysis {#pca}

_Principal Component Analysis_ (PCA) is such a basic technique, it has been rediscovered and renamed independently in many fields. 
It can be found under the names of 
	Discrete Karhunen–Loève Transform; Hotteling Transform; Proper Orthogonal Decomposition; Eckart–Young  Theorem; Schmidt–Mirsky Theorem; Empirical Orthogonal Functions; Empirical Eigenfunction Decomposition; Empirical Component Analysis; Quasi-Harmonic Modes; Spectral Decomposition; Empirical Modal Analysis, and possibly more^[http://en.wikipedia.org/wiki/Principal_component_analysis].
The many names are quite interesting as they offer an insight into the different problems that led to PCA's (re)discovery.


Return to the BMI problem in Example \@ref(ex:bmi).
Assume you wish to give each individual a "size score", that is a __linear__ combination of height and weight: PCA does just that. 
It returns the linear combination that has the largest variability, i.e., the combination which best distinguishes between individuals. 

The variance maximizing motivation above was the one that guided @hotelling1933analysis.
But $30$ years before him, @pearson1901liii derived the same procedure with a different motivation in mind. 
Pearson was also trying to give each individual a score. 
He did not care about variance maximization, however. 
He simply wanted a small set of coordinates in some (linear) space that approximates the original data well. 
As it turns out, the best linear-space approximation of the data is also the variance maximizing score. 
more precisely: For $n$ observations on $p$ variables, in matrix form $X_{n\times p}$, then the __sequence__ of $1,\dots,p$ dimensional linear spaces that best approximate $X$ in Euclidean distance, is exactly the sequence of $1,\dots,p$ dimensional scores, that best separate between the $n$ samples. 
It is thus no surprise that Pearson and Hotelling (among others) arrived to the exact same solution, despite their different motivations. 

Before we proceed, we give an example to fix ideas.
Consider the crime rate data in `USArrests`, which encodes reported murder events, assaults, rapes, and the urban population of each americam state. 

```{r}
head(USArrests)
```

We may want to given each state a crimilality scores. 
PCA returns the set of $4$ scores that best separate the states. 

```{r, cache=TRUE}
USArrests.1 <- USArrests[,-3] %>% scale # note the scaling, which is required by some
pca.1 <- prcomp(USArrests.1, scale = TRUE)
pca.1
```

Things to note:

- The data is first centered and scaled with the `scale` function. This means each column is converted to z-score scale.
- The `prcomp` function returns the weight of each of the original variables in each of the new scores. These weights are called the _loadings_.
- The number of possible scores, is the same as the number of original variables in the data. 
- The new scores are called the _principal components_, labeled `PC1`,...,`PC4` in our output.
- The loadings on PC1 tell us that the best separation between states is along the average crime rate. 
Why is this? 
Because all the $3$ crime variables have a similar loading on PC1.
- The other PCs are slightly harder to interpret, but it is an interesting excercise.

__If we now represent each state, not with its original $4$ variables, but only with the first $2$ PCs (for example), we have reduced the dimensionality of the data.__

#### The Most Distinguishing Index View of PCA

We now state the PCA problem as stated by Harold Hotelling, i.e, as the creation of variance maximizing linear scores.
We will do so for the \emph{population} covariance, $\Sigma$, and wrap up by plugging its empirical counterpart, $X'X$ (assuming a centered $X$). 
		
Starting with the first principal component.
For a random $p$-vector, $x$ denote $\Sigma:=Cov[x]$, so that for a fixed $p$-vector $v$: $Cov[v'x]=v' \Sigma v$.
Finding a linear combination of $x$ that best separates individuals, means maximizing $Cov[v'x]$ w.r.t. to $v$.
Clearly, $Cov[v'x]$ may explode if any $v$ is allowed. 
It is most convenient, mathematically, to constrain the $l_2$ norm: $\Vert v \Vert_2=1$.
Maximizing under a constraint: 
\begin{align}
		argmax_v \{ v' \Sigma v \text{  s.t.  } \Vert v \Vert_2 = 1 \}.
		(\#eq:pca-problem) 
\end{align}



#### The Linear Subspace Embedding View

In here, we try to find a series of $\mathcal{M}_q; q=1,\dots,p$, such that $\mathcal{M}_q$ is a __linear__ subspace of dimension $q$ which well approximates $X$ in the Frobenius norm. 
The Frobenius norm can be thought of an Euclidean distance between matrices. 

```{definition}
The Frobenius norm of matrix $A$ is defined as 
$$
 \Vert A \Vert_{Frob}:= \sum_{i,j} a_{i,j}^2
$$
```

The linear space embedding view of PCA is thus
\begin{align}
	argmin_{A \in \mathbb{R}^{q \times p}, S \in \mathbb{R}^{n \times q}} \{ \Vert X- S A \Vert_{Frob} \}
	(\#eq:pca-dim-reduce)
\end{align}
where the target manifold, $\mathcal{M}_q$, is the span of $S$, and the $q$ PCs are $A'X$.


#### How many PCs can we recover? {#how-many-pcs}
How many PCs can we recover is the same as asking, how well can we learn the manifold in which the data resides. 
Put differently: can we trust the low dimensional representation of the data?

On the face of it, with $p$ variables one can find $p$ PCs. 
Things are not that simple however.

This matter is far from trivial.
Clearly, if $p>n$, variables of $X$ have to be linearly dependent, so that $X$, thus $X'X$, cannot possibly be of full rank.
Put differently, the number of PCs that may be computed is the same as the dimension of the rank of $X$, which is typically $min\{n,p\}$. 

While the algebraic problem ends when $p<n$, the statistical one only begins, and ends if $n \gg p$. 
This is because the PCA solution includes roughly $\mathcal{O}(p+p^2)$ parameters to be learned^[As the number of eigen values and eigen vectors of $X'X$.]. 
If $p<n$ but $p\sim n$ then we do not have many observations per estimated parameter. 
In the statistical literature, this is known as a __high-dimensional__ problem. 
In the engineering parlance, we say we have low __signal-to-noise__.
For a an analytical treatment of the statistical properties of PCA, see @nadler2008finite.
A purely algorithmic approach to the choice of $q$ is also available by adopting the supervised learning view of dimensionality reduction.
As can be seen from Eq.\@ref(eq:pca-dim-reduce), PCA, like most other reduction approaches, is an ERM  problem \@ref(eq:erm).
As such, unbiased risk estimation methods like cross-validation may be adopted to choose $q$.



#### PCA as a Graph Embedding
While omitting the mathematical proof, it turns out that to find a series of appriximating linear spaces to $X$, we don't need the data points themselves, but rather, only the distances between them: $XX'$.
This suggests we can decompose the learnings of the PCs to:

1. Learn the distances between points, a.k.a. the dissimilarity graph.
1. Find the linear spaces the approximate the dissimilarity graph.

PCA is thus a coupling between stages 1 and 2.
Many dimensionality reduction methods assume a dissimilarity graph is already given, not neccesarily Euclidean distances, and start operating from there.


#### PCA as Supervised Linear Regression
There is a strong link between dimensionality reduction and supervised learning.
Look at linear embedding problem in Eq.\@ref(eq:pca-dim-reduce), does it not look very familiar to a linear regression?
Indeed, PCA can be thought of as finding a small set of predictors that linearly predict $X$. 
Almost all supervised learning algorithms can thus be applied to dimensionality reduction, by viewing $X$ as the labels ($y$) of some unobservable scores in $\mathcal{M}_q$.

### Preliminaries

Before presenting methods other than PCA, we need some terminology.

- __Variable__: A.k.a. _dimension_, or _feature_, or _column_ for reasons that will be obvious in the next item.
- __Data__: A.k.a. _sample_, _observations_. 
Will typically consist of $n$, $p$ dimensional vectors.
We typically denote the data as a $n\times p$ matrix $X$. 
- __Manifold__: A generalization of a linear space, which is regular enough so that, __locally__, it has all the properties of a linear space. 
We will denote an arbitrary manifold by $\mathcal{M}$, and by $\mathcal{M}_q$ a $q$ dimensional^[You are probably used to thinking of the __dimension__ of linear spaces. We will not rigorously define what is the dimension of a manifold, but you may think of it as the number of free coordinates needed to navigate along the manifold.] manifold.
- __Embedding__: Informally speaking: a ``shape preserving'' mapping of a space into another. 
- __Linear Embedding__: An embedding done via a linear operation (thus representable by a matrix). 
- __Generative Model__: Known to statisticians as the __sampling distribution__. 
The assumed stochastic process that generated the observed $X$. 

There are many motivations for dimensionality reduction:

1. __Scoring__: Give each observation an interpretable, simple score (Hotelling's motivation).
1. __Latent structure__: Recover unobservables from indirect measurements. 
E.g: Blind signal reconstruction, CT scan, cryo-electron microscopy, etc. 
1. __Signal to Noise__: Denoise measurements before further processing like clustering, supervised learning, etc. 
1. __Compression__: Save on RAM ,CPU, and communication when operating on a lower dimensional representation of the data. 
	
Dimensionality reduction approaches differ in the following properties:
The reader is advised to try and characterize each approach along the following properties:

1. __Generative vs. algorithmic__:Refers to the motivation of the approach. Is it stated as an algorithm, or stated via some generative probabilistic model. 
PCA is purely algorithmic. 
1. __Linear $\mathcal{M}$ vs. non-linear $\mathcal{M}$__:
Is the target manifold linear or not?
In PCA, $\mathcal{M}$ is linear.
1. __Linear embedding vs. non-linear embedding__:
Is the embedding into $\mathcal{M}$ a linear operation?
In PCA, the embedding is linear, and indeed, represented by a matrix. 
1. __Learning an embedding vs. an embedding function__:
Will we need to apply the reduction to new data? 
If yes, we need to learn an __embedding function__, $g:x \mapsto g(x)\in \mathcal{M}$.
If no, and we merely want to low dimensional representation of existing data, $\{ g(x_i) \}_{i=1}^n$, we only need to __learn an embedding__. 
PCA learns an embedding function.
1. __Euclidean vs. non-Euclidean__:
Many dimensionality reduction methods only need a dissimilarity (i.e. distance) graph to operate. 
The Euclidean distance is historically the most popular dissimilarity measure.
Some approaches are agnostic to the measure used, some have Euclid hard-wired into them, and some, hard-wire themselves with some non-Euclidean norm. 
PCA has Euclid hard-wired. 



### Latent Variable Approaches
All generative approaches to dimensionality reduction will include some unobserved set of variables, which we can try to recover from the observable $X$. 
The unobservable variables will typically have a lower dimension than the observables, thus, dimension is reduced. 
We start with the simplest case of linear Factor Analysis. 

#### Factor Analysis 
FA originates from the psychometric literature. 
We thus revisit the IQ (actually g-factor^[https://en.wikipedia.org/wiki/G_factor_(psychometrics)]) Example~\@ref(ex:iq):

```{example}
Assume $n$ respondents answer $p$ quantitative questions: $x_i \in \mathbb{R}^p, i=1,\dots,n$. 
Also assume, their responses are some linear function $A \in \mathbb{R}^p$ of a single personality attribute, $s_i$. 
We can think of $s_i$ as the subject's ``intelligence''.
We thus have 
\begin{align}
	x_i = A s_i + \varepsilon_i
\end{align}
And in matrix notation, for $q<p$ latent attributes:
\begin{align}
	X = S A+\varepsilon,
	(\#eq:factor)
\end{align}
where $A$ is the $q \times p$ matrix of factor loadings, and $S$ the $n \times q$ matrix of latent personality traits. 
In our particular example where $q=1$, the problem is to recover the unobservable intelligence scores, $s_1,\dots,s_n$, from the observed answers $X$.	
```



We may try to estimate $S A$ by assuming some distribution on $S$ and $\varepsilon$ and apply maximum likelihood.
Under standard assumptions on the distribution of $S$ and $\varepsilon$, recovering  $S$ from $\widehat{S A }$ is still impossible as there are infinitely many such solutions.
In the statistical parlance we say the problem is _non identifiable_, and in the applied mathematics parlance we say the problem is _ill posed_.
To see this, consider an orthogonal _rotation_ matrix $R$ ($R' R=I$). 
For each such $R$: $ S A = A R' R S = S^* A^* $.
While both solve Eq.\@ref(eq:factor), $A$ and $A^*$ may have very different interpretations. 
This is why many researchers find FA an unsatisfactory inference tool.

```{remark}
The non-uniqueness (non-identifiability) of the FA solution under variable rotation is never mentioned in the PCA context. Why is this?
This is because the methods solve different problems. 
The reason the solution to PCA is well defined is that PCA does not seek a single $S$ but rather a __sequence__ of $S_q$ with dimensions growing from $q=1$ to $q=p$. 
```

```{remark}
In classical FA in Eq.\@ref(eq:factor) is clearly an embedding to a linear space. 
The one spanned by $S$. 
Under the classical probabilistic assumptions on $S$ and $\varepsilon$ the embedding itself is also linear, and is sometimes solved with PCA. 
Being a generative model, there is no restriction for the embedding to be linear, and there certainly exists sets of assumptions for which the FA embedding is non linear. 
```

The FA terminology is slightly different than PCA:

- __Factors__: 
The unobserved attributes $S$. 
Not to be confused with the _principal components_ in the context of PCA.
- __Loading__:
The $A$ matrix; the contribution of each factor to the observed $X$.
- __Rotation__:	
An arbitrary orthogonal re-combination of the factors, $S$, and loadings, $A$, which changes the interpretation of the result.


The FA literature does offer several heuristics to ``fix'' the solution of the FA. 
These are known as _rotations_, and go under the names of _Varimax_, _Quartimax_, _Equimax_, _Oblimin_, _Promax_, and possibly others. 


#### Independent Component Analysis
Like FA, _ICA_independent compoent analysis_ (ICA) is a family of latent space models, thus, a _meta-method_.
It assumes data is generated as some function of the latent variables $S$. 
In many cases this function is assumed to be linear in $S$ so that ICA is compared, if not confused, with PCA and even more so with FA. 

The fundamental idea of ICA is that $S$ has a joint distribution of __non-Gaussian__, __independent__ variables. 
This independence assumption, solves the the non-uniquness of $S$ in FA.

Being a generative model, estimation of $S$ can then be done using maximum likelihood, or other estimation principles. 

ICA is a popular technique in signal processing, where $A$ is actually the signal, such as sound in Example \@ref(ex:blind-signal).
Recovering $A$ is thus recovering the original signals mixing in the recorded $X$. 



\begin{remark}[ICA and FA]
The solutions to the (linear) ICA problem can ultimately be seen as a solution to the FA problem with a particular rotation $R$ implied by the probabilistic assumptions on $S$.
Put differently, the formulation of the (linear) ICA problem, implies a unique rotation, which can be thought of as the rotation that returns components that are as far from Gaussian as possible. 
\end{remark}


\begin{remark}[Linear and non-linear embeddings]
	In classical ICA in Eq.(\ref{eq:factor}) is clearly an embedding to a linear space. 
	The one spanned by $S$. 
	The probabilistic assumptions on $S$ and $\varepsilon$ the embedding itself being non linear, thus solved as an optimization problem, and not via PCA. 
\end{remark}





### Purely Algorithmic Approaches

We now discuss dimensionality reduction approaches that are not stated via their generative model, but rather, directly as an algorithm.
This does not mean that they cannot be cast via their generative model, but rather they were not motivated as such.


#### Multidimensional Scaling
Very roughly speaking, MDS can be thought of as a variation on PCA, that 
(i) begins with a distance graph^[The term Graph is typically used in this context instead of Network. But a graph allows only yes/no relations, while a network, which is a weighted graph, allows a continuous measure of similarity (or dissimilarity). _Network_ is thus more appropriate than _graph_.]}, and 
(ii) embeds into a two-dimensional space.
Regarding (i), we have already seen in Section \@ref(pca) that PCA really only needs a similarity graph, $X'X$, and not the whole $X$.  
Regarding (ii), the embedding into two dimensions is motivated by that fact that MDS is typically used for visualization. 

MDS aims at embedding a graph of distances, while preserving the original distances.
Basic results in graph/network theory [@graham1988isometric] suggest that the geometry of a graph cannot be preserved when embedding it into lower dimensions. 
The different types of MDSs, such as _Classical MDS_, and _Sammon Mappings_, differ in the _stress function_ penalizing for geometric distortion.

Sadly, MDS may scale poorly to large dissimilarity matrices, and the optimization may converge to a local minimum.
The solution to MDS is an embedding and not an embedding function. 
When new data points are made available, the embedding will thus have to be re-learned.


#### Local Multidimensional Scaling

```{example, label='non-euclidean'}
Consider data of coordinates on the globe. 
At short distances, constructing a dissimilarity graph with Euclidean distances will capture the true distance between points. 
At long distances, however, the Euclidean distances as grossly inappropriate. 
A more extreme example is coordinates on the brain's cerebral cortex.
Being a highly folded surface, the Euclidean distance between points is far from the true geodesic distances along the cortex's surface^[Then again, it is possible that the true distances are the white matter fibers connecting going within the cortex, in which case, Euclidean distances are more appropriate than geodesic distances. We put that aside for now.].
```

Local MDS is aimed at solving the case where we don't know how to properly measure distances. 
It is an algorithm that compounds both the construction of the dissimilarity graph, and the embedding. 
The solution of local MDS, as the name suggests, rests on the computation of _local_ distances, where the Euclidean assumption may still be plausible, and then aggregate many such local distances, before calling upon regular MDS for the embedding.

Because local MDS ends with a regular MDS, it can be seen as a non-linear embedding into a linear $\mathcal{M}$. 

Local MDS is not popular.
Why is this? 
Because it makes no sense: 
If we believe the points reside in a non-Euclidean space, thus motivating the use of geodesic distances, why would we want to wrap up with regular MDS, which embeds in a linear space?!

This motivated the following methods, which embed into a non-linear space:

- Kernel PCA.
- Isometric Feature Mapping (IsoMap).
- Local Linear Embedding (LLE).
- Laplacian Eigen Maps.
- Maximum Variance Unfoldind.



### Dealing with a high-dimension

As we previously mentioned \@ref(how-many-pcs) it is hard to accurately estimates directions of co-variability with little data. 
For some intuition, think of recovering a "bigness score" in the BMI example \@ref(ex:bmi) from only 3 individuals ($p=2,n=3$). 
Would you trust the recovered index?

In high dimensional problems, i.e., low signal-to-noise regimes, we will introduce some regularization, to reduce variance, as the cost of some bias. 
For generative models, regularization can always be introduced with Bayesian priors, say, some matrix-valued distribution as a prior on $S$. 
For purely algorithmic approaches (such as PCA), regularization has to be be hard-wired into the algorithm.

Some dimensionality reduction methods designed for $n<p$ problems include:

- Simplified Component Technique LASSO (SCoTLASS)
- Sparse Principal Component Analysis (sPCA)
- Sparse kernel principal component analysis (skPCA)



### Dimensionality Reduction in R

#### PCA

We already saw the basics of PCA in \@ref(pca).
The fitting is done with the `procomp` function.
The _bi-plot_ is a useful way to visualize the output of PCA.

```{r}
library(devtools)
# install_github("vqv/ggbiplot")
ggbiplot::ggbiplot(pca.1) 
```

Things to note:

- The bi-plot plots each data point along its PCs.
- We used the `ggbiplot` function from the __ggbiplot__ (available from github, but not from CRAN), because it has a nicer output than `stats::biplot`. 
- The bi-plot also plots the loadings as arrows. The coordinates of the arrows belong to the weight of each of the original variables in each PC. 
For example, the x-value of each arrow is the loadings on the first PC (on the x-axis).
Since the weights of Murder, Assult, and Rape are almost the same, and larger then UrbanPop, we conclude that PC1 captures the avarage crime rate in each state.

The _scree plot_ depicts the quality of the approximation of $X$ as $q$ grows. 
This is depicted using the proportion of variability in $X$ that is removed by each added PC.

```{r}
ggbiplot::ggscreeplot(pca.1)
```

See the projections of the raw variables on the first PCs:
```{r, echo=FALSE}
USArrests.1 <- USArrests[,-3] %>% scale
load <- pca.1$rotation
slope <- load[2, ]/load[1, ]
mn <- apply(USArrests.1, 2, mean)
intcpt <- mn[2] - (slope * mn[1])

# scatter plot with the two new axes added
USArrests.2 <- USArrests[,1:2] %>%  scale
xlim <- range(USArrests.2)  # overall min, max
plot(USArrests.2, xlim = xlim, ylim = xlim, pch = 16, col = "purple")  # both axes same length
abline(intcpt[1], slope[1], lwd = 2)  # first component solid line
abline(intcpt[2], slope[2], lwd = 2, lty = 2)  # second component dashed
legend("right", legend = c("PC 1", "PC 2"), lty = c(1, 2), lwd = 2, cex = 1)

# projections of points onto PCA 1
y1 <- intcpt[1] + slope[1] * USArrests.2[, 1]
x1 <- (USArrests.1[, 2] - intcpt[1])/slope[1]
y2 <- (y1 + USArrests.1[, 2])/2
x2 <- (x1 + USArrests.1[, 1])/2
segments(USArrests.1[, 1], USArrests.1[, 2], x2, y2, lwd = 2, col = "purple")
```

More implementations of PCA:
```{r many PCA implementations, eval=FALSE}
# FAST solutions:
gmodels::fast.prcomp()

# More detail in output:
FactoMineR::PCA()

# For flexibility in algorithms and visualization:
ade4::dudi.pca()

# Another one...
amap::acp()
```


#### FA


```{r FA}
fa.1 <- psych::principal(USArrests.1, nfactors = 2, rotate = "none")
fa.1
biplot(fa.1, labels =  rownames(USArrests.1)) 

# Numeric comparison with PCA:
fa.1$loadings
pca.1$rotation
```

Things to note:

- We perform FA with the `psych::principal` function.
- The first factor (`fa.1$loadings`) has different weights than the firts PC (`pca.1$rotation`), but both capture the average crime rate.

Graphical model fans will like the following plot, where the contribution of each variable to each factor is encoded in the width of the arrow. 

```{r}
# Graph comparison: loadings encoded in colors
qgraph::qgraph(fa.1)
```

Let's add a rotaion (Varimax), and note that the rotation has indeed changed the loadings of the variables, thus the interpretation of the factors. 

```{r varimax}
fa.2 <- psych::principal(USArrests.1, nfactors = 2, rotate = "varimax")

fa.2$loadings
fa.1$loadings
pca.1$rotation
```




#### ICA

```{r ICA}
ica.1 <- fastICA::fastICA(USArrests.1, n.com=2) # Also performs projection pursuit

plot(ica.1$S)
abline(h=0, v=0, lty=2)
text(ica.1$S, pos = 4, labels = rownames(USArrests.1))

# Compare with two PCA (first two PCs):
arrows(x0 = ica.1$S[,1], y0 = ica.1$S[,2], x1 = pca.1$x[,2], y1 = pca.1$x[,1], col='red', pch=19, cex=0.5)
```

Things to note:

- ICA is fitted with `fastICA::fastICA`.
- The ICA componetns are very different than the PCA components. 




#### Sparse PCA

```{r sPCA}
# Compute similarity graph
state.similarity <- MASS::cov.rob(USArrests.1)$cov

spca1 <- elasticnet::spca(state.similarity, K=2,type="Gram",sparse="penalty",trace=TRUE, para=c(0.06,0.16))
spca1$loadings
```



#### Kernel PCA
```{r kPCA, eval=FALSE}
kernlab::kpca()
```

#### MDS

Classical MDS, also compared with PCA.
```{r MDS, results='hold'}
# We first need a dissimarity matrix/graph:
state.disimilarity <- dist(USArrests.1)

mds.1 <- stats::cmdscale(state.disimilarity)

plot(mds.1, pch = 19)
abline(h=0, v=0, lty=2)
USArrests.2 <- USArrests[,1:2] %>%  scale
text(mds.1, pos = 4, labels = rownames(USArrests.2), col = 'tomato')

# Compare with two PCA (first two PCs):
points(pca.1$x[,1:2], col='red', pch=19, cex=0.5)
# So classical MDS with Euclidean distance, is the same as PCA on two dimensions!
```

Things to note:

- For MDS, we firts compute a dissimilarity graph with `dist`, and then learn the embedding with `stats::cmdscale`.
- As previously stated, the embedding of PCA is the same as classical MDS with Euclidean distances. 
- See the `cluster::daisy` function for more dissimilarity measures.


Let's try other strain functions for MDS, like Sammon's strain, and compare it with the PCs.
```{r Sammon MDS}
mds.2 <- MASS::sammon(state.disimilarity, trace = FALSE)
plot(mds.2$points, pch = 19)
abline(h=0, v=0, lty=2)
text(mds.2$points, pos = 4, labels = rownames(USArrests.2))

# Compare with two PCA (first two PCs):
arrows(x0 = mds.2$points[,1], y0 = mds.2$points[,2], x1 = pca.1$x[,1], y1 = pca.1$x[,2], col='red', pch=19, cex=0.5)
# So Sammon's MDS with Euclidean distance, is *not* the same as PCA on two dimensions.
```

Things to note:

- `MASS::sammon` does the fitting.
- The Sammon strain is different than PCA.





## Clustering {#cluster}


### Clustering in R



## Bibliographic Notes
