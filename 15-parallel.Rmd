# Parallel Computing {#parallel}

You would think that because you have an expensive multicore computer your computations will speed up. 
Well, no.
At least not if you don't make sure they do.
By default, no matter how many cores you have, the operating system will allocate each R session to a single core. 

For starters, we need to distinguish between two types of parallelism:

1. __Explicit parallelism__: where the user handles the parallelisation. 
1. __Implicit parallelism__: where the compiler handles the nonparallel.

Clearly, implicit parallelism is more desirable, but the state of mathematical computing is such that no sufficiently general implicit parallelism framework exists. 
The [R Consortium](https://www.r-consortium.org/projects/awarded-projects) is currently financing a major project for a A "Unified Framework For Distributed Computing in R" so we can expect things to change soon. 
In the meanwhile, most of the parallel implementations are explicit. 



## Explicit Parallelism

R provides many frameworks for explicit parallelism.
Because the parallelism is initiated by the user, we first need to decide __when to parallelize?__
As a rule of thumb, you want to parallelise when you encounter a CPU bottleneck, and not a memory bottlenck.
Memory bottlenecks are released with Sparsity (Chapter \@ref(sparse)), or efficient representation (Chapter \@ref(memory)).

Several ways to diagnose your bottlenck include:

- Keep your Windows Task Manager, or Linux `top` open, and look for the CPU load. 
- The computation takes a long time, and when you stop it pressign ESC, R is immediatly responsive (if it is not immediatly responsive, you have a memory bottlenck).
- Profile your code. See Hadley's [guide](http://adv-r.had.co.nz/Profiling.html).

For reasons detailed in @kane2013scalable, we will present the __foreach__ parallelisation package [@foreach]. 
It will allow us to: 

1. Decouple between our parallel algorithm and the parallelisation mechanism: we write parallelisable code once, and can then switch the underlying parallesation mechanism. 

1. Combine with the `big.matrix` object from Chapter \@ref(memory) for _shared memory parallisation_: all the machines may see the same data, so that we don't need to export objects from machine to machine. 


What do we mean by "switch the underlying parallesation mechanism"? 
It means there are several packages that will handle communication between machines. 
Some are very general and will work on any cluster that does not even need to be on the same machine. 
Some are more specific and will work only on a single multicore machine (not a cluster) with a particular operating system.
These mechanisms include __multicore__, __snow__, __parallel__, and __Rmpi__.
The compatibility between these mechanisms and __foreach__ is provided by another set of packages:
__doMC__ , __doMPI__, __doRedis__, __doParallel__, and __doSNOW__.


```{remark}
I personally prefer the __multicore__ mechanism, with the __doMC__ adapter for __foreach__.
I will not use this combo, however, because __multicore__ will not work on Windows machines.
I will thus use the more general __snow__ and __doParallel__ combo. 
If you do happen to run on Linux, or Unix, you will want to replace all __doParallel__ functionality with __doMC__.
```


Let's start with a simple example, taken from ["Getting Started with doParallel and foreach"](http://debian.mc.vanderbilt.edu/R/CRAN/web/packages/doParallel/vignettes/gettingstartedParallel.pdf).

```{r}
library(doParallel)
cl <- makeCluster(2)
registerDoParallel(cl)
result <- foreach(i=1:3) %dopar% sqrt(i)
class(result)
result
```

Things to note:

- `makeCluster` creates an object with the information our cluster. One a single machine it is very simple. On a cluster of machines, you will need to specify the i.p. adresses or other identifiers ot the machines. 
- `registerDoParallel` is used to inform the __foreach__ package of the presence of our cluster. 
- The `foreach` function handles the looping. In particular note the `%dopar` operator that ensures that looping is in parallel. `%dopar%` can be replaced by `%do%` if you want serial looping (like the `for` loop), for instance, for debugging. 
- The output of the various machines is collected by `foreach` to a list object. 
- In this simple example, no data is shared between machines so we are not putting the shared memory capabilities to the test. 
- We can check how many workers were involved using the `getDoParWorkers()` function.
- We can check the parallelisation mechanism used with the `getDoParName()` function.


Here is a more involved example.
We now try to make [Bootstrap](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) inference on the coeffieicents of a logistic regression.
Bootstrapping means that in each iteration, we resample the data, and refit the model. 

```{r}
x <- iris[which(iris[,5] != "setosa"), c(1,5)]
trials <- 1e4
ptime <- system.time({
 r <- foreach(icount(trials), .combine=cbind) %dopar% {
 ind <- sample(100, 100, replace=TRUE)
 result1 <- glm(x[ind,2]~x[ind,1], family=binomial(logit))
 coefficients(result1)
 }
 })[3]
ptime
```

Things to note:

- As usual, we use the `foreach` function with the `%dopar%` operator to loop in parallel.
- The `icounts` function generates a counter. 
- The `.combine=cbind` argument tells the `foreach` function how to combine the output of different machines, so that the returned object is not the default list. 


How long would that have taken in a simple (serial) loop? 
We only need to replace `%dopar%` with `%do%` to test.

```{r}
stime <- system.time({
 r <- foreach(icount(trials), .combine=cbind) %do% {
 ind <- sample(100, 100, replace=TRUE)
 result1 <- glm(x[ind,2]~x[ind,1], family=binomial(logit))
 coefficients(result1)
 }
 })[3]
stime
```

Yes. This takes clearly longer. 


Let's see how we can combine the power of __bigmemory__ and __foreach__ by creating a file mapped `big.matrix` object, which is shared by all machines.
The following example is taken from @kane2013scalable, and uses the `big.matrix` object we created in Chapter \@ref(memory).

```{r}
library(bigmemory)
x <- attach.big.matrix("airline.desc")

library(foreach)
library(doSNOW)
cl <- makeSOCKcluster(rep("localhost", 4)) # make a cluster of 4 machines
registerDoSNOW(cl) # register machines for foreach()
```

Get a "description" of the `big.matrix` object that will be used to call it from each machine.
```{r}
xdesc <- describe(x) 
```

Split the data along values of `BENE_AGE_CAT_CD`.
```{r}
G <- split(1:nrow(x), x[, "BENE_AGE_CAT_CD"]) 
```

Define a function that coputes quantiles of `CAR_LINE_ICD9_DGNS_CD`.
```{r}
GetDepQuantiles <- function(rows, data) {
 quantile(data[rows, "CAR_LINE_ICD9_DGNS_CD"], probs = c(0.5, 0.9, 0.99),
 na.rm = TRUE)
}
```

We are all set up to loop, in paralle, and compute quantiles of `CAR_LINE_ICD9_DGNS_CD` for each value of `BENE_AGE_CAT_CD`.

```{r}
qs <- foreach(g = G, .combine = rbind) %dopar% {
 require("bigmemory")
 x <- attach.big.matrix(xdesc)
 GetDepQuantiles(rows = g, data = x)
}
qs
```







## Implicit Parallelism







## Bibliographic Notes
For a brief and excellent explanation on parallel computing in R see @schmidberger2009state.
For a full review see @chapple2016mastering.
For an up-to-date list of packages supporting parallel programming see the High Performance Computing [R task view](https://cran.r-project.org/web/views/HighPerformanceComputing.html).
