<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>R (BGU course)</title>
  <meta name="description" content="Class notes for the R course at the BGU’s IE&amp;M dept.">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="R (BGU course)" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Class notes for the R course at the BGU’s IE&amp;M dept." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="R (BGU course)" />
  
  <meta name="twitter:description" content="Class notes for the R course at the BGU’s IE&amp;M dept." />
  

<meta name="author" content="Jonathan D. Rosenblatt">


<meta name="date" content="2018-04-18">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="multivariate.html">
<link rel="next" href="unsupervised.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.9/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.7.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotlyjs-1.29.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.29.2/plotly-latest.min.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">R Course</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#notation-conventions"><i class="fa fa-check"></i><b>1.1</b> Notation Conventions</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i><b>1.2</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#what-r"><i class="fa fa-check"></i><b>2.1</b> What is R?</a></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#ecosystem"><i class="fa fa-check"></i><b>2.2</b> The R Ecosystem</a></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#bibliographic-notes"><i class="fa fa-check"></i><b>2.3</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="2.4" data-path="intro.html"><a href="intro.html#practice-yourself"><i class="fa fa-check"></i><b>2.4</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="basics.html"><a href="basics.html"><i class="fa fa-check"></i><b>3</b> R Basics</a><ul>
<li class="chapter" data-level="3.0.1" data-path="basics.html"><a href="basics.html#other-ides"><i class="fa fa-check"></i><b>3.0.1</b> Other IDEs</a></li>
<li class="chapter" data-level="3.1" data-path="basics.html"><a href="basics.html#file-types"><i class="fa fa-check"></i><b>3.1</b> File types</a></li>
<li class="chapter" data-level="3.2" data-path="basics.html"><a href="basics.html#simple-calculator"><i class="fa fa-check"></i><b>3.2</b> Simple calculator</a></li>
<li class="chapter" data-level="3.3" data-path="basics.html"><a href="basics.html#probability-calculator"><i class="fa fa-check"></i><b>3.3</b> Probability calculator</a></li>
<li class="chapter" data-level="3.4" data-path="basics.html"><a href="basics.html#getting-help"><i class="fa fa-check"></i><b>3.4</b> Getting Help</a></li>
<li class="chapter" data-level="3.5" data-path="basics.html"><a href="basics.html#variable-asignment"><i class="fa fa-check"></i><b>3.5</b> Variable Asignment</a></li>
<li class="chapter" data-level="3.6" data-path="basics.html"><a href="basics.html#missing"><i class="fa fa-check"></i><b>3.6</b> Missing</a></li>
<li class="chapter" data-level="3.7" data-path="basics.html"><a href="basics.html#piping"><i class="fa fa-check"></i><b>3.7</b> Piping</a></li>
<li class="chapter" data-level="3.8" data-path="basics.html"><a href="basics.html#vector-creation-and-manipulation"><i class="fa fa-check"></i><b>3.8</b> Vector Creation and Manipulation</a></li>
<li class="chapter" data-level="3.9" data-path="basics.html"><a href="basics.html#search-paths-and-packages"><i class="fa fa-check"></i><b>3.9</b> Search Paths and Packages</a></li>
<li class="chapter" data-level="3.10" data-path="basics.html"><a href="basics.html#simple-plotting"><i class="fa fa-check"></i><b>3.10</b> Simple Plotting</a></li>
<li class="chapter" data-level="3.11" data-path="basics.html"><a href="basics.html#object-types"><i class="fa fa-check"></i><b>3.11</b> Object Types</a></li>
<li class="chapter" data-level="3.12" data-path="basics.html"><a href="basics.html#data-frames"><i class="fa fa-check"></i><b>3.12</b> Data Frames</a></li>
<li class="chapter" data-level="3.13" data-path="basics.html"><a href="basics.html#exctraction"><i class="fa fa-check"></i><b>3.13</b> Exctraction</a></li>
<li class="chapter" data-level="3.14" data-path="basics.html"><a href="basics.html#non-data.frame-object-classes"><i class="fa fa-check"></i><b>3.14</b> Non data.frame object classes</a></li>
<li class="chapter" data-level="3.15" data-path="basics.html"><a href="basics.html#data-import-and-export"><i class="fa fa-check"></i><b>3.15</b> Data Import and Export</a><ul>
<li class="chapter" data-level="3.15.1" data-path="basics.html"><a href="basics.html#import-from-web"><i class="fa fa-check"></i><b>3.15.1</b> Import from WEB</a></li>
<li class="chapter" data-level="3.15.2" data-path="basics.html"><a href="basics.html#export-as-csv"><i class="fa fa-check"></i><b>3.15.2</b> Export as CSV</a></li>
<li class="chapter" data-level="3.15.3" data-path="basics.html"><a href="basics.html#export-non-csv-files"><i class="fa fa-check"></i><b>3.15.3</b> Export non-CSV files</a></li>
<li class="chapter" data-level="3.15.4" data-path="basics.html"><a href="basics.html#reading-from-text-files"><i class="fa fa-check"></i><b>3.15.4</b> Reading From Text Files</a></li>
<li class="chapter" data-level="3.15.5" data-path="basics.html"><a href="basics.html#writing-data-to-text-files"><i class="fa fa-check"></i><b>3.15.5</b> Writing Data to Text Files</a></li>
<li class="chapter" data-level="3.15.6" data-path="basics.html"><a href="basics.html#xlsx-files"><i class="fa fa-check"></i><b>3.15.6</b> .XLS(X) files</a></li>
<li class="chapter" data-level="3.15.7" data-path="basics.html"><a href="basics.html#massive-files"><i class="fa fa-check"></i><b>3.15.7</b> Massive files</a></li>
<li class="chapter" data-level="3.15.8" data-path="basics.html"><a href="basics.html#databases"><i class="fa fa-check"></i><b>3.15.8</b> Databases</a></li>
</ul></li>
<li class="chapter" data-level="3.16" data-path="basics.html"><a href="basics.html#functions"><i class="fa fa-check"></i><b>3.16</b> Functions</a></li>
<li class="chapter" data-level="3.17" data-path="basics.html"><a href="basics.html#looping"><i class="fa fa-check"></i><b>3.17</b> Looping</a></li>
<li class="chapter" data-level="3.18" data-path="basics.html"><a href="basics.html#apply"><i class="fa fa-check"></i><b>3.18</b> Apply</a></li>
<li class="chapter" data-level="3.19" data-path="basics.html"><a href="basics.html#recursion"><i class="fa fa-check"></i><b>3.19</b> Recursion</a></li>
<li class="chapter" data-level="3.20" data-path="basics.html"><a href="basics.html#dates-and-times"><i class="fa fa-check"></i><b>3.20</b> Dates and Times</a></li>
<li class="chapter" data-level="3.21" data-path="basics.html"><a href="basics.html#bibliographic-notes-1"><i class="fa fa-check"></i><b>3.21</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="3.22" data-path="basics.html"><a href="basics.html#practice-yourself-1"><i class="fa fa-check"></i><b>3.22</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="eda.html"><a href="eda.html"><i class="fa fa-check"></i><b>4</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="eda.html"><a href="eda.html#summary-statistics"><i class="fa fa-check"></i><b>4.1</b> Summary Statistics</a><ul>
<li class="chapter" data-level="4.1.1" data-path="eda.html"><a href="eda.html#categorical-data"><i class="fa fa-check"></i><b>4.1.1</b> Categorical Data</a></li>
<li class="chapter" data-level="4.1.2" data-path="eda.html"><a href="eda.html#continous-data"><i class="fa fa-check"></i><b>4.1.2</b> Continous Data</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="eda.html"><a href="eda.html#visualization"><i class="fa fa-check"></i><b>4.2</b> Visualization</a><ul>
<li class="chapter" data-level="4.2.1" data-path="eda.html"><a href="eda.html#categorical-data-1"><i class="fa fa-check"></i><b>4.2.1</b> Categorical Data</a></li>
<li class="chapter" data-level="4.2.2" data-path="eda.html"><a href="eda.html#continuous-data"><i class="fa fa-check"></i><b>4.2.2</b> Continuous Data</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="eda.html"><a href="eda.html#bibliographic-notes-2"><i class="fa fa-check"></i><b>4.3</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="4.4" data-path="eda.html"><a href="eda.html#practice-yourself-2"><i class="fa fa-check"></i><b>4.4</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lm.html"><a href="lm.html"><i class="fa fa-check"></i><b>5</b> Linear Models</a><ul>
<li class="chapter" data-level="5.1" data-path="lm.html"><a href="lm.html#problem-setup"><i class="fa fa-check"></i><b>5.1</b> Problem Setup</a></li>
<li class="chapter" data-level="5.2" data-path="lm.html"><a href="lm.html#ols-estimation-in-r"><i class="fa fa-check"></i><b>5.2</b> OLS Estimation in R</a></li>
<li class="chapter" data-level="5.3" data-path="lm.html"><a href="lm.html#inference"><i class="fa fa-check"></i><b>5.3</b> Inference</a><ul>
<li class="chapter" data-level="5.3.1" data-path="lm.html"><a href="lm.html#testing-a-hypothesis-on-a-single-coefficient"><i class="fa fa-check"></i><b>5.3.1</b> Testing a Hypothesis on a Single Coefficient</a></li>
<li class="chapter" data-level="5.3.2" data-path="lm.html"><a href="lm.html#constructing-a-confidence-interval-on-a-single-coefficient"><i class="fa fa-check"></i><b>5.3.2</b> Constructing a Confidence Interval on a Single Coefficient</a></li>
<li class="chapter" data-level="5.3.3" data-path="lm.html"><a href="lm.html#multiple-regression"><i class="fa fa-check"></i><b>5.3.3</b> Multiple Regression</a></li>
<li class="chapter" data-level="5.3.4" data-path="lm.html"><a href="lm.html#anova"><i class="fa fa-check"></i><b>5.3.4</b> ANOVA (*)</a></li>
<li class="chapter" data-level="5.3.5" data-path="lm.html"><a href="lm.html#testing-a-hypothesis-on-a-single-contrast"><i class="fa fa-check"></i><b>5.3.5</b> Testing a Hypothesis on a Single Contrast (*)</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="lm.html"><a href="lm.html#bibliographic-notes-3"><i class="fa fa-check"></i><b>5.4</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="5.5" data-path="lm.html"><a href="lm.html#practice-yourself-3"><i class="fa fa-check"></i><b>5.5</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>6</b> Generalized Linear Models</a><ul>
<li class="chapter" data-level="6.1" data-path="glm.html"><a href="glm.html#problem-setup-1"><i class="fa fa-check"></i><b>6.1</b> Problem Setup</a></li>
<li class="chapter" data-level="6.2" data-path="glm.html"><a href="glm.html#logistic-regression"><i class="fa fa-check"></i><b>6.2</b> Logistic Regression</a><ul>
<li class="chapter" data-level="6.2.1" data-path="glm.html"><a href="glm.html#logistic-regression-with-r"><i class="fa fa-check"></i><b>6.2.1</b> Logistic Regression with R</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="glm.html"><a href="glm.html#poisson-regression"><i class="fa fa-check"></i><b>6.3</b> Poisson Regression</a></li>
<li class="chapter" data-level="6.4" data-path="glm.html"><a href="glm.html#extensions"><i class="fa fa-check"></i><b>6.4</b> Extensions</a></li>
<li class="chapter" data-level="6.5" data-path="glm.html"><a href="glm.html#bibliographic-notes-4"><i class="fa fa-check"></i><b>6.5</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="6.6" data-path="glm.html"><a href="glm.html#practice-glm"><i class="fa fa-check"></i><b>6.6</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="lme.html"><a href="lme.html"><i class="fa fa-check"></i><b>7</b> Linear Mixed Models</a><ul>
<li class="chapter" data-level="7.1" data-path="lme.html"><a href="lme.html#problem-setup-2"><i class="fa fa-check"></i><b>7.1</b> Problem Setup</a><ul>
<li class="chapter" data-level="7.1.1" data-path="lme.html"><a href="lme.html#non-linear-mixed-models"><i class="fa fa-check"></i><b>7.1.1</b> Non-Linear Mixed Models</a></li>
<li class="chapter" data-level="7.1.2" data-path="lme.html"><a href="lme.html#generalized-linear-mixed-models-glmm"><i class="fa fa-check"></i><b>7.1.2</b> Generalized Linear Mixed Models (GLMM)</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="lme.html"><a href="lme.html#mixed-models-with-r"><i class="fa fa-check"></i><b>7.2</b> Mixed Models with R</a><ul>
<li class="chapter" data-level="7.2.1" data-path="lme.html"><a href="lme.html#a-single-random-effect"><i class="fa fa-check"></i><b>7.2.1</b> A Single Random Effect</a></li>
<li class="chapter" data-level="7.2.2" data-path="lme.html"><a href="lme.html#multiple-random-effects"><i class="fa fa-check"></i><b>7.2.2</b> Multiple Random Effects</a></li>
<li class="chapter" data-level="7.2.3" data-path="lme.html"><a href="lme.html#a-full-mixed-model"><i class="fa fa-check"></i><b>7.2.3</b> A Full Mixed-Model</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="lme.html"><a href="lme.html#manova"><i class="fa fa-check"></i><b>7.3</b> Relation to MANOVA</a></li>
<li class="chapter" data-level="7.4" data-path="lme.html"><a href="lme.html#the-variance-components-view"><i class="fa fa-check"></i><b>7.4</b> The Variance-Components View</a></li>
<li class="chapter" data-level="7.5" data-path="lme.html"><a href="lme.html#bibliographic-notes-5"><i class="fa fa-check"></i><b>7.5</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="7.6" data-path="lme.html"><a href="lme.html#practice-yourself-4"><i class="fa fa-check"></i><b>7.6</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="multivariate.html"><a href="multivariate.html"><i class="fa fa-check"></i><b>8</b> Multivariate Data Analysis</a><ul>
<li class="chapter" data-level="8.1" data-path="multivariate.html"><a href="multivariate.html#signal-detection"><i class="fa fa-check"></i><b>8.1</b> Signal Detection</a><ul>
<li class="chapter" data-level="8.1.1" data-path="multivariate.html"><a href="multivariate.html#hotellings-t2-test"><i class="fa fa-check"></i><b>8.1.1</b> Hotelling’s T2 Test</a></li>
<li class="chapter" data-level="8.1.2" data-path="multivariate.html"><a href="multivariate.html#simes-test"><i class="fa fa-check"></i><b>8.1.2</b> Simes’ Test</a></li>
<li class="chapter" data-level="8.1.3" data-path="multivariate.html"><a href="multivariate.html#signal-detection-with-r"><i class="fa fa-check"></i><b>8.1.3</b> Signal Detection with R</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="multivariate.html"><a href="multivariate.html#signal-counting"><i class="fa fa-check"></i><b>8.2</b> Signal Counting</a></li>
<li class="chapter" data-level="8.3" data-path="multivariate.html"><a href="multivariate.html#identification"><i class="fa fa-check"></i><b>8.3</b> Signal Identification</a><ul>
<li class="chapter" data-level="8.3.1" data-path="multivariate.html"><a href="multivariate.html#signal-identification-in-r"><i class="fa fa-check"></i><b>8.3.1</b> Signal Identification in R</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="multivariate.html"><a href="multivariate.html#signal-estimation"><i class="fa fa-check"></i><b>8.4</b> Signal Estimation (*)</a></li>
<li class="chapter" data-level="8.5" data-path="multivariate.html"><a href="multivariate.html#multivariate-regression"><i class="fa fa-check"></i><b>8.5</b> Multivariate Regression (*)</a><ul>
<li class="chapter" data-level="8.5.1" data-path="multivariate.html"><a href="multivariate.html#multivariate-regression-with-r"><i class="fa fa-check"></i><b>8.5.1</b> Multivariate Regression with R</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="multivariate.html"><a href="multivariate.html#graphical-models"><i class="fa fa-check"></i><b>8.6</b> Graphical Models (*)</a><ul>
<li class="chapter" data-level="8.6.1" data-path="multivariate.html"><a href="multivariate.html#graphical-models-in-r"><i class="fa fa-check"></i><b>8.6.1</b> Graphical Models in R</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="multivariate.html"><a href="multivariate.html#biblipgraphic-notes"><i class="fa fa-check"></i><b>8.7</b> Biblipgraphic Notes</a></li>
<li class="chapter" data-level="8.8" data-path="multivariate.html"><a href="multivariate.html#practice-yourself-5"><i class="fa fa-check"></i><b>8.8</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="supervised.html"><a href="supervised.html"><i class="fa fa-check"></i><b>9</b> Supervised Learning</a><ul>
<li class="chapter" data-level="9.1" data-path="supervised.html"><a href="supervised.html#problem-setup-3"><i class="fa fa-check"></i><b>9.1</b> Problem Setup</a><ul>
<li class="chapter" data-level="9.1.1" data-path="supervised.html"><a href="supervised.html#common-hypothesis-classes"><i class="fa fa-check"></i><b>9.1.1</b> Common Hypothesis Classes</a></li>
<li class="chapter" data-level="9.1.2" data-path="supervised.html"><a href="supervised.html#common-complexity-penalties"><i class="fa fa-check"></i><b>9.1.2</b> Common Complexity Penalties</a></li>
<li class="chapter" data-level="9.1.3" data-path="supervised.html"><a href="supervised.html#unbiased-risk-estimation"><i class="fa fa-check"></i><b>9.1.3</b> Unbiased Risk Estimation</a></li>
<li class="chapter" data-level="9.1.4" data-path="supervised.html"><a href="supervised.html#collecting-the-pieces"><i class="fa fa-check"></i><b>9.1.4</b> Collecting the Pieces</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="supervised.html"><a href="supervised.html#supervised-learning-in-r"><i class="fa fa-check"></i><b>9.2</b> Supervised Learning in R</a><ul>
<li class="chapter" data-level="9.2.1" data-path="supervised.html"><a href="supervised.html#least-squares"><i class="fa fa-check"></i><b>9.2.1</b> Linear Models with Least Squares Loss</a></li>
<li class="chapter" data-level="9.2.2" data-path="supervised.html"><a href="supervised.html#svm"><i class="fa fa-check"></i><b>9.2.2</b> SVM</a></li>
<li class="chapter" data-level="9.2.3" data-path="supervised.html"><a href="supervised.html#neural-nets"><i class="fa fa-check"></i><b>9.2.3</b> Neural Nets</a></li>
<li class="chapter" data-level="9.2.4" data-path="supervised.html"><a href="supervised.html#classification-and-regression-trees-cart"><i class="fa fa-check"></i><b>9.2.4</b> Classification and Regression Trees (CART)</a></li>
<li class="chapter" data-level="9.2.5" data-path="supervised.html"><a href="supervised.html#k-nearest-neighbour-knn"><i class="fa fa-check"></i><b>9.2.5</b> K-nearest neighbour (KNN)</a></li>
<li class="chapter" data-level="9.2.6" data-path="supervised.html"><a href="supervised.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.2.6</b> Linear Discriminant Analysis (LDA)</a></li>
<li class="chapter" data-level="9.2.7" data-path="supervised.html"><a href="supervised.html#naive-bayes"><i class="fa fa-check"></i><b>9.2.7</b> Naive Bayes</a></li>
<li class="chapter" data-level="9.2.8" data-path="supervised.html"><a href="supervised.html#random-forrest"><i class="fa fa-check"></i><b>9.2.8</b> Random Forrest</a></li>
<li class="chapter" data-level="9.2.9" data-path="supervised.html"><a href="supervised.html#gradient-boosting"><i class="fa fa-check"></i><b>9.2.9</b> Gradient Boosting</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="supervised.html"><a href="supervised.html#bibliographic-notes-6"><i class="fa fa-check"></i><b>9.3</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="9.4" data-path="supervised.html"><a href="supervised.html#practice-yourself-6"><i class="fa fa-check"></i><b>9.4</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="unsupervised.html"><a href="unsupervised.html"><i class="fa fa-check"></i><b>10</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="10.1" data-path="unsupervised.html"><a href="unsupervised.html#dim-reduce"><i class="fa fa-check"></i><b>10.1</b> Dimensionality Reduction</a><ul>
<li class="chapter" data-level="10.1.1" data-path="unsupervised.html"><a href="unsupervised.html#pca"><i class="fa fa-check"></i><b>10.1.1</b> Principal Component Analysis</a></li>
<li class="chapter" data-level="10.1.2" data-path="unsupervised.html"><a href="unsupervised.html#dimensionality-reduction-preliminaries"><i class="fa fa-check"></i><b>10.1.2</b> Dimensionality Reduction Preliminaries</a></li>
<li class="chapter" data-level="10.1.3" data-path="unsupervised.html"><a href="unsupervised.html#latent-variable-generative-approaches"><i class="fa fa-check"></i><b>10.1.3</b> Latent Variable Generative Approaches</a></li>
<li class="chapter" data-level="10.1.4" data-path="unsupervised.html"><a href="unsupervised.html#purely-algorithmic-approaches"><i class="fa fa-check"></i><b>10.1.4</b> Purely Algorithmic Approaches</a></li>
<li class="chapter" data-level="10.1.5" data-path="unsupervised.html"><a href="unsupervised.html#dimensionality-reduction-in-r"><i class="fa fa-check"></i><b>10.1.5</b> Dimensionality Reduction in R</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="unsupervised.html"><a href="unsupervised.html#cluster"><i class="fa fa-check"></i><b>10.2</b> Clustering</a><ul>
<li class="chapter" data-level="10.2.1" data-path="unsupervised.html"><a href="unsupervised.html#latent-variable-generative-approaches-1"><i class="fa fa-check"></i><b>10.2.1</b> Latent Variable Generative Approaches</a></li>
<li class="chapter" data-level="10.2.2" data-path="unsupervised.html"><a href="unsupervised.html#purely-algorithmic-approaches-1"><i class="fa fa-check"></i><b>10.2.2</b> Purely Algorithmic Approaches</a></li>
<li class="chapter" data-level="10.2.3" data-path="unsupervised.html"><a href="unsupervised.html#clustering-in-r"><i class="fa fa-check"></i><b>10.2.3</b> Clustering in R</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="unsupervised.html"><a href="unsupervised.html#bibliographic-notes-7"><i class="fa fa-check"></i><b>10.3</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="10.4" data-path="unsupervised.html"><a href="unsupervised.html#practice-yourself-7"><i class="fa fa-check"></i><b>10.4</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="plotting.html"><a href="plotting.html"><i class="fa fa-check"></i><b>11</b> Plotting</a><ul>
<li class="chapter" data-level="11.1" data-path="plotting.html"><a href="plotting.html#the-graphics-system"><i class="fa fa-check"></i><b>11.1</b> The graphics System</a><ul>
<li class="chapter" data-level="11.1.1" data-path="plotting.html"><a href="plotting.html#using-existing-plotting-functions"><i class="fa fa-check"></i><b>11.1.1</b> Using Existing Plotting Functions</a></li>
<li class="chapter" data-level="11.1.2" data-path="plotting.html"><a href="plotting.html#exporting-a-plot"><i class="fa fa-check"></i><b>11.1.2</b> Exporting a Plot</a></li>
<li class="chapter" data-level="11.1.3" data-path="plotting.html"><a href="plotting.html#fancy"><i class="fa fa-check"></i><b>11.1.3</b> Fancy graphics Examples</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="plotting.html"><a href="plotting.html#the-ggplot2-system"><i class="fa fa-check"></i><b>11.2</b> The ggplot2 System</a><ul>
<li class="chapter" data-level="11.2.1" data-path="plotting.html"><a href="plotting.html#extensions-of-the-ggplot2-system"><i class="fa fa-check"></i><b>11.2.1</b> Extensions of the ggplot2 System</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="plotting.html"><a href="plotting.html#interactive-graphics"><i class="fa fa-check"></i><b>11.3</b> Interactive Graphics</a><ul>
<li class="chapter" data-level="11.3.1" data-path="plotting.html"><a href="plotting.html#plotly"><i class="fa fa-check"></i><b>11.3.1</b> Plotly</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="plotting.html"><a href="plotting.html#bibliographic-notes-8"><i class="fa fa-check"></i><b>11.4</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="11.5" data-path="plotting.html"><a href="plotting.html#practice-yourself-8"><i class="fa fa-check"></i><b>11.5</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="report.html"><a href="report.html"><i class="fa fa-check"></i><b>12</b> Reports</a><ul>
<li class="chapter" data-level="12.1" data-path="report.html"><a href="report.html#knitr"><i class="fa fa-check"></i><b>12.1</b> knitr</a><ul>
<li class="chapter" data-level="12.1.1" data-path="report.html"><a href="report.html#installation"><i class="fa fa-check"></i><b>12.1.1</b> Installation</a></li>
<li class="chapter" data-level="12.1.2" data-path="report.html"><a href="report.html#pandoc-markdown"><i class="fa fa-check"></i><b>12.1.2</b> Pandoc Markdown</a></li>
<li class="chapter" data-level="12.1.3" data-path="report.html"><a href="report.html#rmarkdown"><i class="fa fa-check"></i><b>12.1.3</b> Rmarkdown</a></li>
<li class="chapter" data-level="12.1.4" data-path="report.html"><a href="report.html#compiling"><i class="fa fa-check"></i><b>12.1.4</b> Compiling</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="report.html"><a href="report.html#bookdown"><i class="fa fa-check"></i><b>12.2</b> bookdown</a></li>
<li class="chapter" data-level="12.3" data-path="report.html"><a href="report.html#shiny"><i class="fa fa-check"></i><b>12.3</b> Shiny</a><ul>
<li class="chapter" data-level="12.3.1" data-path="report.html"><a href="report.html#installation-1"><i class="fa fa-check"></i><b>12.3.1</b> Installation</a></li>
<li class="chapter" data-level="12.3.2" data-path="report.html"><a href="report.html#the-basics-of-shiny"><i class="fa fa-check"></i><b>12.3.2</b> The Basics of Shiny</a></li>
<li class="chapter" data-level="12.3.3" data-path="report.html"><a href="report.html#beyond-the-basics"><i class="fa fa-check"></i><b>12.3.3</b> Beyond the Basics</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="report.html"><a href="report.html#flexdashboard"><i class="fa fa-check"></i><b>12.4</b> Flexdashboard</a></li>
<li class="chapter" data-level="12.5" data-path="report.html"><a href="report.html#bibliographic-notes-9"><i class="fa fa-check"></i><b>12.5</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="12.6" data-path="report.html"><a href="report.html#practice-yourself-9"><i class="fa fa-check"></i><b>12.6</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="hadley.html"><a href="hadley.html"><i class="fa fa-check"></i><b>13</b> The Hadleyverse</a><ul>
<li class="chapter" data-level="13.1" data-path="hadley.html"><a href="hadley.html#readr"><i class="fa fa-check"></i><b>13.1</b> readr</a></li>
<li class="chapter" data-level="13.2" data-path="hadley.html"><a href="hadley.html#dplyr"><i class="fa fa-check"></i><b>13.2</b> dplyr</a></li>
<li class="chapter" data-level="13.3" data-path="hadley.html"><a href="hadley.html#tidyr"><i class="fa fa-check"></i><b>13.3</b> tidyr</a></li>
<li class="chapter" data-level="13.4" data-path="hadley.html"><a href="hadley.html#reshape2"><i class="fa fa-check"></i><b>13.4</b> reshape2</a></li>
<li class="chapter" data-level="13.5" data-path="hadley.html"><a href="hadley.html#stringr"><i class="fa fa-check"></i><b>13.5</b> stringr</a></li>
<li class="chapter" data-level="13.6" data-path="hadley.html"><a href="hadley.html#anytime"><i class="fa fa-check"></i><b>13.6</b> anytime</a></li>
<li class="chapter" data-level="13.7" data-path="hadley.html"><a href="hadley.html#biblipgraphic-notes-1"><i class="fa fa-check"></i><b>13.7</b> Biblipgraphic Notes</a></li>
<li class="chapter" data-level="13.8" data-path="hadley.html"><a href="hadley.html#practice-yourself-10"><i class="fa fa-check"></i><b>13.8</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="sparse.html"><a href="sparse.html"><i class="fa fa-check"></i><b>14</b> Sparse Representations</a><ul>
<li class="chapter" data-level="14.1" data-path="sparse.html"><a href="sparse.html#sparse-matrix-representations"><i class="fa fa-check"></i><b>14.1</b> Sparse Matrix Representations</a><ul>
<li class="chapter" data-level="14.1.1" data-path="sparse.html"><a href="sparse.html#coo"><i class="fa fa-check"></i><b>14.1.1</b> Coordinate List Representation</a></li>
<li class="chapter" data-level="14.1.2" data-path="sparse.html"><a href="sparse.html#compressed-column-oriented-representation"><i class="fa fa-check"></i><b>14.1.2</b> Compressed Column Oriented Representation</a></li>
<li class="chapter" data-level="14.1.3" data-path="sparse.html"><a href="sparse.html#compressed-row-oriented-representation"><i class="fa fa-check"></i><b>14.1.3</b> Compressed Row Oriented Representation</a></li>
<li class="chapter" data-level="14.1.4" data-path="sparse.html"><a href="sparse.html#sparse-algorithms"><i class="fa fa-check"></i><b>14.1.4</b> Sparse Algorithms</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="sparse.html"><a href="sparse.html#sparse-matrices-and-sparse-models-in-r"><i class="fa fa-check"></i><b>14.2</b> Sparse Matrices and Sparse Models in R</a><ul>
<li class="chapter" data-level="14.2.1" data-path="sparse.html"><a href="sparse.html#the-matrix-package"><i class="fa fa-check"></i><b>14.2.1</b> The Matrix Package</a></li>
<li class="chapter" data-level="14.2.2" data-path="sparse.html"><a href="sparse.html#the-matrixmodels-package"><i class="fa fa-check"></i><b>14.2.2</b> The MatrixModels Package</a></li>
<li class="chapter" data-level="14.2.3" data-path="sparse.html"><a href="sparse.html#the-glmnet-package"><i class="fa fa-check"></i><b>14.2.3</b> The glmnet Package</a></li>
<li class="chapter" data-level="14.2.4" data-path="sparse.html"><a href="sparse.html#the-sparsem-package"><i class="fa fa-check"></i><b>14.2.4</b> The SparseM Package</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="sparse.html"><a href="sparse.html#bibliographic-notes-10"><i class="fa fa-check"></i><b>14.3</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="14.4" data-path="sparse.html"><a href="sparse.html#practice-yourself-11"><i class="fa fa-check"></i><b>14.4</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="memory.html"><a href="memory.html"><i class="fa fa-check"></i><b>15</b> Memory Efficiency</a><ul>
<li class="chapter" data-level="15.1" data-path="memory.html"><a href="memory.html#efficient-computing-from-ram"><i class="fa fa-check"></i><b>15.1</b> Efficient Computing from RAM</a><ul>
<li class="chapter" data-level="15.1.1" data-path="memory.html"><a href="memory.html#summary-statistics-from-ram"><i class="fa fa-check"></i><b>15.1.1</b> Summary Statistics from RAM</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="memory.html"><a href="memory.html#computing-from-a-database"><i class="fa fa-check"></i><b>15.2</b> Computing from a Database</a></li>
<li class="chapter" data-level="15.3" data-path="memory.html"><a href="memory.html#file-structure"><i class="fa fa-check"></i><b>15.3</b> Computing From Efficient File Structrures</a><ul>
<li class="chapter" data-level="15.3.1" data-path="memory.html"><a href="memory.html#bigmemory"><i class="fa fa-check"></i><b>15.3.1</b> bigmemory</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="memory.html"><a href="memory.html#ff"><i class="fa fa-check"></i><b>15.4</b> ff</a></li>
<li class="chapter" data-level="15.5" data-path="memory.html"><a href="memory.html#matter"><i class="fa fa-check"></i><b>15.5</b> matter</a></li>
<li class="chapter" data-level="15.6" data-path="memory.html"><a href="memory.html#iotools"><i class="fa fa-check"></i><b>15.6</b> iotools</a></li>
<li class="chapter" data-level="15.7" data-path="memory.html"><a href="memory.html#hdf5"><i class="fa fa-check"></i><b>15.7</b> HDF5</a></li>
<li class="chapter" data-level="15.8" data-path="memory.html"><a href="memory.html#delayedarray"><i class="fa fa-check"></i><b>15.8</b> DelayedArray</a><ul>
<li class="chapter" data-level="15.8.1" data-path="memory.html"><a href="memory.html#delayedmatrixstats"><i class="fa fa-check"></i><b>15.8.1</b> DelayedMatrixStats</a></li>
<li class="chapter" data-level="15.8.2" data-path="memory.html"><a href="memory.html#beachmat"><i class="fa fa-check"></i><b>15.8.2</b> beachmat</a></li>
<li class="chapter" data-level="15.8.3" data-path="memory.html"><a href="memory.html#restfulse"><i class="fa fa-check"></i><b>15.8.3</b> restfulSE</a></li>
</ul></li>
<li class="chapter" data-level="15.9" data-path="memory.html"><a href="memory.html#computing-from-a-distributed-file-system"><i class="fa fa-check"></i><b>15.9</b> Computing from a Distributed File System</a></li>
<li class="chapter" data-level="15.10" data-path="memory.html"><a href="memory.html#bibliographic-notes-11"><i class="fa fa-check"></i><b>15.10</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="15.11" data-path="memory.html"><a href="memory.html#practice-yourself-12"><i class="fa fa-check"></i><b>15.11</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="parallel.html"><a href="parallel.html"><i class="fa fa-check"></i><b>16</b> Parallel Computing</a><ul>
<li class="chapter" data-level="16.1" data-path="parallel.html"><a href="parallel.html#implicit-parallelism"><i class="fa fa-check"></i><b>16.1</b> Implicit Parallelism</a></li>
<li class="chapter" data-level="16.2" data-path="parallel.html"><a href="parallel.html#explicit-parallelism"><i class="fa fa-check"></i><b>16.2</b> Explicit Parallelism</a><ul>
<li class="chapter" data-level="16.2.1" data-path="parallel.html"><a href="parallel.html#caution-implicit-with-explicit-parallelism"><i class="fa fa-check"></i><b>16.2.1</b> Caution: Implicit with Explicit Parallelism</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="parallel.html"><a href="parallel.html#bibliographic-notes-12"><i class="fa fa-check"></i><b>16.3</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="16.4" data-path="parallel.html"><a href="parallel.html#practice-yourself-13"><i class="fa fa-check"></i><b>16.4</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="algebra.html"><a href="algebra.html"><i class="fa fa-check"></i><b>17</b> Numerical Linear Algebra</a><ul>
<li class="chapter" data-level="17.1" data-path="algebra.html"><a href="algebra.html#lu-factorization"><i class="fa fa-check"></i><b>17.1</b> LU Factorization</a></li>
<li class="chapter" data-level="17.2" data-path="algebra.html"><a href="algebra.html#cholesky-factorization"><i class="fa fa-check"></i><b>17.2</b> Cholesky Factorization</a></li>
<li class="chapter" data-level="17.3" data-path="algebra.html"><a href="algebra.html#qr-factorization"><i class="fa fa-check"></i><b>17.3</b> QR Factorization</a></li>
<li class="chapter" data-level="17.4" data-path="algebra.html"><a href="algebra.html#singular-value-factorization"><i class="fa fa-check"></i><b>17.4</b> Singular Value Factorization</a></li>
<li class="chapter" data-level="17.5" data-path="algebra.html"><a href="algebra.html#iterative-methods"><i class="fa fa-check"></i><b>17.5</b> Iterative Methods</a></li>
<li class="chapter" data-level="17.6" data-path="algebra.html"><a href="algebra.html#solving-ols"><i class="fa fa-check"></i><b>17.6</b> Solving the OLS Problem</a></li>
<li class="chapter" data-level="17.7" data-path="algebra.html"><a href="algebra.html#bibliographic-notes-13"><i class="fa fa-check"></i><b>17.7</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="17.8" data-path="algebra.html"><a href="algebra.html#practice-yourself-14"><i class="fa fa-check"></i><b>17.8</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="convex.html"><a href="convex.html"><i class="fa fa-check"></i><b>18</b> Convex Optimization</a><ul>
<li class="chapter" data-level="18.1" data-path="convex.html"><a href="convex.html#theoretical-backround"><i class="fa fa-check"></i><b>18.1</b> Theoretical Backround</a></li>
<li class="chapter" data-level="18.2" data-path="convex.html"><a href="convex.html#optimizing-with-r"><i class="fa fa-check"></i><b>18.2</b> Optimizing with R</a><ul>
<li class="chapter" data-level="18.2.1" data-path="convex.html"><a href="convex.html#the-optim-function"><i class="fa fa-check"></i><b>18.2.1</b> The optim Function</a></li>
<li class="chapter" data-level="18.2.2" data-path="convex.html"><a href="convex.html#the-nloptr-package"><i class="fa fa-check"></i><b>18.2.2</b> The nloptr Package</a></li>
<li class="chapter" data-level="18.2.3" data-path="convex.html"><a href="convex.html#minqa-package"><i class="fa fa-check"></i><b>18.2.3</b> minqa Package</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="convex.html"><a href="convex.html#bibliographic-notes-14"><i class="fa fa-check"></i><b>18.3</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="18.4" data-path="convex.html"><a href="convex.html#practice-yourself-15"><i class="fa fa-check"></i><b>18.4</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="rcpp.html"><a href="rcpp.html"><i class="fa fa-check"></i><b>19</b> RCpp</a><ul>
<li class="chapter" data-level="19.1" data-path="rcpp.html"><a href="rcpp.html#bibliographic-notes-15"><i class="fa fa-check"></i><b>19.1</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="19.2" data-path="rcpp.html"><a href="rcpp.html#practice-yourself-16"><i class="fa fa-check"></i><b>19.2</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="debugging.html"><a href="debugging.html"><i class="fa fa-check"></i><b>20</b> Debugging Tools</a><ul>
<li class="chapter" data-level="20.1" data-path="debugging.html"><a href="debugging.html#bibliographic-notes-16"><i class="fa fa-check"></i><b>20.1</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="20.2" data-path="debugging.html"><a href="debugging.html#practice-yourself-17"><i class="fa fa-check"></i><b>20.2</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="datatable.html"><a href="datatable.html"><i class="fa fa-check"></i><b>21</b> data.table</a><ul>
<li class="chapter" data-level="21.1" data-path="datatable.html"><a href="datatable.html#make-your-own-variables"><i class="fa fa-check"></i><b>21.1</b> Make your own variables</a></li>
<li class="chapter" data-level="21.2" data-path="datatable.html"><a href="datatable.html#join"><i class="fa fa-check"></i><b>21.2</b> Join</a></li>
<li class="chapter" data-level="21.3" data-path="datatable.html"><a href="datatable.html#reshaping-data"><i class="fa fa-check"></i><b>21.3</b> Reshaping data</a><ul>
<li class="chapter" data-level="21.3.1" data-path="datatable.html"><a href="datatable.html#wide-to-long"><i class="fa fa-check"></i><b>21.3.1</b> Wide to long</a></li>
<li class="chapter" data-level="21.3.2" data-path="datatable.html"><a href="datatable.html#long-to-wide"><i class="fa fa-check"></i><b>21.3.2</b> Long to wide</a></li>
</ul></li>
<li class="chapter" data-level="21.4" data-path="datatable.html"><a href="datatable.html#bibliographic-notes-17"><i class="fa fa-check"></i><b>21.4</b> Bibliographic Notes</a></li>
<li class="chapter" data-level="21.5" data-path="datatable.html"><a href="datatable.html#practice-yourself-18"><i class="fa fa-check"></i><b>21.5</b> Practice Yourself</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="bib.html"><a href="bib.html"><i class="fa fa-check"></i><b>22</b> Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">R (BGU course)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="supervised" class="section level1">
<h1><span class="header-section-number">Chapter 9</span> Supervised Learning</h1>
<p>Machine learning is very similar to statistics, but it is certainly not the same. As the name suggests, in machine learning we want machines to learn. This means that we want to replace hard-coded expert algorithm, with data-driven self-learned algorithm.</p>
<p>There are many learning setups, that depend on what information is available to the machine. The most common setup, discussed in this chapter, is <em>supervised learning</em>. The name takes from the fact that by giving the machine data samples with known inputs (a.k.a. features) and desired outputs (a.k.a. labels), the human is effectively supervising the learning. If we think of the inputs as predictors, and outcomes as predicted, it is no wonder that supervised learning is very similar to statistical prediction. When asked “are these the same?” I like to give the example of internet fraud. If you take a sample of fraud “attacks”, a statistical formulation of the problem is highly unlikely. This is because fraud events are not randomly drawn from some distribution, but rather, arrive from an adversary learning the defenses and adapting to it. This instance of supervised learning is more similar to game theory than statistics.</p>
<p>Other types of machine learning problems include <span class="citation">(Sammut and Webb <a href="#ref-sammut2011encyclopedia">2011</a>)</span>:</p>
<ul>
<li><p><strong>Unsupervised learning</strong>: See Chapter <a href="unsupervised.html#unsupervised">10</a>.</p></li>
<li><p><strong>Semi supervised learning</strong>: Where only part of the samples are labeled. A.k.a. <em>co-training</em>, <em>learning from labeled and unlabeled data</em>, <em>transductive learning</em>.</p></li>
<li><p><strong>Active learning</strong>: Where the machine is allowed to query the user for labels. Very similar to <em>adaptive design of experiments</em>.</p></li>
<li><p><strong>Learning on a budget</strong>: A version of active learning where querying for labels induces variable costs.</p></li>
<li><p><strong>Weak learning</strong>: A version of supervised learning where the labels are given not by an expert, but rather by some heuristic rule. Example: mass-labeling cyber attacks by a rule based software, instead of a manual inspection.</p></li>
<li><p><strong>Reinforcement learning</strong>:<br />
Similar to active learning, in that the machine may query for labels. Different from active learning, in that the machine does not receive labels, but <em>rewards</em>.</p></li>
<li><p><strong>Structure learning</strong>: When predicting objects with structure such as dependent vectors, graphs, images, tensors, etc.</p></li>
<li><p><strong>Manifold learning</strong>: An instance of unsupervised learning, where the goal is to reduce the dimension of the data by embedding it into a lower dimensional manifold. A.k.a. <em>support estimation</em>.</p></li>
<li><p><strong>Learning to learn</strong>: Deals with the carriage of “experience” from one learning problem to another. A.k.a. <em>cummulative learning</em>, <em>knowledge transfer</em>, and <em>meta learning</em>.</p></li>
</ul>
<div id="problem-setup-3" class="section level2">
<h2><span class="header-section-number">9.1</span> Problem Setup</h2>
<p>We now present the <em>empirical risk minimization</em> (ERM) approach to supervised learning, a.k.a. <em>M-estimation</em> in the statistical literature.</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> We do not discuss purely algorithmic approaches such as K-nearest neighbour and <em>kernel smoothing</em> due to space constraints. For a broader review of supervised learning, see the Bibliographic Notes.
</div>


<div class="example">
<span id="exm:rental-prices" class="example"><strong>Example 9.1  (Rental Prices)  </strong></span>Consider the problem of predicting if a mail is spam or not based on its attributes: length, number of exclamation marks, number of recipients, etc.
</div>

<p>Given <span class="math inline">\(n\)</span> samples with inputs <span class="math inline">\(x\)</span> from some space <span class="math inline">\(\mathcal{X}\)</span> and desired outcome, <span class="math inline">\(y\)</span>, from some space <span class="math inline">\(\mathcal{Y}\)</span>. In our example, <span class="math inline">\(y\)</span> is the spam/no-spam label, and <span class="math inline">\(x\)</span> is a vector of the mail’s attributes. Samples, <span class="math inline">\((x,y)\)</span> have some distribution we denote <span class="math inline">\(P\)</span>. We want to learn a function that maps inputs to outputs, i.e., that classifies to spam given. This function is called a <em>hypothesis</em>, or <em>predictor</em>, denoted <span class="math inline">\(f\)</span>, that belongs to a hypothesis class <span class="math inline">\(\mathcal{F}\)</span> such that <span class="math inline">\(f:\mathcal{X} \to \mathcal{Y}\)</span>. We also choose some other function that fines us for erroneous prediction. This function is called the <em>loss</em>, and we denote it by <span class="math inline">\(l:\mathcal{Y}\times \mathcal{Y} \to \mathbb{R}^+\)</span>.</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> The <em>hypothesis</em> in machine learning is only vaguely related the <em>hypothesis</em> in statistical testing, which is quite confusing.
</div>


<div class="remark">
 <span class="remark"><em>Remark. </em></span> The <em>hypothesis</em> in machine learning is not a bona-fide <em>statistical model</em> since we don’t assume it is the data generating process, but rather some function which we choose for its good predictive performance.
</div>

<p>The fundamental task in supervised (statistical) learning is to recover a hypothesis that minimizes the average loss in the sample, and not in the population. This is know as the <em>risk minimization problem</em>.</p>

<div class="definition">
<span id="def:unnamed-chunk-182" class="definition"><strong>Definition 9.1  (Risk Function)  </strong></span>The <em>risk function</em>, a.k.a. <em>generalization error</em>, or <em>test error</em>, is the population average loss of a predictor <span class="math inline">\(f\)</span>:
<span class="math display">\[\begin{align}
  R(f):=\mathbb{E}_P[l(f(x),y)].
\end{align}\]</span>
</div>

The best predictor, is the risk minimizer:
<span class="math display" id="eq:risk">\[\begin{align}
  f^* := argmin_f \{R(f)\}.
  \tag{9.1}  
\end{align}\]</span>
<p>To make things more explicit:</p>
<ul>
<li><span class="math inline">\(f\)</span> may be a linear function of the attributes, so that it may be indexed simply with its coefficient vector <span class="math inline">\(\beta\)</span>.</li>
<li><span class="math inline">\(l\)</span> may be a squared error loss: <span class="math inline">\(l(f(x),y):=(f(x)-y)^2\)</span>.</li>
</ul>
Under these conditions, the best predictor <span class="math inline">\(f^* \in \mathcal{F}\)</span> from problem <a href="supervised.html#eq:risk">(9.1)</a> is to
<span class="math display">\[\begin{align}
  f^* := argmin_\beta \{ \mathbb{E}_{P(x,y)}[(x&#39;\beta-y)^2] \}.
\end{align}\]</span>
<p>Another fundamental problem is that we do not know the distribution of all possible inputs and outputs, <span class="math inline">\(P\)</span>. We typically only have a sample of <span class="math inline">\((x_i,y_i), i=1,\dots,n\)</span>. We thus state the <em>empirical</em> counterpart of <a href="supervised.html#eq:risk">(9.1)</a>, which consists of minimizing the average loss. This is known as the <em>empirical risk miminization</em> problem (ERM).</p>

<div class="definition">
<span id="def:unnamed-chunk-183" class="definition"><strong>Definition 9.2  (Empirical Risk)  </strong></span>The <em>empirical risk function</em>, a.k.a. <em>in-sample error</em>, or <em>train error</em>, is the sample average loss of a predictor <span class="math inline">\(f\)</span>:
<span class="math display">\[\begin{align}
  R_n(f):= 1/n \sum_i l(f(x_i),y_i).
\end{align}\]</span>
</div>

A good candidate proxy for <span class="math inline">\(f^*\)</span> is its empirical counterpart, <span class="math inline">\(\hat f\)</span>, known as the <em>empirical risk minimizer</em>:
<span class="math display" id="eq:erm">\[\begin{align}
  \hat f := argmin_f \{ R_n(f) \}.
  \tag{9.2}  
\end{align}\]</span>
Making things more explicit again by using a linear hypothesis with squared loss, we see that the empirical risk minimization problem collapses to an ordinary least-squares problem:
<span class="math display">\[\begin{align}
  \hat f := argmin_\beta \{ \sum_i (x_i&#39;\beta - y_i)^2 \}.
\end{align}\]</span>
<p>When data samples are assumingly independent, then maximum likelihood estimation is also an instance of ERM, when using the (negative) log likelihood as the loss function.</p>
<p>If we don’t assume any structure on the hypothesis, <span class="math inline">\(f\)</span>, then <span class="math inline">\(\hat f\)</span> from <a href="supervised.html#eq:erm">(9.2)</a> will interpolate the data, and <span class="math inline">\(\hat f\)</span> will be a very bad predictor. We say, it will <em>overfit</em> the observed data, and will have bad performance on new data.</p>
<p>We have several ways to avoid overfitting:</p>
<ol style="list-style-type: decimal">
<li>Restrict the hypothesis class <span class="math inline">\(\mathcal{F}\)</span> (such as linear functions).</li>
<li>Penalize for the complexity of <span class="math inline">\(f\)</span>. The penalty denoted by <span class="math inline">\(\Vert f \Vert\)</span>.</li>
<li>Unbiased risk estimation: <span class="math inline">\(R_n(f)\)</span> is not an unbiased estimator of <span class="math inline">\(R(f)\)</span>. Why? Think of estimating the mean with the sample minimum… Because <span class="math inline">\(R_n(f)\)</span> is downward biased, we may add some correction term, or compute <span class="math inline">\(R_n(f)\)</span> on different data than the one used to recover <span class="math inline">\(\hat f\)</span>.</li>
</ol>
<p>Almost all ERM algorithms consist of some combination of all the three methods above.</p>
<div id="common-hypothesis-classes" class="section level3">
<h3><span class="header-section-number">9.1.1</span> Common Hypothesis Classes</h3>
<p>Some common hypothesis classes, <span class="math inline">\(\mathcal{F}\)</span>, with restricted complexity, are:</p>
<ol style="list-style-type: decimal">
<li><strong>Linear hypotheses</strong>: such as linear models, GLMs, and (linear) support vector machines (SVM).</li>
<li><strong>Neural networks</strong>: a.k.a. <em>feed-forward</em> neural nets, <em>artificial</em> neural nets, and the celebrated class of <em>deep</em> neural nets.</li>
<li><strong>Tree</strong>: a.k.a. <em>decision rules</em>, is a class of hypotheses which can be stated as “if-then” rules.</li>
<li><strong>Reproducing Kernel Hilbert Space</strong>: a.k.a. RKHS, is a subset of “the space of all functions<a href="#fn19" class="footnoteRef" id="fnref19"><sup>19</sup></a>” that is both large enough to capture very complicated relations, but small enough so that it is less prone to overfitting, and also surprisingly simple to compute with.</li>
<li><strong>Ensembles</strong>: a “meta” hypothesis class, which consists of taking multiple hypotheses, possibly from different classes, and combining them.</li>
</ol>
</div>
<div id="common-complexity-penalties" class="section level3">
<h3><span class="header-section-number">9.1.2</span> Common Complexity Penalties</h3>
<p>The most common complexity penalty applies to classes that have a finite dimensional parametric representation, such as the class of linear predictors, parametrized via its coefficients <span class="math inline">\(\beta\)</span>. In such classes we may penalize for the norm of the parameters. Common penalties include:</p>
<ol style="list-style-type: decimal">
<li><strong>Ridge penalty</strong>: penalizing the <span class="math inline">\(l_2\)</span> norm of the parameter. I.e. <span class="math inline">\(\Vert f \Vert=\Vert \beta \Vert_2^2=\sum_j \beta_j^2\)</span>.</li>
<li><strong>Lasso penalty</strong>: penalizing the <span class="math inline">\(l_1\)</span> norm of the parameter. I.e., <span class="math inline">\(\Vert f \Vert=\Vert \beta \Vert_1=\sum_j |\beta_j|\)</span></li>
<li><strong>Elastic net</strong>: a combination of the lasso and ridge penalty. I.e. ,<span class="math inline">\(\Vert f \Vert= \alpha \Vert \beta \Vert_2^2 + (1-\alpha) \Vert \beta \Vert_1\)</span>.</li>
<li><strong>Function Norms</strong>: If the hypothesis class <span class="math inline">\(\mathcal{F}\)</span> does not admit a finite dimensional index to penalize representation, we may penalize it with some functional norm such as <span class="math inline">\(\Vert f \Vert=\sqrt{\int f(t)^2 dt}\)</span>.</li>
</ol>
</div>
<div id="unbiased-risk-estimation" class="section level3">
<h3><span class="header-section-number">9.1.3</span> Unbiased Risk Estimation</h3>
<p>The fundamental problem of overfitting, is that the empirical risk, <span class="math inline">\(R_n(\hat f)\)</span>, is downward biased to the population risk, <span class="math inline">\(R(\hat f)\)</span>. Why is that? Unbiased estimation of <span class="math inline">\(R(f)\)</span> broadly fall under: (a) purely algorithmic <em>resampling</em> based approaches, and (b) theory driven estimators.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Train-Validate-Test</strong>: The simplest form of algorithmic validation is to split the data. A <em>train</em> set to train/estimate/learn <span class="math inline">\(\hat f\)</span>. A <em>validation</em> set to compute the out-of-sample expected loss, <span class="math inline">\(R(\hat f)\)</span>, and pick the best performing predictor. A <em>test</em> sample to compute the out-of-sample performance of the selected hypothesis. This is a very simple approach, but it is very “data inefficient”, thus motivating the next method.</p></li>
<li><p><strong>V-Fold Cross Validation</strong>: By far the most popular algorithmic unbiased risk estimator; in <em>V-fold CV</em> we “fold” the data into <span class="math inline">\(V\)</span> non-overlapping sets. For each of the <span class="math inline">\(V\)</span> sets, we learn <span class="math inline">\(\hat f\)</span> with the non-selected fold, and assess <span class="math inline">\(R(\hat f)\)</span>) on the selected fold. We then aggregate results over the <span class="math inline">\(V\)</span> folds, typically by averaging.</p></li>
<li><p><strong>AIC</strong>: Akaike’s information criterion (AIC) is a theory driven correction of the empirical risk, so that it is unbiased to the true risk. It is appropriate when using the likelihood loss.</p></li>
<li><p><strong>Cp</strong>: Mallow’s Cp is an instance of AIC for likelihood loss under normal noise.</p></li>
</ol>
<p>Other theory driven unbiased risk estimators include the <em>Bayesian Information Criterion</em> (BIC, aka SBC, aka SBIC), the <em>Minimum Description Length</em> (MDL), <em>Vapnic’s Structural Risk Minimization</em> (SRM), the <em>Deviance Information Criterion</em> (DIC), and the <em>Hannan-Quinn Information Criterion</em> (HQC).</p>
<p>Other resampling based unbiased risk estimators include resampling <strong>without replacement</strong> algorithms like <em>delete-d cross validation</em> with its many variations, and <strong>resampling with replacement</strong>, like the <em>bootstrap</em>, with its many variations.</p>
</div>
<div id="collecting-the-pieces" class="section level3">
<h3><span class="header-section-number">9.1.4</span> Collecting the Pieces</h3>
An ERM problem with regularization will look like
<span class="math display" id="eq:erm-regularized">\[\begin{align}
  \hat f := argmin_{f \in \mathcal{F}} \{ R_n(f)  + \lambda \Vert f \Vert \}.
  \tag{9.3}  
\end{align}\]</span>
<p>Collecting ideas from the above sections, a typical supervised learning pipeline will include: choosing the hypothesis class, choosing the penalty function and level, unbiased risk estimator. We emphasize that choosing the penalty function, <span class="math inline">\(\Vert f \Vert\)</span> is not enough, and we need to choose how “hard” to apply it. This if known as the <em>regularization level</em>, denoted by <span class="math inline">\(\lambda\)</span> in Eq.<a href="supervised.html#eq:erm-regularized">(9.3)</a>.</p>
<p>Examples of such combos include:</p>
<ol style="list-style-type: decimal">
<li>Linear regression, no penalty, train-validate test.</li>
<li>Linear regression, no penalty, AIC.</li>
<li>Linear regression, <span class="math inline">\(l_2\)</span> penalty, V-fold CV. This combo is typically known as <em>ridge regression</em>.</li>
<li>Linear regression, <span class="math inline">\(l_1\)</span> penalty, V-fold CV. This combo is typically known as <em>lasso regression</em>.</li>
<li>Linear regression, <span class="math inline">\(l_1\)</span> and <span class="math inline">\(l_2\)</span> penalty, V-fold CV. This combo is typically known as <em>elastic net regression</em>.</li>
<li>Logistic regression, <span class="math inline">\(l_2\)</span> penalty, V-fold CV.</li>
<li>SVM classification, <span class="math inline">\(l_2\)</span> penalty, V-fold CV.</li>
<li>Deep network, no penalty, V-fold CV.</li>
<li>Unrestricted, $^2 f _2 $, V-fold CV. This combo is typically known as a <em>smoothing spline</em>.</li>
</ol>
<p>For fans of statistical hypothesis testing we will also emphasize: Testing and prediction are related, but are not the same:</p>
<ul>
<li>In the current chapter, we do not claim our models, <span class="math inline">\(f\)</span>, are generative. I.e., we do not claim that there is some causal relation between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. We only claim that <span class="math inline">\(x\)</span> predicts <span class="math inline">\(y\)</span>.</li>
<li>It is possible that we will want to ignore a significant predictor, and add a non-significant one <span class="citation">(Foster and Stine <a href="#ref-foster2004variable">2004</a>)</span>.</li>
<li>Some authors will use hypothesis testing as an initial screening for candidate predictors. This is a useful heuristic, but that is all it is– a heuristic. It may also fail miserably if predictors are linearly dependent (a.k.a. multicollinear).</li>
</ul>
</div>
</div>
<div id="supervised-learning-in-r" class="section level2">
<h2><span class="header-section-number">9.2</span> Supervised Learning in R</h2>
<p>At this point, we have a rich enough language to do supervised learning with R.</p>
<p>In these examples, I will use two data sets from the <strong>ElemStatLearn</strong> package, that accompanies the seminal book by <span class="citation">Friedman, Hastie, and Tibshirani (<a href="#ref-friedman2001elements">2001</a>)</span>. I use the <code>spam</code> data for categorical predictions, and <code>prostate</code> for continuous predictions. In <code>spam</code> we will try to decide if a mail is spam or not. In <code>prostate</code> we will try to predict the size of a cancerous tumor. You can now call <code>?prostate</code> and <code>?spam</code> to learn more about these data sets.</p>
<p>Some boring pre-processing.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ElemStatLearn) 
<span class="kw">data</span>(<span class="st">&quot;prostate&quot;</span>)
<span class="kw">data</span>(<span class="st">&quot;spam&quot;</span>)
<span class="kw">library</span>(magrittr) <span class="co"># for piping</span>

<span class="co"># Preparing prostate data</span>
prostate &lt;-<span class="st"> </span><span class="kw">as.data.table</span>(prostate)
prostate.train &lt;-<span class="st"> </span>prostate[train<span class="op">==</span><span class="ot">TRUE</span>, <span class="op">-</span><span class="st">&quot;train&quot;</span>]
prostate.test &lt;-<span class="st"> </span>prostate[train<span class="op">!=</span><span class="ot">TRUE</span>, <span class="op">-</span><span class="st">&quot;train&quot;</span>]
y.train &lt;-<span class="st"> </span>prostate.train<span class="op">$</span>lcavol
X.train &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(prostate.train[, <span class="op">-</span><span class="st">&#39;lcavol&#39;</span>] )
y.test &lt;-<span class="st"> </span>prostate.test<span class="op">$</span>lcavol 
X.test &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(prostate.test[, <span class="op">-</span><span class="st">&#39;lcavol&#39;</span>] )

<span class="co"># Preparing spam data:</span>
n &lt;-<span class="st"> </span><span class="kw">nrow</span>(spam)
train.prop &lt;-<span class="st"> </span><span class="fl">0.66</span>
train.ind &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="ot">TRUE</span>,<span class="ot">FALSE</span>) <span class="op">%&gt;%</span><span class="st">  </span>
<span class="st">  </span><span class="kw">sample</span>(<span class="dt">size =</span> n, <span class="dt">prob =</span> <span class="kw">c</span>(train.prop,<span class="dv">1</span><span class="op">-</span>train.prop), <span class="dt">replace=</span><span class="ot">TRUE</span>)
spam.train &lt;-<span class="st"> </span>spam[train.ind,]
spam.test &lt;-<span class="st"> </span>spam[<span class="op">!</span>train.ind,]

y.train.spam &lt;-<span class="st"> </span>spam.train<span class="op">$</span>spam
X.train.spam &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(spam.train[,<span class="kw">names</span>(spam.train)<span class="op">!=</span><span class="st">&#39;spam&#39;</span>] ) 
y.test.spam &lt;-<span class="st"> </span>spam.test<span class="op">$</span>spam
X.test.spam &lt;-<span class="st">  </span><span class="kw">as.matrix</span>(spam.test[,<span class="kw">names</span>(spam.test)<span class="op">!=</span><span class="st">&#39;spam&#39;</span>]) 

spam.dummy &lt;-<span class="st"> </span>spam
spam.dummy<span class="op">$</span>spam &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(spam<span class="op">$</span>spam<span class="op">==</span><span class="st">&#39;spam&#39;</span>) 
spam.train.dummy &lt;-<span class="st"> </span>spam.dummy[train.ind,]
spam.test.dummy &lt;-<span class="st"> </span>spam.dummy[<span class="op">!</span>train.ind,]</code></pre></div>
<p>We also define some utility functions that we will require down the road.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">l2 &lt;-<span class="st"> </span><span class="cf">function</span>(x) x<span class="op">^</span><span class="dv">2</span> <span class="op">%&gt;%</span><span class="st"> </span>sum <span class="op">%&gt;%</span><span class="st"> </span>sqrt 
l1 &lt;-<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">abs</span>(x) <span class="op">%&gt;%</span><span class="st"> </span>sum  
MSE &lt;-<span class="st"> </span><span class="cf">function</span>(x) x<span class="op">^</span><span class="dv">2</span> <span class="op">%&gt;%</span><span class="st"> </span>mean 
missclassification &lt;-<span class="st"> </span><span class="cf">function</span>(tab) <span class="kw">sum</span>(tab[<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)])<span class="op">/</span><span class="kw">sum</span>(tab)</code></pre></div>
<div id="least-squares" class="section level3">
<h3><span class="header-section-number">9.2.1</span> Linear Models with Least Squares Loss</h3>
<p>The simplest approach to supervised learning, is simply with OLS: a linear predictor, squared error loss, and train-test risk estimator. Notice the better in-sample MSE than the out-of-sample. That is overfitting in action.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ols.<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">lm</span>(lcavol<span class="op">~</span>. ,<span class="dt">data =</span> prostate.train)
<span class="co"># Train error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(ols.<span class="dv">1</span>)<span class="op">-</span>prostate.train<span class="op">$</span>lcavol) </code></pre></div>
<pre><code>## [1] 0.4383709</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(ols.<span class="dv">1</span>, <span class="dt">newdata=</span>prostate.test)<span class="op">-</span><span class="st"> </span>prostate.test<span class="op">$</span>lcavol)</code></pre></div>
<pre><code>## [1] 0.5084068</code></pre>
<p>Things to note:</p>
<ul>
<li>I use the <code>newdata</code> argument of the <code>predict</code> function to make the out-of-sample predictions required to compute the test-error.</li>
</ul>
<p>We now implement a V-fold CV, instead of our train-test approach. The assignment of each observation to each fold is encoded in <code>fold.assignment</code>. The following code is extremely inefficient, but easy to read.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">folds &lt;-<span class="st"> </span><span class="dv">10</span>
fold.assignment &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span>folds, <span class="kw">nrow</span>(prostate), <span class="dt">replace =</span> <span class="ot">TRUE</span>)
errors &lt;-<span class="st"> </span><span class="ot">NULL</span>

<span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>folds){
  prostate.cross.train &lt;-<span class="st"> </span>prostate[fold.assignment<span class="op">!=</span>k,] <span class="co"># train subset</span>
  prostate.cross.test &lt;-<span class="st">  </span>prostate[fold.assignment<span class="op">==</span>k,] <span class="co"># test subset</span>
  .ols &lt;-<span class="st"> </span><span class="kw">lm</span>(lcavol<span class="op">~</span>. ,<span class="dt">data =</span> prostate.cross.train) <span class="co"># train</span>
  .predictions &lt;-<span class="st"> </span><span class="kw">predict</span>(.ols, <span class="dt">newdata=</span>prostate.cross.test)
  .errors &lt;-<span class="st">  </span>.predictions<span class="op">-</span>prostate.cross.test<span class="op">$</span>lcavol <span class="co"># save prediction errors in the fold</span>
  errors &lt;-<span class="st"> </span><span class="kw">c</span>(errors, .errors) <span class="co"># aggregate error over folds.</span>
}

<span class="co"># Cross validated prediction error:</span>
<span class="kw">MSE</span>(errors)</code></pre></div>
<pre><code>## [1] 0.5492978</code></pre>
<p>Let’s try all possible variable subsets, and choose the best performer with respect to the Cp criterion, which is an unbiased risk estimator. This is done with <code>leaps::regsubsets</code>. We see that the best performer has 3 predictors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">regfit.full &lt;-<span class="st"> </span>prostate.train <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>leaps<span class="op">::</span><span class="kw">regsubsets</span>(lcavol<span class="op">~</span>.,<span class="dt">data =</span> ., <span class="dt">method =</span> <span class="st">&#39;exhaustive&#39;</span>) <span class="co"># best subset selection</span>
<span class="kw">plot</span>(regfit.full, <span class="dt">scale =</span> <span class="st">&quot;Cp&quot;</span>)</code></pre></div>
<p><img src="Rcourse_files/figure-html/all%20subset-1.png" width="50%" /></p>
<p>Things to note:</p>
<ul>
<li>The plot shows us which is the variable combination which is the best, i.e., has the smallest Cp.</li>
<li>Scanning over all variable subsets is impossible when the number of variables is large.</li>
</ul>
<p>Instead of the Cp criterion, we now compute the train and test errors for all the possible predictor subsets<a href="#fn20" class="footnoteRef" id="fnref20"><sup>20</sup></a>. In the resulting plot we can see overfitting in action.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model.n &lt;-<span class="st"> </span>regfit.full <span class="op">%&gt;%</span><span class="st"> </span>summary <span class="op">%&gt;%</span><span class="st"> </span>length
X.train.named &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(lcavol <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> prostate.train ) 
X.test.named &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(lcavol <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> prostate.test ) 

val.errors &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, model.n)
train.errors &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, model.n)
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>model.n) {
    coefi &lt;-<span class="st"> </span><span class="kw">coef</span>(regfit.full, <span class="dt">id =</span> i) <span class="co"># exctract coefficients of i&#39;th model</span>
    
    pred &lt;-<span class="st">  </span>X.train.named[, <span class="kw">names</span>(coefi)] <span class="op">%*%</span><span class="st"> </span>coefi <span class="co"># make in-sample predictions</span>
    train.errors[i] &lt;-<span class="st"> </span><span class="kw">MSE</span>(y.train <span class="op">-</span><span class="st"> </span>pred) <span class="co"># train errors</span>

    pred &lt;-<span class="st">  </span>X.test.named[, <span class="kw">names</span>(coefi)] <span class="op">%*%</span><span class="st"> </span>coefi <span class="co"># make out-of-sample predictions</span>
    val.errors[i] &lt;-<span class="st"> </span><span class="kw">MSE</span>(y.test <span class="op">-</span><span class="st"> </span>pred) <span class="co"># test errors</span>
}</code></pre></div>
<p>Plotting results.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(train.errors, <span class="dt">ylab =</span> <span class="st">&quot;MSE&quot;</span>, <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&quot;o&quot;</span>)
<span class="kw">points</span>(val.errors, <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">type =</span> <span class="st">&quot;b&quot;</span>, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)
<span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, 
       <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;Training&quot;</span>, <span class="st">&quot;Validation&quot;</span>), 
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;blue&quot;</span>), 
       <span class="dt">pch =</span> <span class="dv">19</span>)</code></pre></div>
<p><img src="Rcourse_files/figure-html/unnamed-chunk-185-1.png" width="50%" /></p>
<p>Checking all possible models is computationally very hard. <em>Forward selection</em> is a greedy approach that adds one variable at a time.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ols.<span class="dv">0</span> &lt;-<span class="st"> </span><span class="kw">lm</span>(lcavol<span class="op">~</span><span class="dv">1</span> ,<span class="dt">data =</span> prostate.train)
model.scope &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">upper=</span>ols.<span class="dv">1</span>, <span class="dt">lower=</span>ols.<span class="dv">0</span>)
<span class="kw">step</span>(ols.<span class="dv">0</span>, <span class="dt">scope=</span>model.scope, <span class="dt">direction=</span><span class="st">&#39;forward&#39;</span>, <span class="dt">trace =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## Start:  AIC=30.1
## lcavol ~ 1
## 
##           Df Sum of Sq     RSS     AIC
## + lpsa     1    54.776  47.130 -19.570
## + lcp      1    48.805  53.101 -11.578
## + svi      1    35.829  66.077   3.071
## + pgg45    1    23.789  78.117  14.285
## + gleason  1    18.529  83.377  18.651
## + lweight  1     9.186  92.720  25.768
## + age      1     8.354  93.552  26.366
## &lt;none&gt;                 101.906  30.097
## + lbph     1     0.407 101.499  31.829
## 
## Step:  AIC=-19.57
## lcavol ~ lpsa
## 
##           Df Sum of Sq    RSS     AIC
## + lcp      1   14.8895 32.240 -43.009
## + svi      1    5.0373 42.093 -25.143
## + gleason  1    3.5500 43.580 -22.817
## + pgg45    1    3.0503 44.080 -22.053
## + lbph     1    1.8389 45.291 -20.236
## + age      1    1.5329 45.597 -19.785
## &lt;none&gt;                 47.130 -19.570
## + lweight  1    0.4106 46.719 -18.156
## 
## Step:  AIC=-43.01
## lcavol ~ lpsa + lcp
## 
##           Df Sum of Sq    RSS     AIC
## &lt;none&gt;                 32.240 -43.009
## + age      1   0.92315 31.317 -42.955
## + pgg45    1   0.29594 31.944 -41.627
## + gleason  1   0.21500 32.025 -41.457
## + lbph     1   0.13904 32.101 -41.298
## + lweight  1   0.05504 32.185 -41.123
## + svi      1   0.02069 32.220 -41.052</code></pre>
<pre><code>## 
## Call:
## lm(formula = lcavol ~ lpsa + lcp, data = prostate.train)
## 
## Coefficients:
## (Intercept)         lpsa          lcp  
##     0.08798      0.53369      0.38879</code></pre>
<p>Things to note: - By default <code>step</code> add variables according to the <a href="https://en.wikipedia.org/wiki/Akaike_information_criterion">AIC</a> criterion, which is a theory-driven unbiased risk estimator. - We need to tell <code>step</code> which is the smallest and largest models to consider using the <code>scope</code> argument. - <code>direction=forward</code> is used to “grow” from a small model. For “shrinking” a large model, use <code>direction=backward</code>, or the default <code>direction=stepwise</code>.</p>
<p>We now learn a linear predictor on the <code>spam</code> data using, a least squares loss, and train-test risk estimator.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Train the predictor</span>
ols.<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">lm</span>(spam<span class="op">~</span>., <span class="dt">data =</span> spam.train.dummy) 

<span class="co"># make in-sample predictions</span>
.predictions.train &lt;-<span class="st"> </span><span class="kw">predict</span>(ols.<span class="dv">2</span>) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span> 
<span class="co"># inspect the confusion matrix</span>
(confusion.train &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.train, <span class="dt">truth=</span>spam.train.dummy<span class="op">$</span>spam)) </code></pre></div>
<pre><code>##           truth
## prediction    0    1
##      FALSE 1762  257
##      TRUE    80  908</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute the train (in sample) misclassification</span>
<span class="kw">missclassification</span>(confusion.train) </code></pre></div>
<pre><code>## [1] 0.1120718</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># make out-of-sample prediction</span>
.predictions.test &lt;-<span class="st"> </span><span class="kw">predict</span>(ols.<span class="dv">2</span>, <span class="dt">newdata =</span> spam.test.dummy) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span> 
<span class="co"># inspect the confusion matrix</span>
(confusion.test &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.test, <span class="dt">truth=</span>spam.test.dummy<span class="op">$</span>spam))</code></pre></div>
<pre><code>##           truth
## prediction   0   1
##      FALSE 905 146
##      TRUE   41 502</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute the train (in sample) misclassification</span>
<span class="kw">missclassification</span>(confusion.test)</code></pre></div>
<pre><code>## [1] 0.1173149</code></pre>
<p>Things to note: - I can use <code>lm</code> for categorical outcomes. <code>lm</code> will simply dummy-code the outcome. - A linear predictor trained on 0’s and 1’s will predict numbers. Think of these numbers as the probability of 1, and my prediction is the most probable class: <code>predicts()&gt;0.5</code>. - The train error is smaller than the test error. This is overfitting in action.</p>
<p>The <code>glmnet</code> package is an excellent package that provides ridge, lasso, and elastic net regularization, for all GLMs, so for linear models in particular.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">suppressMessages</span>(<span class="kw">library</span>(glmnet))
ridge.<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">glmnet</span>(<span class="dt">x=</span><span class="kw">scale</span>(X.train), <span class="dt">y=</span>y.train, <span class="dt">family =</span> <span class="st">&#39;gaussian&#39;</span>, <span class="dt">alpha =</span> <span class="dv">0</span>)

<span class="co"># Train error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(ridge.<span class="dv">2</span>, <span class="dt">newx =</span><span class="kw">scale</span>(X.train))<span class="op">-</span><span class="st"> </span>y.train)</code></pre></div>
<pre><code>## [1] 1.006028</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(ridge.<span class="dv">2</span>, <span class="dt">newx =</span> <span class="kw">scale</span>(X.test))<span class="op">-</span><span class="st"> </span>y.test)</code></pre></div>
<pre><code>## [1] 0.7662527</code></pre>
<p>Things to note:</p>
<ul>
<li>We use the <code>scale</code> function to z-score the predictors. This is recommended when regularizing, because we do not want the penalization to depend on the scale of the predictor. Do not <code>scale</code> on the whole data and then split to train and test. Rather, split and then <code>scale</code>.</li>
<li>The <code>alpha=0</code> parameters tells R to do ridge regression. Setting <span class="math inline">\(alpha=1\)</span> will do lasso, and any other value, with return an elastic net with appropriate weights.</li>
<li>The <code>family='gaussian'</code> argument tells R to fit a linear model, with least squares loss.</li>
<li>The test error is <strong>smaller</strong> than the train error. This may happen because risk estimators are random. Their variance may mask the overfitting.</li>
</ul>
<p>We now use the lasso penalty.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lasso.<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">glmnet</span>(<span class="dt">x=</span><span class="kw">scale</span>(X.train), <span class="dt">y=</span>y.train, , <span class="dt">family=</span><span class="st">&#39;gaussian&#39;</span>, <span class="dt">alpha =</span> <span class="dv">1</span>)

<span class="co"># Train error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(lasso.<span class="dv">1</span>, <span class="dt">newx =</span><span class="kw">scale</span>(X.train))<span class="op">-</span><span class="st"> </span>y.train)</code></pre></div>
<pre><code>## [1] 0.5525279</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(lasso.<span class="dv">1</span>, <span class="dt">newx =</span> <span class="kw">scale</span>(X.test))<span class="op">-</span><span class="st"> </span>y.test)</code></pre></div>
<pre><code>## [1] 0.547752</code></pre>
<p>We now use <code>glmnet</code> for classification.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">logistic.<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">cv.glmnet</span>(<span class="dt">x=</span><span class="kw">scale</span>(X.train.spam), <span class="dt">y=</span>y.train.spam, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>, <span class="dt">alpha =</span> <span class="dv">0</span>)</code></pre></div>
<p>Things to note:</p>
<ul>
<li>We used <code>cv.glmnet</code> to do an automatic search for the optimal level of regularization (the <code>lambda</code> argument in <code>glmnet</code>) using V-fold CV.</li>
<li>Just like the <code>glm</code> function, <code>family='binomial'</code> is used for logistic regression.</li>
<li>We use <code>scale</code> to z-score predictors so that they are all in the same scale.</li>
<li>We set <code>alpha=0</code> for an <span class="math inline">\(l_2\)</span> penalization of the coefficients of the logistic regression.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Train confusion matrix:</span>
.predictions.train &lt;-<span class="st"> </span><span class="kw">predict</span>(logistic.<span class="dv">2</span>, <span class="dt">newx =</span> <span class="kw">scale</span>(X.train.spam), <span class="dt">type =</span> <span class="st">&#39;class&#39;</span>) 
(confusion.train &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.train, <span class="dt">truth=</span>spam.train<span class="op">$</span>spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email  1763  184
##      spam     79  981</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Train misclassification error</span>
<span class="kw">missclassification</span>(confusion.train)</code></pre></div>
<pre><code>## [1] 0.08746259</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test confusion matrix:</span>
.predictions.test &lt;-<span class="st"> </span><span class="kw">predict</span>(logistic.<span class="dv">2</span>, <span class="dt">newx =</span> <span class="kw">scale</span>(X.test.spam), <span class="dt">type=</span><span class="st">&#39;class&#39;</span>) 
(confusion.test &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.test, <span class="dt">truth=</span>y.test.spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email   906  109
##      spam     40  539</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test misclassification error:</span>
<span class="kw">missclassification</span>(confusion.test)</code></pre></div>
<pre><code>## [1] 0.09347553</code></pre>
</div>
<div id="svm" class="section level3">
<h3><span class="header-section-number">9.2.2</span> SVM</h3>
<p>A support vector machine (SVM) is a linear hypothesis class with a particular loss function known as a <a href="https://en.wikipedia.org/wiki/Hinge_loss">hinge loss</a>. We learn an SVM with the <code>svm</code> function from the <strong>e1071</strong> package, which is merely a wrapper for the <a href="https://www.csie.ntu.edu.tw/~cjlin/libsvm/">libsvm</a> C library; the most popular implementation of SVM today.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(e1071)
svm.<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">svm</span>(spam<span class="op">~</span>., <span class="dt">data =</span> spam.train)

<span class="co"># Train confusion matrix:</span>
.predictions.train &lt;-<span class="st"> </span><span class="kw">predict</span>(svm.<span class="dv">1</span>) 
(confusion.train &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.train, <span class="dt">truth=</span>spam.train<span class="op">$</span>spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email  1780   94
##      spam     62 1071</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.train)</code></pre></div>
<pre><code>## [1] 0.05187895</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test confusion matrix:</span>
.predictions.test &lt;-<span class="st"> </span><span class="kw">predict</span>(svm.<span class="dv">1</span>, <span class="dt">newdata =</span> spam.test) 
(confusion.test &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.test, <span class="dt">truth=</span>spam.test<span class="op">$</span>spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email   912   75
##      spam     34  573</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.test)</code></pre></div>
<pre><code>## [1] 0.06838143</code></pre>
<p>We can also use SVM for regression.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">svm.<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">svm</span>(lcavol<span class="op">~</span>., <span class="dt">data =</span> prostate.train)

<span class="co"># Train error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(svm.<span class="dv">2</span>)<span class="op">-</span><span class="st"> </span>prostate.train<span class="op">$</span>lcavol)</code></pre></div>
<pre><code>## [1] 0.3336868</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(svm.<span class="dv">2</span>, <span class="dt">newdata =</span> prostate.test)<span class="op">-</span><span class="st"> </span>prostate.test<span class="op">$</span>lcavol)</code></pre></div>
<pre><code>## [1] 0.5633183</code></pre>
</div>
<div id="neural-nets" class="section level3">
<h3><span class="header-section-number">9.2.3</span> Neural Nets</h3>
<p>Neural nets (non deep) can be fitted, for example, with the <code>nnet</code> function in the <strong>nnet</strong> package. We start with a nnet regression.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(nnet)
nnet.<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">nnet</span>(lcavol<span class="op">~</span>., <span class="dt">size=</span><span class="dv">20</span>, <span class="dt">data=</span>prostate.train, <span class="dt">rang =</span> <span class="fl">0.1</span>, <span class="dt">decay =</span> <span class="fl">5e-4</span>, <span class="dt">maxit =</span> <span class="dv">1000</span>, <span class="dt">trace=</span><span class="ot">FALSE</span>)

<span class="co"># Train error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(nnet.<span class="dv">1</span>)<span class="op">-</span><span class="st"> </span>prostate.train<span class="op">$</span>lcavol)</code></pre></div>
<pre><code>## [1] 1.178329</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(nnet.<span class="dv">1</span>, <span class="dt">newdata =</span> prostate.test)<span class="op">-</span><span class="st"> </span>prostate.test<span class="op">$</span>lcavol)</code></pre></div>
<pre><code>## [1] 1.204695</code></pre>
<p>And nnet classification.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nnet.<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">nnet</span>(spam<span class="op">~</span>., <span class="dt">size=</span><span class="dv">5</span>, <span class="dt">data=</span>spam.train, <span class="dt">rang =</span> <span class="fl">0.1</span>, <span class="dt">decay =</span> <span class="fl">5e-4</span>, <span class="dt">maxit =</span> <span class="dv">1000</span>, <span class="dt">trace=</span><span class="ot">FALSE</span>)

<span class="co"># Train confusion matrix:</span>
.predictions.train &lt;-<span class="st"> </span><span class="kw">predict</span>(nnet.<span class="dv">2</span>, <span class="dt">type=</span><span class="st">&#39;class&#39;</span>) 
(confusion.train &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.train, <span class="dt">truth=</span>spam.train<span class="op">$</span>spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email  1801   25
##      spam     41 1140</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.train)</code></pre></div>
<pre><code>## [1] 0.02194879</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test confusion matrix:</span>
.predictions.test &lt;-<span class="st"> </span><span class="kw">predict</span>(nnet.<span class="dv">2</span>, <span class="dt">newdata =</span> spam.test, <span class="dt">type=</span><span class="st">&#39;class&#39;</span>) 
(confusion.test &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.test, <span class="dt">truth=</span>spam.test<span class="op">$</span>spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email   910   54
##      spam     36  594</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.test)</code></pre></div>
<pre><code>## [1] 0.05646173</code></pre>
</div>
<div id="classification-and-regression-trees-cart" class="section level3">
<h3><span class="header-section-number">9.2.4</span> Classification and Regression Trees (CART)</h3>
<p>A CART, is not a linear hypothesis class. It partitions the feature space <span class="math inline">\(\mathcal{X}\)</span>, thus creating a set of if-then rules for prediction or classification. It is thus particularly useful when you believe that the predicted classes may change abruptly with small changes in <span class="math inline">\(x\)</span>.</p>
<div id="the-rpart-package" class="section level4">
<h4><span class="header-section-number">9.2.4.1</span> The rpart Package</h4>
<p>This view clarifies the name of the function <code>rpart</code>, which <em>recursively partitions</em> the feature space.</p>
<p>We start with a regression tree.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rpart)
tree.<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">rpart</span>(lcavol<span class="op">~</span>., <span class="dt">data=</span>prostate.train)

<span class="co"># Train error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(tree.<span class="dv">1</span>)<span class="op">-</span><span class="st"> </span>prostate.train<span class="op">$</span>lcavol)</code></pre></div>
<pre><code>## [1] 0.4909568</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test error:</span>
<span class="kw">MSE</span>( <span class="kw">predict</span>(tree.<span class="dv">1</span>, <span class="dt">newdata =</span> prostate.test)<span class="op">-</span><span class="st"> </span>prostate.test<span class="op">$</span>lcavol)</code></pre></div>
<pre><code>## [1] 0.5623316</code></pre>
<p>We can use the <strong>rpart.plot</strong> package to visualize and interpret the predictor.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rpart.plot<span class="op">::</span><span class="kw">rpart.plot</span>(tree.<span class="dv">1</span>)</code></pre></div>
<p><img src="Rcourse_files/figure-html/unnamed-chunk-190-1.png" width="50%" /></p>
<p>Tree are very prone to overfitting. To avoid this, we reduce a tree’s complexity by <em>pruning</em> it. This is done with the <code>prune</code> function (not demonstrated herein).</p>
<p>We now fit a classification tree.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tree.<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">rpart</span>(spam<span class="op">~</span>., <span class="dt">data=</span>spam.train)

<span class="co"># Train confusion matrix:</span>
.predictions.train &lt;-<span class="st"> </span><span class="kw">predict</span>(tree.<span class="dv">2</span>, <span class="dt">type=</span><span class="st">&#39;class&#39;</span>) 
(confusion.train &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.train, <span class="dt">truth=</span>spam.train<span class="op">$</span>spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email  1766  216
##      spam     76  949</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.train)</code></pre></div>
<pre><code>## [1] 0.09710675</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test confusion matrix:</span>
.predictions.test &lt;-<span class="st"> </span><span class="kw">predict</span>(tree.<span class="dv">2</span>, <span class="dt">newdata =</span> spam.test, <span class="dt">type=</span><span class="st">&#39;class&#39;</span>) 
(confusion.test &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.test, <span class="dt">truth=</span>spam.test<span class="op">$</span>spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email   910  121
##      spam     36  527</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.test)</code></pre></div>
<pre><code>## [1] 0.09849435</code></pre>
</div>
<div id="the-caret-package" class="section level4">
<h4><span class="header-section-number">9.2.4.2</span> The caret Package</h4>
<p>TODO</p>
</div>
</div>
<div id="k-nearest-neighbour-knn" class="section level3">
<h3><span class="header-section-number">9.2.5</span> K-nearest neighbour (KNN)</h3>
<p>KNN is not an ERM problem. It is so fundamental, however, that we still show how to fit such a hypothesis class. Is it good? I have never seen a learning problem where KNN beats other methods. Others claim differently.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(class)
knn.<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">knn</span>(<span class="dt">train =</span> <span class="kw">scale</span>(X.train.spam), <span class="dt">test =</span> <span class="kw">scale</span>(X.test.spam), <span class="dt">cl =</span>y.train.spam, <span class="dt">k =</span> <span class="dv">1</span>)

<span class="co"># Test confusion matrix:</span>
.predictions.test &lt;-<span class="st"> </span>knn.<span class="dv">1</span> 
(confusion.test &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.test, <span class="dt">truth=</span>spam.test<span class="op">$</span>spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email   832   78
##      spam    114  570</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.test)</code></pre></div>
<pre><code>## [1] 0.1204517</code></pre>
</div>
<div id="linear-discriminant-analysis-lda" class="section level3">
<h3><span class="header-section-number">9.2.6</span> Linear Discriminant Analysis (LDA)</h3>
<p>LDA is equivalent to least squares classification <a href="supervised.html#least-squares">9.2.1</a>. This means that we actually did LDA when we used <code>lm</code> for binary classification (feel free to compare the confusion matrices). There are, however, some dedicated functions to fit it which we now introduce.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(MASS)
lda.<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">lda</span>(spam<span class="op">~</span>., spam.train)

<span class="co"># Train confusion matrix:</span>
.predictions.train &lt;-<span class="st"> </span><span class="kw">predict</span>(lda.<span class="dv">1</span>)<span class="op">$</span>class
(confusion.train &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.train, <span class="dt">truth=</span>spam.train<span class="op">$</span>spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email  1761  254
##      spam     81  911</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.train)</code></pre></div>
<pre><code>## [1] 0.1114067</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test confusion matrix:</span>
.predictions.test &lt;-<span class="st"> </span><span class="kw">predict</span>(lda.<span class="dv">1</span>, <span class="dt">newdata =</span> spam.test)<span class="op">$</span>class
(confusion.test &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.test, <span class="dt">truth=</span>spam.test<span class="op">$</span>spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email   905  144
##      spam     41  504</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.test)</code></pre></div>
<pre><code>## [1] 0.1160602</code></pre>
</div>
<div id="naive-bayes" class="section level3">
<h3><span class="header-section-number">9.2.7</span> Naive Bayes</h3>
<p>A Naive-Bayes classifier is also not part of the ERM framework. It is very popular, so we present it. It can be thought of LDA, i.e. linear regression, where predictors are assume to be uncorrelated. This may not be true, and take its toll on accuracy, but it makes computations extremely fast.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(e1071)
nb.<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">naiveBayes</span>(spam<span class="op">~</span>., <span class="dt">data =</span> spam.train)

<span class="co"># Train confusion matrix:</span>
.predictions.train &lt;-<span class="st"> </span><span class="kw">predict</span>(nb.<span class="dv">1</span>, <span class="dt">newdata =</span> spam.train)
(confusion.train &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.train, <span class="dt">truth=</span>spam.train<span class="op">$</span>spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email  1038   66
##      spam    804 1099</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.train)</code></pre></div>
<pre><code>## [1] 0.2893249</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test confusion matrix:</span>
.predictions.test &lt;-<span class="st"> </span><span class="kw">predict</span>(nb.<span class="dv">1</span>, <span class="dt">newdata =</span> spam.test)
(confusion.test &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="dt">prediction=</span>.predictions.test, <span class="dt">truth=</span>spam.test<span class="op">$</span>spam))</code></pre></div>
<pre><code>##           truth
## prediction email spam
##      email   526   41
##      spam    420  607</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">missclassification</span>(confusion.test)</code></pre></div>
<pre><code>## [1] 0.2892095</code></pre>
</div>
<div id="random-forrest" class="section level3">
<h3><span class="header-section-number">9.2.8</span> Random Forrest</h3>
<p>TODO</p>
<div id="the-randomforest-package" class="section level4">
<h4><span class="header-section-number">9.2.8.1</span> The randomForest Package</h4>
</div>
<div id="the-ranger-package" class="section level4">
<h4><span class="header-section-number">9.2.8.2</span> The ranger Package</h4>
</div>
</div>
<div id="gradient-boosting" class="section level3">
<h3><span class="header-section-number">9.2.9</span> Gradient Boosting</h3>
<div id="the-gbm-package" class="section level4">
<h4><span class="header-section-number">9.2.9.1</span> The gbm Package</h4>
</div>
<div id="the-xgboost-package" class="section level4">
<h4><span class="header-section-number">9.2.9.2</span> The xgboost Package</h4>
</div>
</div>
</div>
<div id="bibliographic-notes-6" class="section level2">
<h2><span class="header-section-number">9.3</span> Bibliographic Notes</h2>
<p>The ultimate reference on (statistical) machine learning is <span class="citation">Friedman, Hastie, and Tibshirani (<a href="#ref-friedman2001elements">2001</a>)</span>. For a softer introduction, see <span class="citation">James et al. (<a href="#ref-james2013introduction">2013</a>)</span>. A statistician will also like <span class="citation">Ripley (<a href="#ref-ripley2007pattern">2007</a>)</span>. For an R oriented view see <span class="citation">Lantz (<a href="#ref-lantz2013machine">2013</a>)</span>. For a very algorithmic view, see the seminal <span class="citation">Leskovec, Rajaraman, and Ullman (<a href="#ref-leskovec2014mining">2014</a>)</span> or <span class="citation">Conway and White (<a href="#ref-conway2012machine">2012</a>)</span>. For a much more theoretical reference, see <span class="citation">Mohri, Rostamizadeh, and Talwalkar (<a href="#ref-mohri2012foundations">2012</a>)</span>, <span class="citation">Vapnik (<a href="#ref-vapnik2013nature">2013</a>)</span>, <span class="citation">Shalev-Shwartz and Ben-David (<a href="#ref-shalev2014understanding">2014</a>)</span>. Terminology taken from <span class="citation">Sammut and Webb (<a href="#ref-sammut2011encyclopedia">2011</a>)</span>. For a review of resampling based unbiased risk estimation (i.e. cross validation) see the exceptional review of <span class="citation">Arlot, Celisse, and others (<a href="#ref-arlot2010survey">2010</a>)</span>. If you want to know about Deep-Nets in R see <a href="https://www.datacamp.com/community/tutorials/keras-r-deep-learning">here</a>.</p>
</div>
<div id="practice-yourself-6" class="section level2">
<h2><span class="header-section-number">9.4</span> Practice Yourself</h2>
<ol style="list-style-type: decimal">
<li>In <a href="glm.html#practice-glm">6.6</a> we fit a GLM for the <code>MASS::epil</code> data (Poisson family). We assume that the number of seizures (<span class="math inline">\(y\)</span>) depending on the age of the patient (<code>age</code>) and the treatment (<code>trt</code>).
<ol style="list-style-type: decimal">
<li>What was the MSE of the model?</li>
<li>Now, try the same with a ridge penalty using <code>glmnet</code> (<code>alpha=0</code>).</li>
<li>Do the same with a lasso penalty (<code>alpha=1</code>).</li>
<li>Compare the test MSE of the three models. Which is the best ?</li>
</ol></li>
<li>Read about the <code>Glass</code> dataset using <code>library(e1071)</code> and <code>?Glass</code>.
<ol style="list-style-type: decimal">
<li>Divide the dataset to train set and test set.</li>
<li>Apply the various predictors from this chapter, and compare them using the proportion of missclassified.</li>
</ol></li>
</ol>

</div>
</div>
<h3> Bibliography</h3>
<div id="refs" class="references">
<div id="ref-sammut2011encyclopedia">
<p>Sammut, Claude, and Geoffrey I Webb. 2011. <em>Encyclopedia of Machine Learning</em>. Springer Science &amp; Business Media.</p>
</div>
<div id="ref-foster2004variable">
<p>Foster, Dean P, and Robert A Stine. 2004. “Variable Selection in Data Mining: Building a Predictive Model for Bankruptcy.” <em>Journal of the American Statistical Association</em> 99 (466). Taylor &amp; Francis: 303–13.</p>
</div>
<div id="ref-friedman2001elements">
<p>Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. <em>The Elements of Statistical Learning</em>. Vol. 1. Springer series in statistics Springer, Berlin.</p>
</div>
<div id="ref-james2013introduction">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 6. Springer.</p>
</div>
<div id="ref-ripley2007pattern">
<p>Ripley, Brian D. 2007. <em>Pattern Recognition and Neural Networks</em>. Cambridge university press.</p>
</div>
<div id="ref-lantz2013machine">
<p>Lantz, Brett. 2013. <em>Machine Learning with R</em>. Packt Publishing Ltd.</p>
</div>
<div id="ref-leskovec2014mining">
<p>Leskovec, Jure, Anand Rajaraman, and Jeffrey David Ullman. 2014. <em>Mining of Massive Datasets</em>. Cambridge University Press.</p>
</div>
<div id="ref-conway2012machine">
<p>Conway, Drew, and John White. 2012. <em>Machine Learning for Hackers</em>. “ O’Reilly Media, Inc.”</p>
</div>
<div id="ref-mohri2012foundations">
<p>Mohri, Mehryar, Afshin Rostamizadeh, and Ameet Talwalkar. 2012. <em>Foundations of Machine Learning</em>. MIT press.</p>
</div>
<div id="ref-vapnik2013nature">
<p>Vapnik, Vladimir. 2013. <em>The Nature of Statistical Learning Theory</em>. Springer science &amp; business media.</p>
</div>
<div id="ref-shalev2014understanding">
<p>Shalev-Shwartz, Shai, and Shai Ben-David. 2014. <em>Understanding Machine Learning: From Theory to Algorithms</em>. Cambridge university press.</p>
</div>
<div id="ref-arlot2010survey">
<p>Arlot, Sylvain, Alain Celisse, and others. 2010. “A Survey of Cross-Validation Procedures for Model Selection.” <em>Statistics Surveys</em> 4. The author, under a Creative Commons Attribution License: 40–79.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="19">
<li id="fn19"><p>It is even a subset of the Hilbert space, itself a subset of the space of all functions.<a href="supervised.html#fnref19">↩</a></p></li>
<li id="fn20"><p>Example taken from <a href="https://lagunita.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/ch6.html" class="uri">https://lagunita.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/ch6.html</a><a href="supervised.html#fnref20">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="multivariate.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="unsupervised.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/08-supervised.Rmd",
"text": "Edit"
},
"download": ["Rcourse.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
