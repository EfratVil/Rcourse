# Memory Efficiency {#memory}

<!-- our of ram -->
<!-- databases -->

As we have seen in the Sparsity Chapter \@ref(sparse), an efficient representation of your data in RAM will reduce computing time, and will allow you to fit models that would otherwise require tremendous amounts of RAM.
It is possible, however, to avoid altogether the need to fit your data in RAM.
The fundamental idea is that your data can be stored on the _hard disk_ (HD), _solid state disk_ (SSD), or any other storage. 
Since these types of storage are typically much larger than your RAM, you will be able to compute with much larger data sets. 
The downside is that HDs, SSDs, and others, are typically slower than RAM.
This is painfully true for rotating magnetic plates HDs. 
This is true, but less painful, with newer SSDs.

We will now meet several facilities that will allow you to avoid loading your data to RAM, and still be able to fit models with it.
The fundamental idea is to use efficient file structures, that will save your data in a way that is aware of the upcoming model-fitting, so that fetching it from HD is efficient.
This is not a unique capability of R. 
Indeed, SAS, SPSS, Pyton's Sci-Kit Learn, and other software have this same capability.
Moreover, there are many facilities in R that provide this same capability. 
For instance, the commercial version of R, _Microsoft R_^[Previously known as _Revolutions R_], provides this capability with the __RevoScaleR__ family of packages. 

The facilities that allow _out of RAM_ computations can be broadly calssified as:

1. Facilities that use efficient represantations in RAM.
1. Facilities that rely on temporary files with efficient structure. 
1. Facilities that rely on databases.


```{remark}
If you use Linux, you may be better of than Windows users. 
Linux will allow you to compute with larger datasets using its _swap file_ that extends RAM using your HD or SSD.
On the other hand, relying on the swap file is a BAD practice since it is much slower than RAM, and you can typically do much better using the tricks of this chapter. 
Also, while I LOVE Linux, I would never dare to recommend switching to Linux just to deal with memory contraints. 
```


## Efficient Computing from RAM

If our data can fit in RAM, but is still too large to compute with it (recall that fitting a model requires roughly 2-3 times more memory than saving it), there are several facilities to be used.
The first, is the sparse represantation discussed in Chapter \@ref(sparse), which is relavant when you have factors, which will typically map to sparse model matrices. 
Another way is to use memory efficient algorithms. 
We do not discuss this avenue here, since the approaches in the next sections are more general (at the possible cost of some computing time).




## Computing From Efficient File Structrures

The foundamental idea of computing out of RAM is to save your data on disk in an efficient structure, and then use algorithms that are aware of this structure. 
These algorithms will typically be fed with chunks, or batches, of the whole data, so that at no time will you need the whole data in RAM.

Our first package that provides this facility is the __ff__ package. 




<!-- ff-->
<!-- ffbase -->
<!-- biglm -->
<!-- biganalytics -->
<!-- bigtabulate -->
<!-- bigalgebra -->




## Computing From Databases
<!-- dplyr -->
<!-- MonetDB.R -->




If your data is so big that it cannot fit on your HD you will need a distributed file system.
We do not cover this topic here, and refer the reader to the __RHipe__, __RHadoop__, and __RSpark__ packages and references therein.


## Bibliographic Notes

For an up-to-date list of the packages that deal with memory constraints, see the __Large memory and out-of-memory data__ section in the High Performance Computing [R task view](https://cran.r-project.org/web/views/HighPerformanceComputing.html).
For a historical review of the R's progress in dealing with large datasets, see [Joseph Rickert's review] (http://blog.revolutionanalytics.com/2014/05/quick-history-2-glms-r-and-large-data-sets.html).
For a general review (not only R) of computing with big data, see @wang2015statistical.
